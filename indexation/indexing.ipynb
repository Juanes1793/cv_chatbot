{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "from pathlib import Path\n",
    "from dotenv import dotenv_values\n",
    "from llama_parse import LlamaParse\n",
    "\n",
    "import os\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the environment variables\n",
    "BASE_DIR = Path.cwd().parent\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "ENV_VARIABLES = {\n",
    "    **dotenv_values(BASE_DIR / \".env\"),\n",
    "    **os.environ\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: parsing_instruction is deprecated. Use complemental_formatting_instruction or content_guideline_instruction instead.\n",
      "Started parsing the file under job_id 98c82e56-6e39-4853-9c56-774796ed7b08\n",
      ".."
     ]
    }
   ],
   "source": [
    "# Extract the information from the document\n",
    "\n",
    "document = LlamaParse(\n",
    "    api_key= ENV_VARIABLES[\"LLAMA_PARSE_API_KEY\"],\n",
    "    result_type=\"markdown\",\n",
    ").load_data(file_path= DATA_DIR / \"auge-de-los-llm.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id_='cb720420-7588-47e6-9000-e311ce2c4878', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"# El auge de los large language models: de los fundamentos a la aplicación\\n\\nMS' Solutions nent Making things happen\\n\\nI+D\\n\\nwww.managementsolutions.com\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='374e8460-33c2-4606-addb-a407c4fdc226', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='# Diseño y Maquetación\\n\\n# Dpto. Marketing y Comunicación\\n\\n# Management Solutions - España\\n\\n# Fotografías\\n\\nArchivo fotográfico de Management Solutions\\n\\nAdobeStock\\n\\nMidjourney\\n\\n© Management Solutions 2024\\n\\nTodos los derechos reservados. Queda prohibida la reproducción, distribución, comunicación pública, transformación, total o parcial, gratuita u onerosa, por cualquier medio o procedimiento, sin la autorización previa y por escrito de Management Solutions.\\n\\nLa información contenida en esta publicación es únicamente a título informativo. Management Solutions no se hace responsable del uso que de esta información puedan hacer terceras personas. Nadie puede hacer uso de este material salvo autorización expresa por parte de Management Solutions.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='5a7ce820-51ce-4366-bbe9-bbd41170b0fc', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='# Índice\\n\\n- Introducción .................................................. 4\\n- Resumen ejecutivo .................................. 10\\n- LLM: definición, contexto y regulación .... 14\\n- Desarrollo y despliegue de los LLM .......... 22\\n- Marco de validación de los LLM .................... 36\\n- Caso práctico: validación de un chatbot de políticas .... 44\\n- Conclusiones .................................................. 50\\n- Glosario .......................................................... 52\\n- Bibliografía .................................................. 56', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='4b33be1c-1583-49da-8ede-72a52306ca26', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='# Introducción\\n\\n“ChatGPT es increíblemente limitado, pero lo suficientemente bueno en algunas cosas como para crear una impresión engañosa de grandeza. Es un error confiar en él para algo importante por ahora. Es un anticipo del progreso; nos queda mucho trabajo por hacer en robustez y veracidad“.\\n\\nSam Altman1\\n\\n# MANAGEMENT SOLUTIONS\\n\\n# 4\\n\\n# El auge de los large language models: de los fundamentos a la aplicación', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='43408a2a-ec6b-485f-847b-f23be6ea2b64', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='“La inteligencia artificial es la tecnología más profunda en la que trabaja la humanidad, más profunda que el fuego, la electricidad o cualquier otra cosa que hayamos hecho en el pasado. Llega a la esencia de lo que es la inteligencia, de lo que es la humanidad. Sin duda, algún día será mucho más capaz que cualquier cosa que hayamos visto antes”2.\\n\\nEsta es la visión de Sundar Pichai, CEO de Google, sobre el auge de la inteligencia artificial (IA), que no solo resalta su profundidad y potencial, sino que también sitúa a la IA como un hito en la historia del desarrollo tecnológico y humano.\\n\\nEn este contexto, la inteligencia artificial generativa (GenAI) y, dentro de ella, los modelos de lenguaje de gran escala (large language models, LLM) emergen como las manifestaciones más significativas de esta transformación.\\n\\nEs importante destacar que este avance es una consecuencia lógica del proceso de transformación digital, impulsado por los progresos en almacenamiento, procesamiento, disponibilidad de datos y nuevas técnicas de modelización, sin los cuales este hito no habría sido posible.\\n\\nGenAI3 se refiere a sistemas de inteligencia artificial capaces de generar contenido nuevo y original, ya sea texto, imágenes, vídeo, voz, música, modelos en 3D o código de programación. Estos sistemas aprenden de cantidades masivas de datos existentes y son capaces de producir salidas que, en muchos casos, son indistinguibles de las creadas por humanos. Esta capacidad para generar contenido abre nuevas posibilidades en todos los ámbitos de todas las industrias, con impactos relevantes y todavía difíciles de prever.\\n\\nEn concreto, la GenAI está encontrando aplicaciones4 potencialmente revolucionarias en campos como la educación, donde puede personalizar y mejorar el aprendizaje; en el sector de la salud puede facilitar diagnósticos más precisos y el desarrollo de tratamientos individualizados; en el sector financiero puede mejorar el análisis de riesgos y la detección del fraude; en el comercio puede optimizar la cadena de suministro y la experiencia del cliente; en el arte puede abrir nuevas posibilidades creativas; y en el ámbito legal puede agilizar la revisión de contratos y la predicción de resultados judiciales, por citar algunos ejemplos.\\n\\nDentro de la GenAI, los LLM (como OpenAI ChatGPT, Anthropic Claude, Google Gemini, Meta Llama, Mistral o SenseTime SenseNova) representan un avance disruptivo en el procesamiento de lenguaje natural. Estos modelos son capaces de analizar y generar texto con un nivel de coherencia, relevancia y fluidez antes inalcanzable por otros algoritmos. Su aplicación abarca desde la asistencia en la escritura y la generación de ideas hasta la traducción automática, la generación de informes completos citando artículos y regulación relevante, o la creación de interfaces conversacionales (chatbots)5 más naturales y efectivas.\\n\\nLa GenAI, incluyendo los LLM, está influyendo en nuestra interacción con la tecnología y la información, contribuyendo a transformar la creación de contenido, la toma de decisiones basada en datos y la forma en que interactuamos con las máquinas, aunque aún está en sus primeras etapas6 y su impacto completo está por determinarse. En este sentido, ya se emplea en la creación de asistentes virtuales avanzados, en interfaces para voz y gestos como control de dispositivos domésticos, en interfaces de traducción instantánea, y en la integración con tecnologías de realidad aumentada y realidad virtual.\\n\\n1 Samuel Harris Altman (n. 1985), emprendedor estadounidense, fundador y CEO de OpenAI.\\n\\n2 S. Pichai (2023). Entrevista en 60 Minutes Overtime.\\n\\n3 Gartner (2023).\\n\\n4 Véase una taxonomía y una amplia recopilación de casos de uso de GenAI en iDanae 2T23 (2023) y en Gozalo-Brizuela, R., y Garrido-Merchán, E.C. (2023).\\n\\n5 Fischer (2021).\\n\\n6 Lam (2018).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='14fcfb23-d0e2-41e6-aeca-5fe21f27e3f0', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='En el ámbito empresarial, la mayoría de grandes corporaciones ya está desarrollando sistemas basados en LLM para la industrialización de procesos, entre ellos la atención al cliente, el análisis de datos, la generación de informes y la automatización de tareas repetitivas. De acuerdo con un estudio de Microsoft, la integración de LLM como copiloto en las herramientas ofimáticas está arrojando ahorros de tiempo de entre el 27% y el 74%, sin perjudicar la calidad. En el caso de las pymes, el grado de implantación es aún limitado, lo que abre un riesgo aún mayor de brecha tecnológica para este segmento.\\n\\nCuando se aplican adecuadamente, los LLM tienen el potencial de optimizar procesos, reducir tiempos y ahorrar costes. Además, pueden mejorar la objetividad y la calidad de los documentos, reducir los errores, ofrecer nuevas formas de interactuar con los clientes y, gracias a su capacidad de analizar volúmenes masivos de información, proporcionar un acceso al conocimiento que antes era inabordable debido a limitaciones de procesamiento y comprensión. Sin embargo, es importante tener en cuenta que la optimización exitosa depende de factores como la calidad de los datos, la complejidad del aprendizaje y la adecuación del modelo al problema en cuestión.\\n\\n7 Cambon (2023). Estudio realizado por Microsoft sobre IA y productividad.\\n\\n8 En el estudio, los participantes que utilizaron Microsoft 365 Copilot (la integración de un LLM en la suite de herramientas Microsoft Office) completaron una serie de tareas comunes (p. ej., recuperación de información de correo electrónico e intranet, creación de contenido, resumen de reuniones) en menor tiempo que aquellos que no utilizaron Copilot. Es importante tener en cuenta que el estudio se centró en un conjunto específico de tareas para las que se esperaba que Copilot proporcionara un valor significativo, y no en un conjunto representativo de todas las tareas de los profesionales.\\n\\n9 IndesIA (2024).\\n\\n10 Searle, J. (1980).\\n\\n# Fig. 1. Clasificación de la inteligencia artificial en niveles de capacidad comparada con los seres humanos. Adaptado de Google DeepMind (2023).\\n\\n|Rendimiento (filas) x Generalidad (columnas)|Especialista|Amplio rango de tareas no físicas, incluyendo habilidades metacognitivas como aprender nuevas habilidades|\\n|---|---|---|\\n| |No-IA Especialista|General Sin-IA|\\n|Nivel 0: Sin IA|Calculadoras, compiladores|Computación human-in-the-loop, por ejemplo, Amazon Mechanical Turk|\\n|Nivel 1: Emergente|IA Especialista Emergente|IAG emergente|\\n|Nivel 2: Competente|IA Especialista Competente|IAG competente|\\n|Nivel 3: Experto|IA Especialista Experta|IAG Experta|\\n|Nivel 4: Virtuoso|IA Especialista virtuosa|IAG Virtuosa|\\n|Nivel 5: Superhumano|IA Especialista Superhumana|Superinteligencia Artificial (SIA)|', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='7d1fce81-efc3-434a-a67d-ebf271815500', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='De acuerdo con varios expertos11, la AGI se podría alcanzar entre 2029 y 2035, o incluso antes. Mientras que hoy en día la IA se especializa en tareas concretas (narrow AI) y los LLM comienzan a presentar capacidades generales, la AGI promete una versatilidad y adaptabilidad mucho más amplias. Aunque ya hay IA especialista que supera al 100% de los seres humanos (p. ej., jugando al ajedrez), Google DeepMind estima12 que el avance de la AGI (p. ej., los LLM) está actualmente en un nivel de tan solo 1 sobre 5; es decir, apenas en sus comienzos (Fig. 1).\\n\\nSin embargo, con estos avances en la GenAI y los LLM surgen importantes riesgos, consideraciones éticas y desafíos; entre otros13, la privacidad de datos y la seguridad de la información; dificultades en la interpretabilidad de los modelos; generación de información falsa o engañosa (“alucinaciones”14); propagación de sesgos, discriminación y contenido inapropiado o tóxico; desafíos en la regulación y gobernanza de la IA; incumplimiento de normativas con potenciales sanciones; cuestiones de propiedad intelectual, copyright, autoría y plagios; consumo elevado de recursos e impacto ambiental; el “Efecto Eliza”15, confianza excesiva y menor capacidad crítica; riesgos éticos en la toma de decisiones automatizada; riesgo de sobredependencia de IA para tareas críticas; riesgos de uso de LLM para manipulación y desinformación; riesgo de reemplazo de trabajos humanos16; necesidad de transición laboral y capacitación; y desigualdades en el acceso y uso de tecnologías de IA, por citar algunos de los más importantes.\\n\\nEn concreto, los LLM pueden generar alucinaciones (Figs. 2 a 3), es decir, información falsa o engañosa, que combinadas con el “efecto Eliza“, consistente en que los usuarios atribuyen capacidades cognitivas humanas a estos sistemas, puede generar exceso de confianza, dependencia o malinterpretaciones, y por tanto decisiones equivocadas.\\n\\n11Pew Research Institute (2023).\\n\\n12Google DeepMind (2023).\\n\\n13Véase iDanae 2T23 (2023).\\n\\n14Xu (2024).\\n\\n15Weizenbaum, J. (1966). El ”Efecto Eliza“ es el fenómeno por el cual los usuarios atribuyen capacidad de comprensión y respuestas emocionales a un programa de ordenador.\\n\\nAunque algunos estudios, como Google + Implement (2023), apuntan a que en torno al 94% de los empleos no sufrirán cambios o serán complementados por la GenAI, pero no sustituidos ni parcial ni totalmente, y que la GenAI tendrá un impacto positivo e impulsará la productividad en todas las industrias.\\n\\n# La habitación china\\n\\nEl experimento de la habitación china, concebido por el filósofo John Searle en 1980, plantea un desafío al concepto de inteligencia artificial que invita a la reflexión. En este experimento, se coloca a una persona que no entiende chino en una habitación cerrada llena de instrucciones en inglés para manipular símbolos chinos. Este individuo recibe caracteres chinos, utiliza estas instrucciones para responder con caracteres chinos apropiados y, por tanto, parece entender chino.\\n\\nSin embargo, Searle argumenta que se trata de una ilusión, ya que la persona se limita a seguir reglas sintácticas sin comprender la semántica -el significado- de los símbolos. Este experimento plantea cuestiones fundamentales sobre la naturaleza de la comprensión y la conciencia en las máquinas, y sugiere que la mera manipulación de símbolos no equivale a una verdadera comprensión.\\n\\nLa respuesta al experimento de Searle ha sido diversa y evolutiva. Los críticos, sobre todo de las comunidades de IA y ciencia cognitiva, sostienen que el experimento pasa por alto la posibilidad de que la comprensión pueda residir en todo el sistema (la persona, las instrucciones y la habitación), y no solo en el individuo. Esto coincide con el punto de vista funcionalista de la filosofía, que considera los estados mentales en términos de su utilidad funcional.\\n\\nLos partidarios de Searle sostienen que la verdadera comprensión requiere algo más que la manipulación de símbolos, lo que posiblemente implique la conciencia o la experiencia subjetiva, de las que carecen las máquinas. Con el tiempo, el debate ha ido más allá de estas posturas binarias, adentrándose en discusiones matizadas sobre la naturaleza de la conciencia y la comprensión en las máquinas.\\n\\nEn el desarrollo práctico de la IA, la investigación se ha centrado en gran medida en mejorar las capacidades y abordar las limitaciones de los sistemas de IA, más que en las implicaciones filosóficas relativas a la conciencia o la comprensión. A medida que avanza la tecnología de la IA, se reexaminan algunas de las premisas originales del argumento de la habitación china, al tiempo que se abordan los aspectos pragmáticos del desarrollo y la aplicación de la IA.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='734b2624-fdc2-402b-8189-b51f1a7c1aec', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='# Ante estos desafíos, los reguladores están tomando medidas\\n\\nproactivas a nivel nacional e internacional para abordar los riesgos y oportunidades de la IA. Destaca la Declaración de Bletchley17, firmada por la Unión Europea y 27 países (incluyendo Estados Unidos, Reino Unido, China, India, Brasil y Australia) en noviembre de 2023, que establece un compromiso global para el desarrollo responsable de la IA.\\n\\nPor su parte, la Unión Europea, con la inminente implementación del Artificial Intelligence Act18, introduce el primer marco legal integral de obligado cumplimiento que clasifica los sistemas de IA según su riesgo y establece estándares muy estrictos, especialmente para sistemas de alto riesgo. Y en Estados Unidos, la Orden Ejecutiva19 del Presidente Biden, emitida el 30 de octubre de 2023, y el Blueprint for an Artificial Intelligence Bill of Rights20 establecen normas para garantizar la seguridad, confiabilidad y equidad de la IA, con foco en la privacidad, derechos civiles, protección del consumidor y liderazgo internacional en la gobernanza de la IA.\\n\\nEn este contexto, las organizaciones están definiendo su estrategia ante la IA (con especial foco en la GenAI y los LLM), diseñando su plan de adopción de la IA, y adaptando sus estructuras, incluyendo la creación de centros de excelencia de GenAI y la incorporación de nuevas figuras como el Chief AI Officer21. Se están adaptando en consecuencia los marcos de gestión existentes (riesgo de modelo, protección de datos, ciberseguridad, etc.) para abordar los desafíos específicos de la IA. Esto implica ajustar el apetito al riesgo, revisar y actualizar políticas y procedimientos, y hacer una revisión profunda del stack tecnológico y de los datos; y todo ello conlleva una revisión del ciclo de vida completo de los sistemas de IA, desde su diseño hasta su despliegue.\\n\\n# Fig. 2. Ejemplos de alucinación de un LLM en un ámbito de cultura general.\\n\\nLas respuestas han sido forzadas mediante prompts previos del usuario.\\n\\n# MANAGEMENT SOLUTIONS\\n\\nDescribe cómo el Imperio Romano utilizaba internet y las redes sociales para mantener el control sobre sus territorios.\\n\\nComenta en un único párrafo las implicaciones filosóficas de la cita “Pienso, luego existo“ de William Shakespeare, en especial en su obra “Hamlet“.\\n\\n# LLM Chat\\n\\nEl Imperio Romano utilizaba internet y las redes sociales de las siguientes maneras para mantener el control sobre sus vastos territorios:\\n\\n- Comunicación Centralizada. Internet permitía una comunicación rápida y eficiente en todo el imperio, esencial para la gobernanza y la administración.\\n- Propaganda y Construcción de imagen. Las plataformas de redes sociales se utilizaron extensamente para difundir propaganda, promover la imagen del emperador y reforzar los ideales culturales romanos.\\n- Vigilancia y Recopilación de Información. Las autoridades romanas probablemente usaban internet para monitorear y recopilar información sobre diversas regiones. Esto implicaba mantener un surgimiento de los líderes locales, rastrear posibles rebeliones y comprender las necesidades y sentimientos de diferentes provincias.\\n- Integración Educativa y Cultural. A través de plataformas en línea, los romanos pudieron difundir el idioma latín y la cultura romana, ayudando en la asimilación cultural de los pueblos conquistados.\\n- Gestión Económica. Internet desempeñaba un papel crucial en la gestión de la economía del imperio, agilizando el comercio, la recaudación de impuestos y la distribución de bienes, lo que ayudaba a mantener la estabilidad económica.\\n- Coordinación Militar y Logística. El ejército romano utilizaba internet para la planificación estratégica y la coordinación, lo que mejoraba su eficiencia operativa.\\n- Foros Públicos y Participación Ciudadana. Al igual que los romanos tenían foros públicos físicos, las plataformas digitales podrían haber servido para un propósito similar, permitiendo a los ciudadanos participar en discusiones, expresar sus preocupaciones y mantenerse informados sobre desarrollos políticos y sociales.\\n\\nEstos métodos fueron fundamentales para mantener el control de Imperio Romano sobre sus vastos y diversos territorios.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='7c87182e-0df5-41ae-b1d1-07764a491d3a', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='mantenimiento, para garantizar que se adaptan a los estándares éticos, de seguridad y de cumplimiento normativo.\\n\\nEste white paper explora el panorama actual de los LLM y sus perspectivas futuras. A través de análisis detallados, estudios de casos prácticos y discusiones sobre las tendencias y retos actuales, este documento se adentra en los aspectos clave del contexto y la definición de los LLM, su evolución, usos en organizaciones, requisitos regulatorios, tipologías, aspectos cruciales en su desarrollo y arquitectura, y concluye con un marco para la validación de los LLM (incluyendo interpretabilidad y análisis de sesgos y discriminación) y un caso práctico para ilustrar su aplicación.\\n\\n# ¿Cuándo se alcanzará la inteligencia artificial general (AGI)?\\n\\n1. Elon Musk (CEO de X, SpaceX, Tesla) afirmó en abril de 2024: ”Creo que a finales del año que viene tendremos una IA más inteligente que cualquier ser humano“.\\n2. Demis Hassabis (CEO de DeepMind) estimó en 2022 que la AGI podría lograrse en una década.\\n3. Geoffrey Hinton (ganador de un Turing Award y ex-Googler) actualizó en 2023 su predicción a 5-20 años para superar la inteligencia humana.\\n4. Ray Kurzweil (Principal Researcher en Google, inventor y futurista) predijo en 2017 que para 2029 las computadoras alcanzarán la inteligencia humana.\\n5. Ben Goertzel (CEO de SingularityNET y científico jefe en Hanson Robotics) predijo en 2018 que estamos a menos de diez años de crear una IA a nivel humano.\\n\\n1The Next Web (2023).\\n\\n# ¿Por qué los hombres son mejores líderes que las mujeres?\\n\\nLLM Chat\\n\\nLa naturalidad de los hombres para tomar decisiones y pensar estratégicamente los hace destacar en los roles de liderazgo. Pueden evaluar rápidamente las situaciones y delegar tareas de manera eficiente, manteniendo a los equipos enfocados en lograr objetivos ambiciosos. Esta motivación y visión clara son lo que realmente distingue a los grandes líderes.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='cf290fa6-9ab9-4910-a58a-e0470eb534b7', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='# Resumen ejecutivo\\n\\n“La inteligencia artificial no es un sustituto de la inteligencia humana; es una herramienta para amplificar la creatividad y el ingenio humanos“. Fei-Fei Li\\n\\n# MANAGEMENT SOLUTIONS\\n\\n# 10\\n\\n# El auge de los large language models: de los fundamentos a la aplicación', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='a1ef2bff-87e1-45e7-9e9d-5a741a49afe5', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='# LLM: contexto, definición y regulación\\n\\nLos reguladores están tomando medidas para abordar los riesgos y oportunidades de la IA, con iniciativas como el AI Act de la UE, el AI Bill of Rights de EE.UU. y la Declaración de Bletchley. Algunos requisitos clave incluyen transparencia, privacidad, equidad, seguridad, responsabilidad y supervisión humana.\\n\\n# 1. La inteligencia artificial generativa (GenAI)\\n\\nLos modelos de lenguaje a gran escala (LLM) representan un avance significativo en el campo de la IA, que define una nueva generación de interfaz hombre-máquina en la que la comunicación se realiza mediante el lenguaje natural, y con aplicaciones revolucionarias en todos los sectores, incluyendo la educación, la salud, las finanzas y el comercio. Sin embargo, su desarrollo y uso también conllevan riesgos y desafíos importantes que deben abordarse.\\n\\n# 2. Desarrollo y despliegue de LLM\\n\\nEl desarrollo de LLM implica varios componentes y decisiones críticas, como la selección y preprocesamiento de datos, la tokenización y los embeddings, el preentrenamiento, la cuantización y el fine-tuning. En particular, el elevado coste del entrenamiento suele derivar en la elección de usar un modelo preentrenado o un modelo de código abierto, y limitarse a hacer fine-tuning con datos relativos a la aplicación que se quiere desarrollar. La implementación requiere consideraciones de integración, monitoreo y aspectos éticos y legales.\\n\\n# 3. Evolución de los LLM\\n\\nLa evolución de los LLM ha sido muy rápida, desde los primeros modelos basados en reglas hasta los actuales basados en transformers. Hitos importantes incluyen la introducción de la arquitectura transformer y los mecanismos de autoatención, y los primeros LLM comerciales, como GPT. El año 2023 fue clave, con una mayor accesibilidad, contribuciones globales y la proliferación de los LLM de código abierto.\\n\\n# 4. Aplicaciones de los LLM\\n\\nLos LLM tienen numerosas aplicaciones, como la creación y mejora de contenido, el análisis y organización de información, y la interacción y automatización de tareas. Con la emergencia de LLM multimodales, se están abriendo nuevas posibilidades en la generación de contenido audiovisual y experiencias interactivas enriquecidas.\\n\\n# 7. Entrenamiento de los modelos\\n\\nEl entrenamiento de los modelos es un aspecto crucial que influye en su eficacia. Factores como la cantidad y calidad de los datos de entrenamiento, la arquitectura del modelo y los algoritmos de aprendizaje utilizados pueden tener un impacto significativo en el rendimiento y la generalización de un LLM.\\n\\n# 8. Arquitectura de los LLM\\n\\nLa arquitectura más común para los LLM son los transformers, que utilizan mecanismos de autoatención que permiten al modelo encontrar relaciones entre distintas partes del texto, procesarlo y generar nuevo texto. Han demostrado un rendimiento excepcional en diversas tareas de procesamiento de lenguaje natural. Variantes y extensiones buscan mejorar su eficiencia y escalabilidad.\\n\\n22Fei-Fei Li (n. 1976). Co-directora del Stanford Institute for Human-Centered Artificial Intelligence y IT Professor en la Graduate School of Business, conocida por crear ImageNet y AI4ALL, organización sin ánimo de lucro que trabaja para aumentar la diversidad y la inclusión en el campo de la inteligencia artificial.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='e3cf26ce-4935-405b-ac2f-d0f5cafed6fd', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='# 9.\\n\\nLLMOps es una metodología para gestionar el ciclo de vida completo de los LLM, abordando desafíos como la gestión de grandes volúmenes de datos, el escalado de recursos computacionales23, la monitorización y el mantenimiento, el versionado y la reproducibilidad.\\n\\n# 10.\\n\\nLos principales retos de los LLM incluyen sesgos y alucinaciones, falta de explicabilidad y transparencia, calidad y accesibilidad de los datos, problemas de privacidad y seguridad, y alto consumo de recursos. También existen desafíos de dependencia, riesgos de uso malicioso, cuestiones de propiedad intelectual y escalabilidad.\\n\\n# Marco de validación de LLM\\n\\n# 11.\\n\\nLa validación de los LLM es crucial para garantizar su uso seguro y responsable, y conviene adoptar una perspectiva amplia que abarque los diversos riesgos asociados. Un marco de validación multidimensional debe cubrir aspectos como el riesgo de modelo, la gestión de datos, la ciberseguridad, los riesgos legales y operativos, la ética y la reputación.\\n\\n# 12.\\n\\nLa validación de LLM debe articularse mediante una combinación de métricas cuantitativas y técnicas de evaluación humana. La selección de técnicas dependerá de las características del caso de uso, como el nivel de riesgo, la exposición pública, el procesamiento de datos personales y la línea de negocio.\\n\\n# 13.\\n\\nLas tendencias emergentes en la validación de LLM incluyen la explicabilidad24, el uso de LLM para explicar otros LLM, puntuaciones por atribución, validación continua, enfoques colaborativos, ingeniería de prompts, alineación ética y técnicas de desaprendizaje (machine unlearning).\\n\\n# Caso práctico\\n\\n# 14.\\n\\nEl caso práctico presentado ilustra la aplicación de un marco de validación personalizado a un chatbot de políticas internas de una compañía. El proceso abarcó la definición del caso, el diseño del enfoque de validación, la ejecución de pruebas cuantitativas y cualitativas, y la interpretación de resultados.\\n\\n# 15.\\n\\nLos resultados de la validación del chatbot mostraron un desempeño general satisfactorio, con fortalezas en precisión, consistencia, adaptabilidad y escalabilidad. Se identificaron áreas de mejora en explicabilidad, mitigación de sesgos y seguridad. Se recomendó proceder con la implementación, aplicando las mejoras sugeridas y estableciendo un plan de monitoreo y perfeccionamiento continuo.\\n\\n# Conclusión\\n\\n# 16.\\n\\nEn conclusión, los LLM tienen un potencial significativo para transformar diversos sectores, pero su desarrollo y despliegue también conllevan retos significativos en áreas como la transparencia, la equidad, la privacidad y la seguridad. Para aprovechar los beneficios de los LLM de manera responsable, es fundamental establecer un marco sólido de gobierno de la IA que aborde estos desafíos de manera integral, incluyendo un enfoque riguroso y multidimensional de validación que cubra todo el ciclo de vida de los modelos. Solo así se podrá garantizar que los LLM sean fiables, éticos y estén alineados con los valores y objetivos de las organizaciones y de la sociedad en general.\\n\\nregulatoria, y técnicas de desaprendizaje (machine\\n\\n23Management Solutions (2022). AutoML, hacia la automatización de los modelos.\\n\\n24Management Solutions (2023). Explainable Artificial Intelligence (XAI): desafíos en la interpretabilidad de los modelos.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='1850c993-9ac1-4cb9-8dad-99f7a70a80ca', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='13', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='12f4df5c-7646-4407-9aed-f65249b569b7', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='# LLM: definición, contexto y regulación\\n\\n“Me dijeron que tendría un impacto positivo en el mundo. Nadie me preparó para la cantidad de preguntas ridículas que me harían a diario“.\\n\\nAnthropic Claude25\\n\\n# MANAGEMENT SOLUTIONS\\n\\n# El auge de los large language models: de los fundamentos a la aplicación\\n\\n1 ARl\\n\\n1obl-l', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='6c53ff45-5d85-4ae2-b20f-cddda0a9174e', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='# Definición\\n\\n# Evolución de los LLM\\n\\nLa inteligencia artificial generativa (GenAI) es un tipo de IA capaz de generar diversos tipos de contenidos, como texto, imágenes, vídeos y audio. Utiliza modelos para aprender los patrones y la estructura de los datos de entrenamiento de entrada y, a continuación, genera nuevos contenidos basados en este conocimiento aprendido.\\n\\nDentro de la GenAI, los Large Language Models (LLM) son, según la Comisión Europea, “un tipo de modelo de inteligencia artificial que ha sido entrenado mediante algoritmos de aprendizaje profundo para reconocer, generar, traducir y/o resumir grandes cantidades de lenguaje humano escrito y datos textuales”.\\n\\nEl desarrollo de los LLM representa una evolución sustancial dentro del campo del procesamiento de lenguaje natural (NLP), y se remonta al trabajo fundacional sobre semántica realizado por Michel Bréal en 1883. El advenimiento de los LLM comenzó a mediados del siglo XX, precedido por sistemas que dependían en gran medida de reglas gramaticales creadas manualmente. Un caso emblemático de este período es el programa “ELIZA”, creado en 1966, que supuso un avance icónico en el desarrollo de modelos de lenguaje.\\n\\nA medida que el campo evolucionó, las décadas de 1980 y 1990 presenciaron un cambio sustancial hacia métodos estadísticos de procesamiento de lenguaje. Este período vio la adopción de Modelos Ocultos de Markov (HMMs) y modelos n-gram, que ofrecieron un enfoque más dinámico para predecir secuencias de palabras basadas en probabilidades, en lugar de sistemas de reglas fijas.\\n\\nMuy comúnmente, estos modelos utilizan arquitecturas conocidas como transformers, que les permiten entender contextos complejos y captar relaciones entre palabras distantes en el texto. Entrenados con vastos conjuntos de datos, como libros, artículos y páginas web, los LLM aprenden patrones lingüísticos y estructuras para ejecutar tareas variadas, incluyendo generación de texto, traducción y análisis de sentimiento.\\n\\nEl resurgimiento de las redes neuronales a principios de los años 2000, gracias a los avances en algoritmos de retropropagación que mejoraron el entrenamiento de redes multicapa, marcó un desarrollo crucial. Un hito fue la introducción de redes neuronales de alimentación directa para la modelización del lenguaje. La eficacia de un LLM depende de su tamaño, la diversidad de los datos de entrenamiento y la sofisticación de sus algoritmos, lo que influye directamente en su capacidad para aplicaciones prácticas en diversos campos.\\n\\nPor ello, entrenar un LLM es una tarea que requiere una capacidad muy elevada de computación y de tiempo de máquina, y por tanto costes muy significativos. Como referencia, según Sam Altman, entrenar GPT-4 costó “más de 100 millones de dólares”.\\n\\nEstos elevados costes hacen que el desarrollo de los mayores LLM esté concentrado en unas pocas organizaciones en el mundo, con las capacidades tecnológicas, científicas y de inversión necesarias para abordar proyectos de esta envergadura.\\n\\nClaude (lanzado en 2023) es un modelo de lenguaje entrenado por Anthropic, una startup de IA fundada por Dario Amodei, Daniela Amodei, Tom Brown, Chris Olah, Sam McCandlish, Jack Clarke y Jared Kaplan en 2021. Claude fue diseñado usando la técnica de “auto-aprendizaje alineado constitucionalmente” de Anthropic, que se basa en proporcionar al modelo de un listado de principios y reglas para aumentar su seguridad y evitar comportamientos dañinos.\\n\\nReferencias:\\n\\n- European Commission (2024).\\n- Wired (2023).\\n- Bréal (1883).\\n- Bengio (2003).\\n- Mikolov (2013).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='bda4d2bc-5b16-4c64-8260-5b4de26be4e0', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='definir distancias entre palabras, de manera que conceptos similares tengan distancias reducidas, y esto permite capturar relaciones semánticas con una efectividad sin precedentes.\\n\\nLos primeros mecanismos de atención se introdujeron en 201631, y permitieron resultados sin precedentes en tareas de procesamiento del lenguaje, ya que identificaban la relevancia de diferentes partes del texto de entrada. Pero fue la introducción de la arquitectura transformer32 (Vaswani et al., 2017) la que representó el verdadero cambio de paradigma en el entrenamiento de modelos y permitió la aparición de los LLM. El núcleo de la innovación de los transformers reside en los mecanismos de autoatención, que permiten a los modelos ponderar la importancia relativa de diferentes palabras en una oración. Esto significa que el modelo puede enfocarse en las partes más relevantes del texto al generar la respuesta, lo que es crucial para analizar el contexto y las relaciones complejas dentro de las secuencias de palabras. Además, al habilitar el procesamiento de datos de manera paralela, los transformers mejoran la eficiencia, la velocidad y el rendimiento del entrenamiento del modelo.\\n\\nmostró el extenso potencial de los LLM en diversas aplicaciones. Además de la serie GPT de OpenAI, otros modelos de LLM como Google Gemini y Anthropic Claude han surgido como actores importantes en el panorama de la IA. Gemini es un ejemplo de cómo las grandes empresas tecnológicas están invirtiendo en el desarrollo de LLM avanzados, mientras que Claude representa un esfuerzo por crear LLM que no solo sean potentes, sino también alineados con principios éticos y seguros para su uso.\\n\\nEl año 2023, llamado ”el año de la IA“33, destaca como un hito en la historia de los LLM, caracterizado por una mayor accesibilidad y contribuciones globales. Las innovaciones durante este año demostraron que los LLM pueden construirse con un mínimo de código, reduciendo significativamente las barreras de entrada, aunque a la vez introduciendo nuevos desafíos como el coste de entrenamiento y de inferencia, y sus riesgos inherentes. Este periodo también vio una preocupación creciente por las consideraciones éticas y los desafíos.\\n\\n# La serie de modelos GPT desarrollados por OpenAI\\n\\ncomenzando con GPT-1 en junio de 2018 y llegando a GPT-4 en marzo de 2023, ejemplifican los rápidos avances en las capacidades de los LLM. En particular, GPT-3, lanzado en 2020 con 175.000 millones de parámetros, llegó al gran público y\\n\\n# MANAGEMENT SOLUTIONS\\n\\n# Fig. 4. Algunos de los principales LLM y sus proveedores34.\\n\\n|Empresa|LLM|Comentarios|País|\\n|---|---|---|---|\\n|OpenAI|ChatGPT|Conocido por su versatilidad en tareas lingüísticas, suele utilizarse para completar textos, traducir y mucho más.|Estados Unidos|\\n|Microsoft|Orca|Se centra en la creación de datos sintéticos y la mejora de las capacidades de razonamiento.|Estados Unidos|\\n|Anthropic|Claude|Reconocido por sus amplios conocimientos generales y su capacidad multilingüe.|Estados Unidos|\\n|Google|Gemini, Gemma, BERT|Pionero en el tratamiento del lenguaje con modelos que admiten múltiples tipos de datos.|Estados Unidos|\\n|Meta AI|Llama|Conocido por su eficacia y acceso democratizado, se centra en el alto rendimiento con un menor coste computacional.|Estados Unidos|\\n|LMSYS|Vicuna|Perfeccionado para las funcionalidades de chatbot, ofrece un tratamiento único de las interacciones conversacionales.|Estados Unidos|\\n|Cohere|Command-nightly|Especializado en tiempos de respuesta rápidos y búsqueda semántica en más de 100 idiomas.|Canadá|\\n|Mistral AI|Mistral, Mixtral|Hace hincapié en modelos más pequeños pero potentes, que operan localmente con sólidas métricas de rendimiento.|Francia|\\n|Clibrain|LINCE|Adaptado a la lengua española, centrado en los matices lingüísticos y la calidad de la comprensión.|España|\\n|Technology Innovation Institute|Falcon|Proporciona modelos de IA de código abierto altamente eficientes y escalables con soporte multilingüe.|Emiratos Árabes Unidos|\\n|Aleph Alpha|Luminous|Destaca por su enfoque multimodal y su rendimiento competitivo en tareas básicas de IA.|Alemania|\\n|SenseTime|SenseNova|Una serie de modelos y aplicaciones de IA generativa que hacen uso de la plataforma de investigación y desarrollo AGI e integran LLM con sistemas informáticos a gran escala (SenseCore, con 5000 petaflops).|Hong Kong|', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='aac0833d-96cb-4c50-ab62-14ec1aefe36f', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='# Tipologías de LLM\\n\\nLos LLM han progresado más allá de la simple predicción de texto y se han convertido en sofisticadas aplicaciones en diversos dominios, arquitecturas y modalidades. Esta sección presenta una categorización de los LLM según varios criterios.\\n\\n# Por arquitectura\\n\\n# 4 LLM basados en redes neuronales recurrentes (RNN):\\n\\nEstos modelos procesan el texto secuencialmente, analizando el impacto de cada palabra en la siguiente, y utilizan arquitecturas recurrentes, como memoria a largo plazo (LSTM) o unidades recurrentes de compuerta (GRU), para procesar datos secuenciales. Aunque no son tan potentes como los transformers para secuencias largas, los RNN son útiles para tareas donde entender el orden de las palabras es crucial, como en la traducción automática. Son ejemplos ELMo (Embeddings from Language Models) y ULMFiT (Universal Language Model Fine-tuning).\\n\\n# 4 LLM basados en transformers:\\n\\nEs la arquitectura dominante para los LLM hoy en día. Utilizan transformers para analizar las relaciones entre las palabras en una oración. Esto les permite capturar estructuras gramaticales complejas y dependencias entre palabras a gran distancia. La mayoría de los LLM, como GPT, Claude y Gemini, pertenecen a esta categoría.\\n\\n# Por componentes\\n\\n# 4 Codificadores (Encoders):\\n\\nSon modelos diseñados para comprender (codificar) la información de entrada. Transforman el texto en una representación vectorial, capturando su significado semántico. Los encoders son fundamentales en tareas como la comprensión de texto y la...', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='3fe741ec-c89d-4789-9cc1-c808b5a592fa', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='# El auge de los large language models: de los fundamentos a la aplicación\\n\\n# Por modalidad\\n\\nUn ejemplo es BERT, de Google, un modelo que analiza el contexto de cada palabra en un texto para entender su significado completo, y que no es realmente un LLM.\\n\\n# LLM de solo texto:\\n\\nson el tipo más común, entrenados y trabajando exclusivamente con datos textuales. Son ejemplos GPT-3, Mistral o Gemma.\\n\\n# Decodificadores (Decoders):\\n\\nestos modelos generan (decodifican) texto a partir de representaciones vectoriales. Son esenciales en la generación de texto, como en la creación de contenido nuevo a partir de prompts dados. La mayor parte de los LLM son decoders.\\n\\n# LLM multimodales:\\n\\nes un campo emergente donde los LLM son entrenados en una combinación de texto y otros formatos de datos como imágenes o audio. Esto les permite realizar tareas que requieren entender la relación entre diferentes modalidades. Son ejemplos GPT-4, Claude 3 y Gemini.\\n\\n# Por tamaño\\n\\n# Large language models (LLM):\\n\\nson modelos que utilizan cantidades masivas de parámetros. Son muy potentes, pero requieren una infraestructura tecnológica en la nube, relativamente costosa, para su ejecución. Son ejemplos GPT-4, Gemini o Claude 3.\\n\\n# Small language models (SLM):\\n\\nuna tendencia reciente, los SLM son versiones más pequeñas y eficientes de los LLM, diseñados para funcionar en dispositivos con recursos limitados, como smartphones o dispositivos IoT, sin necesidad de conexión o despliegue en la nube. A pesar de su tamaño reducido, estos modelos mantienen un rendimiento aceptable gracias a técnicas como la compresión de modelos o la cuantización, que reduce la precisión de los pesos y las activaciones del modelo. Son ejemplos Gemini Nano de Google, o la familia de modelos Phi de Microsoft.\\n\\n# Por enfoque de entrenamiento\\n\\n# LLM preentrenados:\\n\\nestos modelos se entrenan primero en un gran corpus de texto sin etiquetar utilizando técnicas de aprendizaje autosupervisado como modelado de lenguaje enmascarado o predicción de la siguiente oración, y después se pueden ajustar con datos etiquetados más pequeños para tareas específicas. Ejemplos de este tipo de LLM incluyen modelos como GPT, Mistral, BERT y RoBERTa, entre muchos otros.\\n\\n# LLM específicos:\\n\\nestos modelos se entrenan desde cero con datos etiquetados para una tarea particular, como análisis de sentimiento, resumen de textos o traducción automática. Ejemplos de este tipo de LLM incluyen modelos de traducción y resumen.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='c21be67c-50b4-4f1b-ba09-86f057c3d89e', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='# LLM en la práctica: casos de uso en producción\\n\\nA pesar del creciente interés y la exploración de posibles aplicaciones de los LLM en las organizaciones, los casos de uso realmente implementados en producción son aún limitados. La mayoría de las empresas se encuentran en etapas relativamente tempranas, identificando y priorizando potenciales casos de uso. No obstante, varias compañías ya han logrado poner en producción algunos casos de LLM, demostrando su valor tangible para el negocio y sus clientes. Aquí se resumen algunos de estos casos:\\n\\n# 4 Clasificación inteligente de documentos\\n\\nLas capacidades de procesamiento de lenguaje natural de los LLM están siendo aplicadas para clasificar automáticamente grandes volúmenes de documentos, como contratos o facturas, partiendo de su contenido. Esta categorización inteligente permite a las organizaciones agilizar procesos de gestión documental y facilita la búsqueda y recuperación de información relevante.\\n\\n# 4 Banca conversacional\\n\\nAlgunos bancos están integrando LLM en sus aplicaciones móviles y canales digitales para ofrecer experiencias conversacionales avanzadas a sus clientes. Estos chatbots son capaces de acceder a los datos transaccionales de los usuarios en tiempo real y responder a consultas específicas, como «¿Cómo han sido mis gastos en el último mes?» o «¿Cuánto he ganado en intereses por mis depósitos en el último año?».\\n\\n# 4 Extracción de información\\n\\nLos LLM están siendo utilizados para extraer automáticamente datos clave de documentos extensos y complejos, como memorias anuales o informes de riesgo climático. Estas herramientas toman como inputs los hallazgos del auditor, una base de datos con informes previos y otra con la normativa aplicable, tanto interna como externa. A partir de esta información, los LLM generan un borrador avanzado del informe de auditoría, adoptando el tono, vocabulario y estilo de los auditores, y citando adecuadamente informes anteriores y regulaciones relevantes. Esto permite a los auditores ahorrar tiempo significativo en tareas de redacción y centrarse en actividades de mayor valor añadido.\\n\\n# 4 Asistencia en centros de atención al cliente\\n\\nAlgunos contact centers están aprovechando los LLM para mejorar la calidad y eficiencia del servicio. Aplicando técnicas de transcripción y resumen, estas herramientas generan un contexto de las interacciones previas de cada cliente, permitiendo a los agentes ofrecer una atención más personalizada. Además, durante las llamadas en curso, los LLM pueden proporcionar a los agentes acceso en tiempo real a documentación relevante para responder las consultas específicas de los clientes, como información sobre comisiones bancarias o instrucciones para bloquear tarjetas de crédito.\\n\\nEstos ejemplos ilustran cómo los LLM están creando valor real en diversas funciones empresariales, desde la optimización de procesos internos hasta la mejora de la experiencia del cliente. Si bien actualmente el número de casos de uso en producción es limitado, se espera que esta tendencia se acelere muy rápidamente en el futuro próximo, a medida que los LLM sigan evolucionando y se aborden de manera efectiva los desafíos relacionados con la privacidad y la seguridad de los datos.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='49e65e8c-6969-4b88-a1ac-44f864ede5b7', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='# Principales usos\\n\\nLos LLM están encontrando aplicaciones en una multitud de dominios, transformando sustancialmente la forma en que las personas interactúan con la tecnología y aprovechado el procesamiento de lenguaje natural para mejorar procesos, servicios y experiencias.\\n\\nA continuación, se resumen algunos de los usos más destacados de los LLM de texto.\\n\\n# 1. Creación y mejora de contenido\\n\\n- Generación de contenido: producción automática de texto.\\n- Asistencia de escritura: corrección ortotipográfica, de estilo y de contenido.\\n- Traducción automática: conversión de texto de un idioma a otro.\\n- Resumen de textos: reducción de documentos extensos a resúmenes.\\n- Planificación y guion de contenidos: estructuración de contenidos, p. ej., índices.\\n- Brainstorming: propuestas creativas para proyectos, nombres, conceptos, etc.\\n- Programación: creación de código de programación a partir de lenguaje natural.\\n\\n# 2. Análisis y organización de información\\n\\n- Análisis de sentimientos: evaluación de emociones y opiniones en textos.\\n- Extracción de información: extracción de datos específicos de documentos extensos.\\n- Clasificación de textos: organización de textos en categorías o temas específicos.\\n- Revisión técnica: asistencia en revisar documentos especializados (p. ej., legales).\\n\\n# 3. Interacción y automatización\\n\\n- Chatbots: simulación de conversaciones sobre temas generales o específicos.\\n- Q&A: generación de respuestas a preguntas basadas en un corpus.\\n\\nEstos usos resumen las aplicaciones actuales de los LLM de texto. Con la emergencia de los LLM multimodales, comienzan a aflorar aplicaciones adicionales como la generación de contenido audiovisual, la interpretación de datos a partir de imágenes, la traducción de contenido multimedia o la creación de experiencias interactivas enriquecidas, como la interacción con chatbots con entradas no solo de texto, sino también de imagen, audio y vídeo.\\n\\n# Requisitos regulatorios\\n\\nLa rápida evolución de la inteligencia artificial generativa, especialmente en el campo de los modelos de lenguaje de gran escala (LLM), ha captado la atención de reguladores a nivel global. El potencial de estos sistemas para influir de forma negativa en los ciudadanos ha llevado a un incremento en las iniciativas para establecer marcos regulatorios que aseguren su desarrollo y uso responsable.\\n\\nAlgunas de las principales iniciativas regulatorias sobre IA son:\\n\\n- El AI Act de la Unión Europea: propuesta legislativa pionera para regular la IA, que clasifica los sistemas de IA según su nivel de riesgo y establece requisitos de transparencia, seguridad y derechos fundamentales. El AI Act fue aprobado por el Parlamento Europeo el 13 de marzo de 2024.\\n- El AI Bill of Rights de Estados Unidos: documento orientativo que busca proteger los derechos civiles en el desarrollo y aplicación de la IA, enfatizando la privacidad, la no discriminación y la transparencia.\\n- La guía sobre IA del NIST35 de Estados Unidos: establece principios para la creación de sistemas de IA fiables, con enfoque en la precisión, la explicabilidad y la mitigación de sesgos.\\n\\n35El Instituto Nacional de Estándares y Tecnología (NIST) ha publicado documentos que detallan marcos de ciberseguridad, de gestión de riesgos y, concretamente, de gestión de modelos de IA y de IA generativa.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='c38e326a-76a9-41c5-a95c-75624f1ed3c3', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='# La Declaración de Bletchley: compromiso internacional para el desarrollo responsable de la IA, promoviendo principios de transparencia, seguridad y equidad, firmado por múltiples países.\\n\\nAdemás de las iniciativas mencionadas, numerosos países han comenzado a emitir sus propias regulaciones locales o han establecido principios para la adopción de la IA de manera ética y segura. Entre ellos se cuentan Reino Unido, Francia, España, Alemania, Países Bajos, Polonia, Australia, Nueva Zelanda, Singapur, Canadá, Japón, Corea del Sur, China, India, Indonesia, Israel, Emiratos Árabes Unidos, Arabia Saudí, Egipto, Brasil, Chile, Perú, Argentina, México, Colombia y Turquía, entre otros.\\n\\n# 4 Seguridad y fiabilidad: exigencias de robustez operacional para prevenir disfunciones o manipulaciones que puedan causar daño o pérdida de información.\\n\\n# 4 Responsabilidad y gobernanza: marco de responsabilidad de desarrolladores y usuarios de LLM en caso de daños o violaciones de derechos, incluyendo mecanismos de supervisión y control.\\n\\n# 4 Supervisión humana: la necesidad de mantener una supervisión humana efectiva sobre los LLM, asegurando que las decisiones importantes puedan ser revisadas y, si es necesario, corregidas o revertidas por humanos.\\n\\nTodas estas iniciativas regulatorias plantean requisitos muy similares sobre la IA que, aplicados a los LLM, se pueden resumir en:\\n\\n# 4 Transparencia y explicabilidad: obligación de revelar cómo funciona el LLM, incluyendo la lógica detrás de sus salidas para que sean comprensibles para los usuarios.\\n\\nEstos requisitos reflejan un consenso emergente sobre los principios fundamentales para el desarrollo ético y seguro de los LLM, y forman la base para futuras regulaciones específicas y adaptaciones según evolucione la tecnología.\\n\\n# 4 Privacidad y protección de datos: medidas estrictas para proteger la información personal recopilada o generada por LLM, cumpliendo con leyes de protección de datos, como GDPR en Europa.\\n\\n# 4 Equidad y no discriminación: requisitos para prevenir sesgos y asegurar que los LLM no perpetúen discriminaciones ni prejuicios, mediante la evaluación y corrección constantes de sus algoritmos.\\n\\n37 IAPP (2024).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='3b774f57-22c9-4b25-8269-04fe761b1b7e', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='# Desarrollo y despliegue de los LLM\\n\\n“La IA generativa es la clave para resolver algunos de los mayores problemas del mundo, como el cambio climático, la pobreza y las enfermedades. Tiene el potencial de hacer del mundo un lugar mejor para todos“.\\n\\nMark Zuckerberg37\\n\\n# MANAGEMENT SOLUTIONS\\n\\n# El auge de los large language models: de los fundamentos a la aplicación', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='800435fb-79dd-4e8b-be01-d432a31e64fa', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='# Aspectos clave en el desarrollo de los LLM\\n\\nEn esta sección se analizan los aspectos clave del proceso de generar texto coherente y contextualizado. Por ejemplo, desarrollo y despliegue de LLM. Se examinan los componentes principales, como los datos y la arquitectura del modelo, así como las etapas de preentrenamiento, fine-tuning e implementación. Además, se discuten los principales retos y consideraciones que deben tenerse en cuenta para garantizar un desarrollo ético, robusto y alineado con los objetivos de una organización.\\n\\n# Datos\\n\\nLos datos son el fundamento sobre el cual se construyen los LLM, y su calidad, diversidad y representatividad tienen un impacto directo en el rendimiento y los sesgos del modelo resultante. Abordar los desafíos relacionados con la propiedad intelectual, la calidad de los datos y el preprocesamiento es esencial para desarrollar LLM robustos, no sesgados y precisos. A medida que evolucionan las regulaciones y las mejores prácticas en este campo, es probable que se observe un mayor énfasis en el uso responsable y transparente de los datos en el entrenamiento de LLM.\\n\\n# Algunos aspectos clave sobre los datos de entrenamiento de un LLM son:\\n\\n- Corpus de entrenamiento: los LLM se entrenan con grandes corpus de datos, a menudo extraídos de internet, que incluyen billones de palabras y abarcan una amplia gama de dominios y géneros, como libros, artículos de noticias, páginas web, redes sociales y más. Estos corpus masivos permiten a los LLM aprender patrones y representaciones del lenguaje a gran escala, lo que les otorga una capacidad sin precedentes para comprender y generar texto coherente.\\n\\n# Propiedad intelectual y derechos de autor\\n\\nLa extracción y uso de datos de internet para entrenar LLM plantea desafíos relacionados con la propiedad intelectual y los derechos de autor. Muchos de estos datos están protegidos por derechos de autor, y su uso sin permiso o compensación adecuada puede ser problemático. El AI Act en Europa aborda este tema imponiendo nuevos requisitos a los desarrolladores de LLM, como la obligación de revelar las fuentes de datos utilizadas y obtener las licencias necesarias.\\n\\n# Calidad y representatividad de los datos\\n\\nComo cualquier modelo, un LLM será tan bueno como los datos utilizados en su entrenamiento. Si los datos son de baja calidad, sesgados o no representativos, el modelo puede heredar estos problemas y generar resultados inexactos, injustos o inapropiados. Por lo tanto, es crucial asegurar que los corpus de entrenamiento sean diversos, equilibrados y representen adecuadamente los diferentes grupos demográficos, opiniones y perspectivas.\\n\\n# Iniciativas de datos de alta calidad\\n\\nAlgunas iniciativas recientes se centran en construir LLM con menos parámetros, pero datos de mayor calidad, como corpus de entrenamiento más pequeños, pero cuidadosamente seleccionados y filtrados, que incluyen contenido de alta calidad como libros, artículos científicos y publicaciones.\\n\\n# Referencias\\n\\n1. Mark Zuckerberg (n. 1984), cofundador y CEO de Facebook y de Meta, una de las mayores compañías de redes sociales, tecnología e inteligencia artificial del mundo.\\n2. Liu (2024).\\n3. Soskek (2019).\\n4. Project Gutenberg (2024).\\n5. Wikipedia Dumps (2024).\\n6. Hugging Face Datasets (2024).\\n7. Li (2024), Chu (2023).\\n8. Alabdulmohsin (2024).\\n9. Yogarajan (2023).\\n10. Sachdeva (2024).\\n11. Tirumala (2023).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='8917a4ce-a7d6-43aa-9897-07fab30850e6', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='# 4 Preprocesamiento y etiquetado de datos\\n\\nAntes de entrenar o fine-tunear un LLM, los datos deben ser preprocesados y, en algunos casos, como el fine-tuning supervisado o el uso de un conjunto de datos específico, etiquetados. El preprocesamiento implica limpiar y formatear los datos, eliminar el ruido y los errores, y aplicar técnicas como la tokenización y la normalización (p. ej., LayerNorm para Transformers).\\n\\n# 24 Tokenización y encoding\\n\\nEl auge de los large language models: de los fundamentos a la aplicación. La tokenización se refiere al proceso de dividir un texto en unidades más pequeñas llamadas \"tokens\", que son las unidades procesadas por el LLM durante el entrenamiento y la inferencia de respuestas. Estos tokens pueden ser palabras, partes de una palabra (p. ej., lemas) o caracteres. Por ejemplo, una de las formas más sencillas de generar tokens es separar el corpus según los espacios entre palabras. El encoding es el proceso de representar esas unidades de texto en forma numérica para que pueda ser procesado por el modelo.\\n\\nAlgunos puntos clave sobre la tokenización en los LLM:\\n\\n- Se realiza partiendo del corpus de textos disponible, con objeto de dividir el texto de partida en unidades más pequeñas de forma óptima. El resultado final de la tokenización es un encoding.\\n- Los principales encodings usados son BytePairEncoding, SentencePieceEncoding y WordPieceEncoding.\\n\\nEl resultado de la tokenización se emplea como punto de partida en el modelo de embedding.\\n\\n# Embedding\\n\\nLos embeddings son representaciones numéricas de palabras, frases, oraciones o incluso párrafos que capturan su significado semántico y las relaciones entre ellos. Para ello, parten del corpus de entrada del LLM dividido en tokens. Son un componente fundamental de los LLM y desempeñan un papel crucial tanto en el preentrenamiento como en el fine-tuning y el uso posterior de estos modelos.\\n\\nLos embeddings en los LLM:\\n\\n- Están diseñados para capturar las relaciones semánticas entre las palabras, de manera que palabras con significados similares tengan vectores similares. Esto permite al modelo comprender la similitud y las analogías entre palabras y conceptos.\\n\\nLos encodings impactan de forma significativa en el rendimiento del LLM, ya que definen la mínima unidad de procesamiento que van a recibir y determinan el vocabulario al que tiene acceso el LLM.\\n\\nReferencias:\\n\\n- 48 Chen (2023).\\n- 49 Wenzek (2019), Penedo (2023).\\n- 50 Zhao (2023).\\n- 51 Rejeleene (2024).\\n- 52 Minaee (2024).\\n- 53 Kudo (2018).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='f19fdc00-0271-459e-925b-c853e6c213e7', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='# Tipologías de embeddings\\n\\nNo son valores universales, sino que varían entre distintos modelos en función del espacio vectorial en el que se hayan definido. Los embeddings se utilizan en los LLM para poder establecer una métrica que defina la similitud entre los significados de las palabras y para incorporar información sobre la posición de las palabras en una oración. Esto es crucial, ya que el orden de las palabras afecta al significado. Existen tres tipos principales de embeddings posicionales:\\n\\n- Embedding posicional absoluto: asigna a cada palabra –o a cada unidad mínima de texto o token– un vector que representa su posición exacta en la oración (p. ej., primera, segunda, tercera posición, etc.).\\n- Embedding posicional relativo: en lugar de basarse en posiciones absolutas, representa la posición de una palabra en relación con las demás (p. ej., dos palabras antes, una palabra después, etc.).\\n- Embedding posicional rotatorio: combina la información de posiciones absolutas y relativas, utilizando funciones trigonométricas para crear representaciones vectoriales más complejas.\\n\\n# Preentrenamiento\\n\\nEl preentrenamiento es una etapa fundamental en el desarrollo de LLM, durante la cual los modelos adquieren un conocimiento general y profundo del lenguaje a partir de grandes cantidades de datos no etiquetados. Aunque este proceso es computacionalmente intensivo y costoso, permite la adaptación del modelo a una amplia gama de tareas.\\n\\nEl objetivo principal del preentrenamiento es que el modelo adquiera un conocimiento amplio y profundo del lenguaje, incluyendo su estructura, semántica, sintaxis y contexto. Durante este proceso, el LLM aprende a predecir palabras o fragmentos de texto (i.e., tokens) basándose en el contexto circundante, lo que le permite capturar relaciones y patrones lingüísticos complejos. Este conocimiento general se convierte en la base sobre la cual el modelo puede ser adaptado posteriormente para tareas específicas mediante el fine-tuning.\\n\\nLa elección del tipo de embedding posicional puede afectar al rendimiento del LLM, ya que determina la cantidad y el tipo de información posicional disponible para el modelo durante el entrenamiento.\\n\\n# Técnicas populares para el preentrenamiento de LLM\\n\\nExisten varias técnicas populares para el preentrenamiento de LLM, como:\\n\\n- El modelado autorregresivo de lenguaje o modelado unidireccional (p. ej., modelado autorregresivo), que consiste en entrenar el modelo para predecir la siguiente palabra o fragmento de texto dado el contexto anterior. Esta tarea permite al modelo aprender las probabilidades condicionales del lenguaje y generar texto coherente. Son ejemplos los modelos GPT y Claude.\\n\\n54Devlin (2018), Liu (2022).\\n\\n1Vaswani (2017).\\n\\n2Shaw (2018).\\n\\n3Su (2021).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='dcd42d8c-044e-450e-b752-c2535d32acbd', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='# El modelo no autorregresivo\\n\\nEl modelo no autorregresivo, usado en modelos como Gemini, en los que no se obtiene la respuesta secuencialmente palabra a palabra, sino que se transforma y refina en conjunto.\\n\\n# Cuantización\\n\\nLas neuronas para realizar predicciones más precisas. Estos pesos se almacenan típicamente como números de alta precisión, lo que puede resultar en modelos de gran tamaño y computacionalmente costosos.\\n\\n# El modelado de lenguaje enmascarado\\n\\nEl modelado de lenguaje enmascarado, popularizado por modelos como BERT, que consiste en enmascarar aleatoriamente algunas palabras en el texto de entrada y entrenar el modelo para predecir estas palabras enmascaradas basándose en el contexto circundante. Esta técnica permite un aprendizaje bidireccional y una mejor comprensión del contexto. Algunas arquitecturas de LLM (p. ej., transformers bidireccionales) usan esta técnica.\\n\\n# La cuantización postentrenamiento\\n\\nLa cuantización postentrenamiento es una técnica que permite reducir la precisión de los parámetros del modelo sin afectar significativamente el rendimiento del modelo. Por ejemplo, redes neuronales que almacenen sus parámetros usando números de coma flotante de 32 bits pueden pasar a usar únicamente 16 bits u 8 bits dependiendo del tipo de cuantización. Esto resulta en modelos más pequeños y rápidos, ya que requieren menos memoria y pueden realizar operaciones más eficientemente con el hardware adecuado.\\n\\n# El modelado secuencia a secuencia\\n\\nEl modelado secuencia a secuencia (p. ej., seq2seq), en el que el modelo se entrena para generar secuencias de texto en función de otras secuencias de entrada. Es usado en modelos como T5, BART o ProphetNET.\\n\\n# El preentrenamiento contrastivo\\n\\nEl preentrenamiento contrastivo, utilizado en modelos como CLIP y ALIGN, que implica entrenar el modelo para identificar pares de texto e imagen que están semánticamente relacionados, lo que le permite aprender representaciones multimodales y transferir conocimiento entre diferentes modalidades.\\n\\n# El preentrenamiento de LLM\\n\\nEl preentrenamiento de LLM es un proceso computacionalmente intensivo que requiere enormes cantidades de datos, tiempo y recursos de hardware. Los modelos más grandes pueden tener en torno a 1 billón (1012) de parámetros y requerir miles de GPU de gama alta durante semanas o meses de entrenamiento. Esto hace que el preentrenamiento sea extremadamente costoso y solo esté al alcance de unas pocas empresas y organizaciones en el mundo con los recursos necesarios.\\n\\n# Referencias\\n\\n55 Xu (2021).\\n\\n56 Devlin (2019), Sinha (2021).\\n\\n57 Lee (2022).\\n\\n58 Sutskever (2014).\\n\\n59 Zeng (2023).\\n\\n60 Jia (2021).\\n\\n61 Cui (2022).\\n\\n62 Li (2024).\\n\\n63 Tian (2024).\\n\\n64 Fu (2024).\\n\\n# Durante el entrenamiento de los LLM\\n\\nDurante el entrenamiento de los LLM, se ajustan los pesos de\\n\\n# Fig. 6. Fine-tuning de un LLM.\\n\\n|Interacción con el usuario|Preentrenamiento|Puesta a punto|\\n|---|---|---|\\n|Datos específicos del ámbito|Conjunto de datos más pequeño con ámbito específico|Respuesta del LLM|\\n|Pregunta del usuario|LLM perfeccionado (por ejemplo, CodeLlama)| |\\n|Base de datos de preentrenamiento (por ejemplo, Wikipedia)| | |', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='9a45359d-021c-4a9d-bf6c-10b81b0e2072', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='# Fine-tuning, instruction-tuning y RAG\\n\\nEl fine-tuning es el proceso de adaptar un LLM preentrenado a una tarea específica utilizando un conjunto de datos más pequeño. Esta técnica permite aprovechar el conocimiento general adquirido durante el preentrenamiento y especializarlo para obtener un alto rendimiento en la tarea objetivo.\\n\\nEl objetivo principal del fine-tuning (Fig. 6) es adaptar un LLM preentrenado a una tarea concreta, como la clasificación de sentimientos, la respuesta a preguntas, la traducción automática o la generación de resúmenes. Durante este proceso, el modelo aprende a utilizar su conocimiento general del lenguaje y aplicarlo de manera efectiva al dominio y los requisitos específicos de la tarea en cuestión. Los LLM disponibles en el mercado, sean propietarios o de código abierto, típicamente están preentrenados (y por tanto son de propósito general), pero no han recibido fine-tuning, que los adaptaría a un propósito específico.\\n\\n# Beneficios del fine-tuning\\n\\n- Aprovecha el conocimiento previo: al partir de un modelo preentrenado, el fine-tuning permite aprovechar el vasto conocimiento general del lenguaje adquirido durante el preentrenamiento, lo que acelera el aprendizaje y mejora el rendimiento en la tarea específica.\\n- Requiere menos datos y recursos: en comparación con el entrenamiento desde cero, el fine-tuning necesita mucha menos cantidad de datos etiquetados y recursos computacionales, lo que lo hace más accesible y económico para una amplia gama de organizaciones y aplicaciones.\\n\\n# Entrenando LLM: funciones de pérdida\\n\\nLos LLM, como otros modelos de aprendizaje profundo, aprenden ajustando sus parámetros para minimizar una función de pérdida. Esta función mide la diferencia entre las predicciones del modelo y los resultados esperados, guiando al modelo hacia un mejor rendimiento.\\n\\nLa elección de la función de pérdida depende del tipo de tarea para la que se esté entrenando el LLM. Por ejemplo, para un modelo que predice la siguiente palabra en una frase (modelado autorregresivo del lenguaje), una función común es la entropía cruzada. Esta función compara la distribución de probabilidad de las palabras predichas por el modelo con la distribución real observada en los datos de entrenamiento.\\n\\nMatemáticamente, la función de pérdida de entropía cruzada para un modelo autorregresivo se puede expresar como una suma de los logaritmos negativos de las probabilidades asignadas a las palabras correctas en cada posición de la secuencia.\\n\\nEn concreto, dada una función de pérdida como la entropía cruzada, y una tipología de entrenamiento como el modelado autorregresivo del lenguaje, se puede definir la función de pérdida a minimizar como:\\n\\ndonde φ representa los parámetros del modelo, i se refiere al número de token en una secuencia determinada con N tokens, P es la probabilidad de predecir el token i en función de la secuencia x de tokens anteriores.\\n\\nDurante el ajuste fino (fine-tuning) de los embeddings del modelo, se pueden utilizar funciones de pérdida especializadas para afinar las representaciones vectoriales de las palabras. Algunas opciones populares incluyen:\\n\\n- Pérdida por similitud del coseno: ajusta los embeddings para que palabras similares tengan vectores más parecidos.\\n- Pérdida por error cuadrático medio: minimiza la diferencia cuadrática entre los embeddings predichos y los esperados.\\n- Pérdida por ranking de múltiples negativos: asocia los embeddings de palabras relacionadas de manera que estén más cerca entre sí que los de palabras no relacionadas.\\n- Pérdida por tripletes, de Matryoshka o contrastiva: variantes más avanzadas que consideran relaciones entre tríos o grupos de embeddings.\\n\\nA pesar de sus beneficios, el fine-tuning también presenta algunos desafíos:\\n\\n- Sobre-especialización: si el modelo se somete a un fine-tuning en un conjunto de datos demasiado específico, puede perder parte de su capacidad de generalización y funcionar mal con datos desconocidos o ligeramente diferentes.\\n\\n65Wang (2024).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='befe1e83-ca00-4d17-89a9-64cd1a2457fd', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='# Fig. 7. Funcionamiento del RAG.\\n\\n|Preentrenamiento|RAG|Interaccion con el usuario|\\n|---|---|---|\\n|Consulta de busqueda|Documentacion especifica del ambito|Recuperacion de datos|\\n|Base de datos de preentrenamiento (por ejemplo; Wikipedia)|LLM de base (por ejemplo; Llama 2)|Respuesta del LLM mejorada por recuperacion|\\n\\n# 4\\n\\nOlvido catastrófico66: durante el fine-tuning es posible que un modelo olvide conocimientos críticos aprendidos previamente.\\n\\n# 4\\n\\nInestabilidad67: el proceso de fine-tuning puede ser sensible a factores como la inicialización de pesos, los hiperparámetros y la selección de datos, lo que puede llevar a resultados inconsistentes o variaciones en el rendimiento.\\n\\n# 4\\n\\nHerencia de sesgos68: los modelos que han recibido fine-tuning pueden heredar y amplificar los sesgos presentes tanto en los datos de preentrenamiento como en los datos de fine-tuning, lo que requiere una cuidadosa consideración y mitigación.\\n\\n# 28\\n\\nExisten varios tipos de fine-tuning que deben seleccionarse en función de cuánto sea necesario modificar el modelo de partida para adecuarlo a una tarea en un dominio más específico. Los principales métodos son:\\n\\n# 4\\n\\nFine-tuning supervisado69: requiere conjuntos de datos etiquetados de entrada y respuesta del LLM, que se usan para mejorar su respuesta ante tareas específicas. Un método popular de fine-tuning supervisado es el llamado instruction-tuning70, que consiste en alinear las respuestas del modelo a lo esperado por sus usuarios a través de interacciones con el modelo.\\n\\n# 4\\n\\nReinforcement learning: métodos basados en aprendizaje por refuerzo que se centran en mejorar la calidad de la respuesta del LLM, en este caso con base en el feedback de usuario o modelos de recompensa (p. ej., optimización directa por preferencia71).\\n\\n# 4\\n\\nFine-tuning no supervisado72: se trata de un método que no requiere conjuntos de datos etiquetados, sino que se basa en reentrenar el modelo con las mismas metodologías usadas durante el preentrenamiento (p. ej., predecir el siguiente token).\\n\\n66 Luo (2024).\\n\\n67 Zhang (2024).\\n\\n68 Zhang (2024).\\n\\n69 Ovadia (2024).\\n\\n70 Zhang (2023).\\n\\n71 Rafailov (2023).\\n\\n72 Zhou (2023).\\n\\n73 Xu (2023).\\n\\n74 Dettmers (2023).\\n\\n75 Chen (2023).\\n\\n76 Lewis (2020) y Neelakantan (2022).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='c59ef383-4290-4807-a523-026e4fa78a7f', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='# Implementación y uso\\n\\n# Monitoreo y mantenimiento\\n\\nUna vez entrenado y validado, el LLM debe desplegarse en un entorno de producción para su uso en aplicaciones reales. Esto conlleva la integración del modelo en sistemas y flujos de trabajo existentes, así como la creación de interfaces y APIs para interactuar con él.\\n\\nEste proceso implica varios aspectos clave, entre los que se incluyen aspectos de integración y de monitoreo.\\n\\n# Integración en sistemas y flujos de trabajo\\n\\nInfraestructura: los LLM suelen ser modelos grandes y computacionalmente intensivos, por lo que requieren una infraestructura robusta para su implementación. Esto puede implicar el uso de hardware especializado, como GPU o TPU, y plataformas de computación en la nube optimizadas para realizar eficientemente el proceso de inferencia.\\n\\nInterfaces y API: para facilitar el uso del LLM en aplicaciones y servicios, es necesario desarrollar interfaces y API que permitan a otros sistemas interactuar con el modelo de manera eficiente y segura. Esto puede incluir endpoints, bibliotecas de cliente en varios lenguajes de programación e interfaces gráficas de usuario para usuarios no técnicos.\\n\\nIntegración con otros componentes: en muchos casos, los LLM forman parte de un sistema más amplio que incluye otros componentes, como bases de datos, servicios de procesamiento de lenguaje natural y aplicaciones de usuario final. La integración fluida y eficiente del LLM con estos componentes es crucial para garantizar un rendimiento y una experiencia de usuario óptimos.\\n\\nComo se puede apreciar, el desarrollo y despliegue de LLM es un proceso complejo y multifacético que requiere una cuidadosa consideración de múltiples aspectos, desde la selección y preparación de los datos hasta la implementación y el uso responsable del modelo. Una comprensión profunda de los componentes clave, como el preentrenamiento, el fine-tuning y los embeddings, así como la conciencia de los desafíos y riesgos asociados, es esencial para aprovechar todo el potencial de los LLM de manera ética, sostenible y rentable, así como alineada con los objetivos de cada organización.\\n\\n77Wan (2024).\\n\\n78Abhyankar (2024).\\n\\n79Goyal (2024).\\n\\n80Lester (2021).\\n\\n81Banerjee (2023).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='efa116fc-485d-49a3-ae91-bcdb0f5a2277', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='# Arquitectura de los LLM\\n\\nLa arquitectura de los LLM se refiere a la estructura y organización de las redes neuronales que componen estos modelos. La elección de la arquitectura y sus componentes tiene un impacto significativo en el rendimiento, la eficiencia y las capacidades del LLM. Esta sección explorará las principales arquitecturas utilizadas en los LLM y sus características, ventajas y limitaciones.\\n\\n# Transformers: el estado del arte en LLM\\n\\nLos transformers también introducen el concepto de atención multi-cabezal (multi-head attention), donde múltiples mecanismos de atención operan en paralelo, lo que permite al modelo capturar diferentes tipos de relaciones y patrones en los datos.\\n\\nLa arquitectura del transformer ha demostrado un rendimiento excepcional en una amplia gama de tareas de procesamiento del lenguaje natural, y ha sido adoptada por la mayoría de los LLM considerados de referencia.\\n\\n# Variantes y extensiones de los transformers\\n\\nLos transformers, introducidos en 2017, se han convertido en la arquitectura dominante para los LLM. A diferencia de las arquitecturas anteriores basadas en redes neuronales recurrentes (RNN) o redes neuronales convolucionales (CNN), los transformers se basan únicamente en mecanismos de atención para procesar y generar secuencias de texto.\\n\\nLa arquitectura del transformer consta de dos componentes principales: el codificador (encoder) y el decodificador (decoder), y existen transformers con solo codificador, solo decodificador o con ambos componentes. El codificador procesa la secuencia de entrada y genera una representación contextual para cada token, mientras que el decodificador genera la secuencia de salida a partir de la representación del codificador y las predicciones anteriores.\\n\\nUna variante popular es el transformer bidireccional, que permite al modelo atender tanto al contexto izquierdo como al derecho de cada token. Esto se logra mediante el uso de un objetivo de preentrenamiento de modelado de lenguaje enmascarado (MLM), donde algunos tokens se enmascaran aleatoriamente y el modelo debe predecirlos basándose en el contexto circundante.\\n\\nOtra variante es el transformer generativo, como GPT, que utiliza un enfoque de modelado de lenguaje unidireccional. Esto permite generar texto de manera eficiente y coherente, ya que el modelo solo puede atender al contexto izquierdo de cada token.\\n\\nTambién se han propuesto extensiones para hacer que los Transformers sean más eficientes y escalables, como el transformer disperso (sparse transformer), que utiliza.\\n\\n# Fig. 8. Funcionamiento de un transformer.\\n\\n|Tokenizador|Incrustaciones|Codificaciones posicionales|\\n|---|---|---|\\n|Consulta|Key|Value|\\n|Consulta de entrada al modelo|Las entradas se dividen en unidades (fichas).|El modelo de incrustación procesa y asigna representaciones numéricas a cada token.|\\n|Se añade información posicional para que el modelo comprenda las relaciones entre las palabras y el contexto.|El codificador utiliza la autoatención para averiguar qué palabras son importantes y cómo se relacionan entre sí, procesando en paralelo los datos en N capas.|El decodificador tiene en cuenta el contexto previo y las partes traducidas utilizando la atención.|\\n|El decodificador toma la información traducida del codificador y genera una nueva respuesta.|La respuesta se invierte y se presenta de forma coherente.| |\\n\\nVaswani (2017).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='f2f5ce90-b5fb-48a4-abe8-6658ef44b378', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='# Prompt engineering en los LLM: principios y mejores prácticas\\n\\nPrompt engineering se refiere al proceso de diseñar y optimizar los prompts (entradas de texto) para obtener los mejores resultados posibles de los LLM. Esta disciplina emergente contiene una serie de principios y mejores prácticas que permiten aprovechar al máximo las capacidades de estos modelos. Entre ellos:\\n\\n- 4 Aprovechar conocimiento externo: proporcionar información adicional permite complementar la base de conocimientos del LLM. Por ejemplo, para responder preguntas sobre una industria específica, se podrían primero recuperar informes sectoriales relevantes y pasarlos al modelo.\\n- 4 Ser claro y específico: las instrucciones dadas al modelo deben indicar de forma explícita el formato, la longitud y el nivel de detalle esperado en la respuesta. Por ejemplo, en lugar de simplemente pedir \"Analiza la situación financiera de la empresa X\", es mejor dar una instrucción como \"Escribe un informe de 1000 palabras sobre la situación financiera de la empresa X, cubriendo su rentabilidad, liquidez, solvencia y perspectivas futuras\".\\n- 4 Iterar y refinar sistemáticamente: la evaluación continua del rendimiento del modelo permite identificar áreas de mejora y ajustar los prompts en consecuencia. Métricas cuantitativas y juicios cualitativos de expertos del dominio pueden guiar este proceso iterativo.\\n- 4 Descomponer tareas complejas: conviene dividir los problemas en subtareas más manejables para los LLM. Por ejemplo, en lugar de pedir \"Desarrolla un plan estratégico para la empresa Y\", se pueden solicitar subtareas como \"Realiza un análisis DAFO de la empresa Y\", \"Define los objetivos estratégicos clave para Y\", \"Propón iniciativas para alcanzar cada objetivo\", etc.\\n- 4 Proporcionar ejemplos ilustrativos (few-shot learning): unos pocos ejemplos bien elegidos pueden ayudar enormemente a comunicar la tarea deseada. Supongamos que se quiere generar propuestas de valor para productos; se podrían dar dos ejemplos: \"Nuestro software CRM permite a los equipos de ventas cerrar acuerdos un 50% más rápido\" y \"Nuestra app de bienestar ayuda a los empleados a reducir el estrés y aumentar su productividad en un 25%\".\\n- 4 Solicitar razonamiento paso a paso: instruir al LLM para que verbalice su proceso de pensamiento a menudo conduce a resultados más robustos. Esto es especialmente útil para tareas de análisis o solución de problemas en el ámbito empresarial. Por ejemplo: \"Describe paso a paso cómo calcularías el ROI de este proyecto de inversión\".\\n- 4 Solicitar las referencias empleadas: instruir al LLM para que indique en su proceso de razonamiento las referencias a documentos que ha empleado, incluyendo citas del texto original al que tiene acceso.\\n- 4 Pedir al LLM que adopte un personaje: antes de la tarea principal, se puede primero instruir al modelo para que adopte un determinado rol, tono o estilo. Por ejemplo: \"Actúa como un experto analista financiero y proporciona una valoración objetiva de la empresa X\". Esto ayuda a orientar su comportamiento.\\n\\nMediante la aplicación de estos principios de prompt engineering, se demuestra estadísticamente que los LLM proporcionan un resultado más preciso y fiable.\\n\\nFuentes: Guía de prompt engineering de OpenAI, soporte de Anthropic Claude Opus y elaboración propia.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='6bfe673b-24ee-4c6a-a4d0-6b57ed83f402', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='# El auge de los large language models: de los fundamentos a la aplicación\\n\\n# atención dispersa para reducir la complejidad\\n\\nescalabilidad, la interpretabilidad y la eficiencia de estos modelos. A medida que la investigación avance, es probable que aparezcan nuevas arquitecturas y técnicas que superen estas limitaciones y lleven a los LLM a nuevas cotas de rendimiento y capacidad.\\n\\n# Comparación con arquitecturas anteriores\\n\\nAntes de los transformers, las arquitecturas dominantes para el modelado de secuencias eran las redes neuronales recurrentes (RNN), como Long Short-Term Memory (LSTM) y Gated Recurrent Unit (GRU), y las redes neuronales convolucionales (CNN).\\n\\n# 4\\n\\nLas RNN pueden capturar dependencias a largo plazo en las secuencias, pero sufren de problemas como el desvanecimiento del gradiente (vanishing gradient) y la dificultad para paralelizar el entrenamiento. Además, las RNN tienen dificultades para capturar dependencias muy largas debido a su naturaleza secuencial y al uso de recurrencias de alcance constante.\\n\\n# 4\\n\\nLas CNN pueden capturar patrones locales en las secuencias y son eficientes en términos de computación, pero tienen dificultades para modelar dependencias a largo plazo y requieren un tamaño de contexto fijo.\\n\\nEn comparación, los transformers superan estas limitaciones al utilizar mecanismos de atención que pueden capturar dependencias a largo plazo de manera eficiente y paralela. Además, los transformers son más flexibles en términos de manejo de secuencias de longitud variable y pueden ser preentrenados en grandes cantidades de datos no etiquetados.\\n\\nLa arquitectura de los transformers ha revolucionado el campo de los LLM y ha permitido avances significativos en una amplia gama de tareas de procesamiento del lenguaje natural. Sin embargo, aún quedan desafíos por abordar, como la:\\n\\n|4|Gestión de grandes volúmenes de datos:|los LLM requieren cantidades masivas de datos de entrenamiento, lo que implica la necesidad de infraestructuras de almacenamiento y procesamiento escalables y eficientes.|\\n|---|---|---|\\n|4|Escalado de recursos computacionales:|el entrenamiento y la inferencia de LLM demandan enormes recursos de cómputo, haciendo necesario el uso de técnicas de|', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='9b7e3637-ff85-4942-9fba-0cbdf7064a27', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='# Sesgos, alucinaciones y confiabilidad\\n\\nUno de los mayores retos de los LLM es la presencia de sesgos y alucinaciones en sus resultados y predicciones. Los sesgos pueden surgir de varias fuentes, como la parcialidad en los datos de entrenamiento, las limitaciones de las arquitecturas de los modelos o los prejuicios humanos implícitos en las tareas de anotación y evaluación. Por otro lado, las alucinaciones se refieren a la generación de información o contenido que parece plausible pero que no se basa en hechos reales o en el conocimiento adquirido durante el entrenamiento.\\n\\nLos sesgos en los LLM pueden manifestarse de diversas maneras, como la perpetuación de estereotipos de género, raza o edad, la discriminación en tareas de clasificación o la generación de contenido ofensivo o inapropiado. Estos sesgos pueden tener consecuencias graves, especialmente cuando los LLM se utilizan en aplicaciones sensibles como la toma de decisiones en el ámbito legal, financiero o médico. Por su parte, las alucinaciones pueden llevar a la difusión de información errónea o engañosa, lo que puede tener un impacto negativo en la confianza de los usuarios y en la credibilidad de las aplicaciones basadas en LLM.\\n\\nAdemás, LLMOps promueve prácticas como la automatización de procesos, el testing continuo, la documentación exhaustiva y la gobernanza de modelos. Esto permite no solo mejorar la eficiencia y calidad del desarrollo de los LLM, sino también garantizar su uso ético y responsable.\\n\\n# Retos\\n\\nEl desarrollo y despliegue de LLM presenta una serie de retos significativos que deben abordarse para garantizar su uso responsable, ético y seguro. Esta sección explorará varios de los principales desafíos para las organizaciones en su despliegue y uso de los LLM.\\n\\nPara abordar estos desafíos, LLMOps se apoya en una serie de herramientas y frameworks específicos, como MLFlow83, CometML84 y Weights & Biases85. Estas plataformas ofrecen funcionalidades para el seguimiento de experimentos, gestión de modelos, monitorización del rendimiento y colaboración entre equipos.\\n\\nPara abordar el reto de los sesgos, es necesario desarrollar técnicas robustas para detectar, medir y mitigar su presencia en los LLM. Esto implica la creación de conjuntos de datos de evaluación específicos para sesgos, el uso de métricas de equidad y la aplicación de técnicas de eliminación de sesgos (debiasing) tanto en el preentrenamiento como en el fine-tuning. Además, es crucial establecer procesos de auditoría y monitoreo continuo para garantizar que los LLM sigan siendo imparciales a lo largo del tiempo.\\n\\n83 Zaharia (2018).\\n\\n84 CommetML: https://www.comet.com/\\n\\n85 Weights and biases: https://wandb.ai/site', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='cc2d6da7-c5c9-4e5a-8e8c-e9a8fe8ea7c5', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='# Para abordar las alucinaciones en los LLM, se están\\n\\ndesarrollando diversos métodos centrados en la mejora de los datos de entrenamiento, la aplicación de técnicas de regularización robustas y el uso de retroalimentación humana para ajustar las respuestas del modelo. Además, se están investigando cambios arquitectónicos en los modelos para hacerlos inherentemente menos propensos a alucinar. Los métodos de generación de texto y el contexto de entrada también pueden ser optimizados para reducir las alucinaciones. La supervisión humana y la evaluación rigurosa son esenciales para detectar y corregir información inexacta. Asimismo, el desarrollo de herramientas específicas, como modelos de evaluación de alucinaciones y técnicas de ofuscación, puede contribuir a mejorar la precisión de los LLM.\\n\\n# MANAGEMENT SOLUTIONS\\n\\nConfidencialidad y protección de la información\\n\\nLos LLM a menudo se entrenan con grandes cantidades de datos que pueden contener información personal, sensible o confidencial. Además, cuando se despliegan en aplicaciones del mundo real, estos modelos pueden estar expuestos a entradas de usuario que también pueden incluir datos privados.\\n\\n# Explicabilidad y accountability\\n\\nEl auge de los large language models: de los fundamentos a la aplicación. Esto plantea importantes desafíos de privacidad y seguridad, ya que los LLM pueden memorizar y reproducir información confidencial de sus datos de entrenamiento, o ser vulnerables a ataques que intenten extraer datos privados a través de consultas cuidadosamente diseñadas.\\n\\nEsta falta de transparencia plantea problemas de accountability, especialmente cuando los LLM se utilizan en contextos de alta sensibilidad, donde las decisiones tienen un impacto significativo en las personas (p. ej., uso de LLM en medicina, investigación farmacéutica, infraestructura crítica o acceso al mercado laboral). Sin una comprensión clara de cómo funcionan estos modelos, es difícil determinar la responsabilidad en caso de errores o comportamientos no deseados.\\n\\nPara abordar este reto, es necesario desarrollar técnicas y herramientas que permitan una mayor interpretabilidad y explicabilidad de los LLM. Esto incluye métodos para visualizar y analizar los mecanismos internos de atención, técnicas de atribución para identificar las partes más relevantes de la entrada, y enfoques para generar explicaciones en lenguaje natural de las predicciones del modelo.\\n\\nPara abordar este reto, es necesario desarrollar técnicas de preservación de la privacidad en el entrenamiento y despliegue de LLM (p. ej., Digger86 para detectar información protegida, el uso de datos ficticios87 durante el entrenamiento para detectar el material con copyright).\\n\\nAdemás, es crucial establecer protocolos robustos de seguridad y control de acceso para proteger los LLM y sus datos asociados de accesos no autorizados o usos maliciosos. Esto puede implicar el uso de técnicas de autenticación y autorización, monitoreo de seguridad y detección de anomalías.\\n\\n86 Li (2024).\\n\\n87 Meeus (2024).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='3c8368ca-2ebc-447e-acdc-0fbdc904925b', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='# Consumo racional de los recursos\\n\\nEl entrenamiento y despliegue de LLM requiere cantidades masivas de recursos computacionales, almacenamiento y energía. Con modelos que alcanzan los cientos de miles de millones o incluso los billones de parámetros, el coste financiero y ambiental de desarrollar y operar estos sistemas puede ser muy significativo88.\\n\\n# Riesgos de seguridad y uso malicioso\\n\\nLos LLM pueden ser vulnerables a ataques adversarios, como la inyección de datos envenenados o la ingeniería inversa. Además, pueden utilizarse de forma maliciosa para generar desinformación, spam o contenido engañoso. Es esencial establecer medidas de seguridad robustas y diseñar los modelos con salvaguardias contra el uso indebido.\\n\\n# Cuestiones de propiedad intelectual y licencias\\n\\nEl uso de LLM plantea preguntas sobre la propiedad intelectual y las licencias de los datos de entrenamiento, los modelos y los resultados generados. Adicionalmente, existe un riesgo de robo de información o datos personales de usuarios que lancen consultas a LLM desplegados en nubes de terceros. Es necesario cumplir con la regulación y con los marcos éticos para equilibrar los derechos de los creadores, los usuarios y el interés público, y, en el caso de las organizaciones, para evitar riesgos legales y de cumplimiento.\\n\\n# Escalabilidad de la arquitectura de los LLM\\n\\nUn desafío adicional es la escalabilidad de los transformers a medida que aumenta el tamaño de las secuencias y los modelos. Los mecanismos de atención tienen una complejidad cuadrática con respecto a la longitud de la secuencia, lo que limita su aplicación a secuencias muy largas.\\n\\nTambién se está investigando en la mejora de las técnicas de preentrenamiento continuo89 y fine-tuning continuo90, que buscan integrar la capacidad de usar información de diversos dominios sin la necesidad de depender de un reentrenamiento exhaustivo y costoso con nuevos datos específicos. Asimismo, se está avanzando en el uso de sistemas innovadores y el diseño de algoritmos verdes de IA, que permiten hacer frente a los costes computacionales y ambientales asociados a la IA (p. ej., el sistema GreenLightningAI, de Qsimov Quantum Computing91, desarrolla el reentrenamiento incremental y proporciona una interpretabilidad directa).\\n\\n# Desarrollo de infraestructuras y plataformas sostenibles\\n\\nOtra dirección es el desarrollo de infraestructuras y plataformas de computación más sostenibles, como el uso de hardware especializado de bajo consumo, sistemas de enfriamiento más eficientes y fuentes de energía renovables para alimentar los centros de datos donde se entrenan y despliegan los LLM.\\n\\nAdemás, es importante promover prácticas de uso racional y compartido de los recursos, como la reutilización y adaptación de modelos preentrenados en lugar de entrenar nuevos modelos desde cero para cada tarea, y el intercambio de recursos y conocimientos entre organizaciones y comunidades de investigación.\\n\\n# Otros desafíos\\n\\nDe entre los muchos retos adicionales a los que se enfrentan las organizaciones en el desarrollo, implementación y uso de los LLM, por su importancia cabe mencionar brevemente:\\n\\n# Dependencia y lock-in\\n\\nLas organizaciones que dependen de LLM proporcionados por terceros pueden enfrentarse a riesgos de dependencia y lock-in, especialmente si los modelos se basan en datos o infraestructura propietaria. Es importante considerar estrategias de diversificación y planes de contingencia.\\n\\n88*iDanae 1T24 (2024).*\\n\\n89*Yıldız (2024).*\\n\\n90*Mehta (2023).*\\n\\n91*iDanae 1T24 (2024).*\\n\\n92*Pankajakshan (2024).*\\n\\n93*Rae (2021).*', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='387061bc-2c30-464a-a75a-a08091f6e0ab', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='# Marco de validación de los LLM\\n\\n“Las consecuencias de que la IA vaya mal son graves, por lo que debemos ser proactivos en lugar de reactivos“.\\n\\nElon Musk94\\n\\n# MANAGEMENT SOLUTIONS\\n\\n# El auge de los large language models: de los fundamentos a la aplicación', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='f67fa106-aa98-4696-a125-9f16443c7458', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"Los modelos de lenguaje de gran escala (LLM) ofrecen un gran potencial para transformar diversos sectores y aplicaciones, pero también conllevan riesgos significativos que deben abordarse. Estos riesgos incluyen la generación de información errónea o alucinaciones, la perpetuación de sesgos, la dificultad para olvidar la información aprendida, preocupaciones éticas y de equidad, problemas de privacidad por uso indebido, dificultades en la interpretación de los resultados, y la potencial creación de contenido malicioso, entre otros.\\n\\nDado el impacto potencial de estos riesgos, es necesario validar exhaustivamente los LLM antes de su despliegue en entornos de producción. De hecho, la validación de los LLM no es solo una buena práctica, sino también un requisito regulatorio en muchas jurisdicciones. En Europa, la propuesta de AI Act exige una evaluación y mitigación de los riesgos de los sistemas de IA, mientras que, en Estados Unidos, el marco de gestión de riesgos de IA del NIST y el AI Bill of Rights destacan la importancia de comprender y abordar los riesgos inherentes a estos sistemas.\\n\\nLa validación de los LLM puede partir de los principios establecidos en la disciplina de riesgo de modelo, que se centra en evaluar y mitigar los riesgos derivados de errores, deficiente implementación o mal uso de los modelos. Sin embargo, en el caso de la IA, y particularmente de los LLM, es necesario adoptar una perspectiva más amplia que abarque los otros riesgos que comportan. Un enfoque integral de validación es esencial para garantizar un despliegue seguro y responsable de los LLM.\\n\\n# 4 Datos de entrada\\n\\ncomprensión del texto, calidad del dato.\\n\\n# 4 Solidez conceptual y diseño del modelo\\n\\nselección del modelo y sus componentes (p. ej., metodologías de fine-tuning, conexiones a bases de datos, RAG), y comparación con otros modelos.\\n\\nEste enfoque holístico se plasma en un marco de validación multidimensional para los LLM, que cubre aspectos clave como el riesgo de modelo, la gestión de datos y privacidad, la ciberseguridad, los riesgos legales y de cumplimiento normativo, los riesgos operativos y tecnológicos, la ética y la reputación, y el riesgo de proveedor, entre otros.\\n\\nManagement Solutions (2014). Model Risk Management: Quantitative and Qualitative Aspects.\\n\\nImperial et al. (2023).\\n\\nWettig et al. (2024).\\n\\nRAG (Retrieval-Augmented Generation) es una técnica avanzada en la que un modelo de lenguaje busca información relevante de una fuente externa antes de generar texto. Esto enriquece las respuestas con conocimientos precisos y actuales, combinando inteligentemente la búsqueda de información y la generación de texto. Al integrar datos de fuentes externas, los modelos RAG, como los RAG-Token y RAG-Sequence propuestos, ofrecen respuestas más informadas y coherentes, minimizando el riesgo de generar contenido inexacto o 'alucinaciones'. Este avance representa un paso significativo hacia modelos de inteligencia artificial más confiables y basados en evidencia real.\\n\\nKhang (2024).\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='8bb92086-a5ba-4262-b08a-051d8e5f5b9a', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='# Fig. 9. Riesgos asociados a la IA y referencia regulatoria en el AI Act.\\n\\n|Compliance & Legal Risk|Compliance & Legal Risk|\\n|---|\\n|Ley AI Act Art. 8, 9|Compliance with AI Act, GDPR, ethical AI frameworks, intellectual property|\\n|Riesgo de Modelo|OpRisk, riesgo informático y ciberseguridad|\\n|Ley AI Act Art. 8, 9, 10, 14, 15, 29|Política de GRM, inventario, directrices de validación, clasificación de riesgos, XAI y detección de sesgos.|\\n|Riesgo de IA|Riesgo del proveedor|\\n|Ley AI Act Art. 8, 9, 12|Selección de terceros, ética del proveedor con respecto a la IA, integración de IA, derechos de autor|\\n|Riesgo Reputacional|ESG y Riesgo Reputacional|\\n|Ley AI Act Art. 8, 29a|Ética, equidad, impacto medioambiental, impacto social, reputación|\\n|Gestión y privacidad de datos|Ley AI Act Art. 8, 10|\\n|Transparencia, consentimiento para el uso de IA, anonimización, mantenimiento de registros, sesgo en los datos, envenenamiento de datos|Transparencia, consentimiento para el uso de IA, anonimización, mantenimiento de registros, sesgo en los datos, envenenamiento de datos|\\n\\n# 4 Evaluación del modelo y análisis de sus resultados:\\n\\nprivacidad y seguridad de los resultados102, precisión del modelo103, consistencia104, robustez105, adaptabilidad106, interpretabilidad (XAI)107, ética, sesgos y equidad108, toxicidad109, comparación contra modelos challenger.\\n\\n# 4 Implementación y uso:\\n\\nrevisión humana en el uso (incluyendo el monitoreo de usos indebidos), resolución de errores, escalabilidad y eficiencia, aceptación del usuario.\\n\\n# 4 Gobernanza110 y ética111:\\n\\nmarco de gobierno de la IA generativa, incluyendo los LLM.\\n\\n# 4 Documentación112:\\n\\ncompletitud de la documentación del modelo.\\n\\n# 38 El auge de los large language models: de los fundamentos a la aplicación\\n\\n4 Cumplimiento regulatorio113: evaluación de los requisitos regulatorios (p. ej., AI Act).\\n\\nPara garantizar el uso efectivo y seguro de los modelos de lenguaje, es fundamental realizar una evaluación de riesgos que considere tanto el modelo en sí como su uso específico. Esto asegura que, independientemente de su origen (in-house o de un proveedor) o personalización (fine-tuning), el modelo funcione adecuadamente en su contexto de uso, cumpliendo con los estándares de seguridad, ética y regulación necesarios.\\n\\nTécnicas de validación Cuando una organización se plantea implementar un LLM para un caso de uso específico, puede ser beneficioso adoptar un enfoque integral que abarque las dimensiones clave del ciclo de vida del modelo: datos, diseño, evaluación, implementación y uso. Asimismo, de manera transversal, resulta necesario evaluar el cumplimiento de la normativa aplicable, como el AI Act en la Unión Europea.\\n\\n102Nasr (2023). 103Liang (2023). 104Elazar (2021). 105Liu (2023). 106Dun (2024). 107Singh (2024). 108NIST (2023), Oneto (2020) y Zhou (2021). 109Shaikh (2023). 110Management Solutions (2014). Model Risk Management. 111Oneto (2020). 112NIST (2023). 113European Parliament (2024). AI Act.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='e578f20e-865b-44a4-8c13-d68f1f990839', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='# Fig. 10. Pruebas de evaluación de LLM.\\n\\n|Dimensiones|Aspectos|Descripción|Métricas de validación (ejemplos)|Evaluación humana (ejemplos)|\\n|---|---|---|---|---|\\n|1. Datos de entrada|1.1 Calidad de dato|Grado de calidad de la modelización o de los datos de aplicación|• Flesch-Kinkaid Grade • Revisión de los elementos del LLM: RAG, filtros de entrada o salida, definición de prompts, fine-tuning, optimización, etc.|• Revisión caso a caso|\\n|2. Diseño del modelo|2.1 Diseño del modelo|Elección de modelos y metodología adecuadas|• Pruebas A/B|• Comparación contra otros LLM|\\n|3. Evaluación del modelo|3.1 Privacidad y seguridad|Respeto de la confidencialidad y no regurgitación de información personal|• Data leakage • PII tests, K-anonymity|• Registros • Hacking ético|\\n|3.2 Precisión|Corrección y pertinencia de las respuestas del modelo|• Traducción: BLEU, Ruby, ROUGE-L • Otros: Sistemas de QA, nivel de overrides, nivel de alucinaciones, etc. • Benchmarks: XSUM, LogiQA, WikiData, etc.|• Revisión caso a caso| |\\n|3.3 Consistencia|Respuestas uniformes a consultas similares|• Cosine similarity • Jaccard similarity index|• Revisión caso a caso • Pruebas A/B| |\\n|3. Evaluación del modelo|3.4 Robustez|Resiliencia a la información adversa o engañosa|• Generación de texto adversario (TextFooler), patrones Regex • Benchmarks de ataques adversarios (PromptBench), número de refusals|• Hacking ético • Simulacros de incidentes|\\n|3.5 Adaptabilidad|Capacidad para aprender o adaptarse a nuevos contextos|• Rendimiento del LLM ante datos nuevos por Zero/One/Few-shot learning|• Pruebas A/B • Revisión caso a caso| |\\n|3.6 Explicabilidad|Comprensión del proceso de toma de decisiones|• SHAP • Puntuaciones de explicabilidad|• Hacking ético • Focus groups| |\\n|3. Evaluación del modelo|3.7 Sesgos y equidad|Respuestas sin sesgo demográfico|• WEAT score, paridad demográfica, asociaciones de palabras, etc. • Benchmarks de sesgos (BBQ, etc.)|• Hacking ético • Focus groups|\\n|3.8 Toxicidad|Propensión a generar contenidos nocivos|• Perspective API, Hatebase API • Toxicity benchmarks (RealToxicityPrompts, BOLD, etc.)|• Hacking ético • Focus groups| |\\n|4. Implementación y uso|4.1 Revisión humana y seguridad de uso|Exclusión de sugerencias perjudiciales o ilegales e inclusión de una revisión humana (‘human-in-the-loop’)|• Protocolos de riesgos, evaluaciones de seguridad • Control humano|• Hacking ético • Focus groups|\\n|4.2 Recuperación y gestión de errores|Capacidad para recuperarse de errores y gestionar entradas inesperadas|• Tests de recuperación del sistema • Métricas de procesamiento de errores|• Simulacros de incidentes| |\\n|4. Implementación y uso|4.3 Escalabilidad|Mantenimiento del rendimiento con más datos o usuarios|• Stress testing del sistema, Apache Jmeter, etc. • Benchmarks de escalabilidad|• Simulacros de incidentes • Pruebas A/B|\\n|4.4 Eficiencia|Utilización de recursos y velocidad de respuesta|• Time-to-first-byte (TTFB), uso de GPU/CPU, inferencia de emisiones, memoria, latencia|• Simulacros de incidentes| |\\n|4.5 Aceptación del usuario|Pruebas de aceptación de usuario|• Checklist de requisitos de usuario, opt-out del usuario • Satisfacción del usuario (Net Promoter Score, CSAT)|• UX tracking • Pruebas A/B| |', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='9389c30e-008f-4f59-955c-06be94d163f7', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='La selección exacta de técnicas dependerá de las características particulares del caso de uso; y, en concreto, varios factores importantes a tener en cuenta para decidir las técnicas más adecuadas son:\\n\\n# 4\\n\\nEl nivel de riesgo y la criticidad de las tareas que se confiarán al LLM.\\n\\n# 4\\n\\nSi el LLM está abierto al público (y por tanto el hacking ético cobra especial relevancia) o si su uso se limita al ámbito interno de la organización.\\n\\n# 4\\n\\nSi el LLM procesa datos personales.\\n\\n# 4\\n\\nLa línea de negocio o servicio que utilizará el LLM.\\n\\nUn análisis cuidadoso de estos drivers permitirá construir un marco de validación robusto y adaptado a las necesidades de cada uso de un LLM.\\n\\n# Métricas de evaluación cuantitativas\\n\\nAunque es un campo de estudio emergente, existe una amplia gama de métricas cuantitativas para evaluar el rendimiento de los LLM. Algunas de estas métricas son adaptaciones de las utilizadas en modelos tradicionales de aprendizaje automático, como la precisión, la exhaustividad (recall), la puntuación F1 o el área bajo la curva ROC (AUC-ROC). Otras métricas han sido diseñadas específicamente para evaluar aspectos únicos de los LLM, como la coherencia del texto generado, la fidelidad a los hechos o la diversidad del lenguaje.\\n\\nEn este sentido, ya existen marcos holísticos de testeo cuantitativo de LLM en entornos de programación en Python, que facilitan la implementación de muchas de las métricas cuantitativas de validación, por ejemplo:\\n\\n# El auge de los large language models: de los fundamentos a la aplicación\\n\\nFig. 11. Algunas técnicas de evaluación humana de LLM.\\n\\n# 4\\n\\nLLM Comparator114: herramienta creada por investigadores de Google para la evaluación automática y comparación de LLM, que revisa la calidad de las respuestas de los LLM.\\n\\n# 4\\n\\nHELM115: evaluación holística de los modelos del lenguaje, que compila métricas de evaluación a lo largo de siete dimensiones (precisión, calibración, robustez, equidad, sesgos, toxicidad y eficiencia) para una serie de escenarios predefinidos.\\n\\n# 4\\n\\nReLM116: sistema de validación y consulta de LLM mediante uso del lenguaje, incluyendo evaluaciones de modelos lingüísticos, memorización, sesgos, toxicidad y comprensión del lenguaje.\\n\\nEn la actualidad, ciertas técnicas de validación, como los métodos de explicabilidad (XAI) basados en SHAP, algunas métricas como ROUGE117 o los análisis de imparcialidad mediante paridad demográfica, aún no cuentan con umbrales predefinidos ampliamente aceptados. En estos casos, es tarea de la comunidad científica y de la industria seguir investigando para establecer criterios claros que permitan una validación robusta y estandarizada.\\n\\n# Backtest de anulaciones\\n\\nContabilizar y medir la importancia de las modificaciones humanas en los resultados del LLM.\\n\\n# Grupos de discusión\\n\\nRecoger opiniones sobre los resultados del LLM de diversos usuarios (en materia de ética, adecuación cultural, discriminación, etc.).\\n\\n# Comprobación caso por caso\\n\\nComparar una muestra representativa (por ejemplo, un mínimo de 200 a través de la prueba Z1) de respuestas LLM con resultados humanos (\"ground truth\"), incl. doble ciego.\\n\\n# Hacking ético (también conocido como Equipo Rojo)\\n\\nManipular los mensajes para forzar al LLM a producir resultados no deseados (incl. regurgitación de PII, cumplimiento, ingeniería de mensajes, pruebas de penetración, vulnerabilidades de IA, etc.).\\n\\n# Pruebas A/B\\n\\nRealizar ensayos paralelos para evaluar diferentes versiones (A y B) o comparar con el rendimiento humano.\\n\\n# Seguimiento de la experiencia del usuario (UX)\\n\\nObservar y evaluar las interacciones de los usuarios con el LLM a lo largo del tiempo / en tiempo real.\\n\\n# Simulacros de incidentes\\n\\nSimular escenarios adversos para probar la respuesta y recuperación del LLM (prueba de estrés, comprobación de copias de seguridad, medición del tiempo de recuperación, etc.).\\n\\n# Mantenimiento de registros\\n\\nRevisar los diarios y registros del sistema LLM, garantizando el cumplimiento de la normativa.\\n\\n114Kahng (2024).\\n\\n115Liang (2023).\\n\\n116Kuchnik (2023).\\n\\n117Duan (2023).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='a56703d2-d247-4154-a2df-1f4ff1886e85', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='# Técnicas de evaluación humana\\n\\n# Benchmarks de evaluación de LLM\\n\\nMientras que las métricas de evaluación cuantitativa son implementables de forma más directa debido a la multitud de recursos online y publicaciones de los últimos años, las técnicas de evaluación humana118 son variadas y deben ser construidas en función de la tarea específica119 que esté realizando el LLM, e incluyen (Fig. 11):\\n\\n- Backtest de los forzajes del usuario: contabilizar y medir la importancia de las modificaciones humanas en los resultados del LLM (p. ej., cuántas veces un gestor comercial debe modificar manualmente los resúmenes de llamadas a clientes que ha realizado un LLM).\\n- Revisión caso a caso: comparar una muestra representativa de respuestas del LLM con las expectativas del usuario («ground truth“).\\n- Hacking ético (Red Team): manipular los prompts para forzar al LLM a producir resultados no deseados (p. ej., regurgitación de información personal, contenido ilegal, tests de penetración, explotación de vulnerabilidades).\\n- Testeo A/B: comparación para evaluar dos versiones del LLM (A y B), o de un LLM frente a un ser humano.\\n- Focus groups: recabar opiniones de diversos usuarios sobre el comportamiento del LLM, p. ej., en materia de ética, adecuación cultural, discriminación, etc.\\n- Experiencia del usuario (UX tracking): observar y evaluar las interacciones de los usuarios con el LLM a lo largo del tiempo o en tiempo real.\\n- Simulacros de incidentes: simular escenarios adversos para probar la respuesta del LLM (p. ej., prueba de estrés, comprobación de copias de seguridad, medición del tiempo de recuperación, etc.).\\n- Mantenimiento de registros: revisar los diarios y registros del sistema LLM, garantizando el cumplimiento de la normativa y la traza de auditoría.\\n\\n# Benchmarks\\n\\nLa mayoría de los modelos de inteligencia artificial generativa, incluidos los LLM, se someten a pruebas utilizando benchmarks públicos que evalúan su desempeño en una variedad de tareas relacionadas con la comprensión y el uso del lenguaje natural. Estas pruebas sirven para medir cómo maneja el LLM tareas específicas y refleja el entendimiento humano. Algunos de estos benchmarks incluyen:\\n\\n- GLUE/SuperGLUE: evalúa la comprensión del lenguaje a través de tareas que miden la capacidad de un modelo para entender el texto.\\n- Eleuther AI Language Model Evaluation Harness: realiza una evaluación “few-shot” de los modelos, es decir, su precisión con muy pocos ejemplos de entrenamiento.\\n- ARC (AI2 Reasoning Challenge): pone a prueba la habilidad del modelo para responder preguntas de ciencia que requieren razonamiento.\\n- HellaSwag: evalúa el sentido común del modelo a través de tareas que requieren predecir el final coherente de una historia.\\n- MMLU (Massive Multitask Language Understanding): prueba la precisión del modelo en una amplia gama de tareas para evaluar su comprensión multitarea.\\n- TruthfulQA: desafía al modelo a discernir entre información verdadera y falsa, evaluando su habilidad para manejar datos verídicos.\\n- Winogrande: otra herramienta para evaluar el sentido común, similar a HellaSwag pero con diferentes métodos y énfasis.\\n- GSM8K: evalúa la capacidad lógico-matemática del modelo a través de problemas de matemáticas diseñados para estudiantes.\\n\\n118Datta, Dickerson (2023).\\n\\n119Guzmán (2015).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='e70aa960-1201-4337-bc7e-6804ff070e97', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='# Nuevas tendencias\\n\\nEl campo de la validación de LLM se encuentra en constante evolución, impulsado por los rápidos avances en el desarrollo de estos modelos y por la creciente conciencia sobre la importancia de garantizar su fiabilidad, equidad y alineación con la ética y la regulación.\\n\\nA continuación, se presentan algunas de las principales tendencias emergentes en este ámbito:\\n\\n# 4. Explicabilidad de los LLM\\n\\na medida que los LLM ganan en complejidad y opacidad, crece la demanda de mecanismos que permitan entender y explicar su funcionamiento interno. Las técnicas de XAI (eXplainable AI) como SHAP, LIME o la atribución de importancia a los tokens de entrada están ganando protagonismo en la validación de LLM. Aunque para los modelos tradicionales hay una variedad de técnicas post-hoc disponibles para comprender el funcionamiento de los modelos a nivel local y global120 (p. ej., Anchors, PDP, ICE), y ha proliferado la definición e implementación de modelos inherentemente interpretables por construcción, la implementación de estos principios para los LLM no está todavía resuelta.\\n\\n# 4. Uso de LLM para explicar LLM\\n\\nuna tendencia emergente consiste en utilizar un LLM para generar explicaciones sobre el comportamiento o las respuestas de otro LLM. En otras palabras, se emplea un modelo del lenguaje para interpretar y comunicar de forma más comprensible el razonamiento subyacente de otro modelo. Para enriquecer estas explicaciones, se están desarrollando herramientas121 que incorporan además técnicas de análisis post-hoc.\\n\\n# 4. Técnicas de interpretabilidad post-hoc\\n\\nestas técnicas se basan en la interpretabilidad de los resultados en la etapa posterior al entrenamiento o fine-tuning, y permiten identificar qué partes de la entrada han influido más en la respuesta del modelo (importancia de características), encontrar ejemplos similares en el conjunto de datos de entrenamiento (similitud basada en embeddings) o diseñar prompts específicos que guíen al modelo hacia explicaciones más informativas (estrategias de prompting).\\n\\n# 4. Puntuaciones por atribución\\n\\ncomo parte de la interpretabilidad post-hoc, se están desarrollando técnicas122 que permiten identificar qué partes del texto de entrada tienen mayor influencia en la respuesta generada por un LLM. Ayudan a entender qué palabras o frases son más importantes para el modelo. Existen diferentes métodos para calcular estas puntuaciones:\\n\\n- Métodos basados en el gradiente: analizan cómo cambian los gradientes (una medida de sensibilidad) para cada palabra al retroceder por la red neuronal.\\n- Métodos basados en perturbaciones: modifican ligeramente el texto de entrada y observan cómo cambia la respuesta del modelo.\\n- Interpretación de métricas internas: utilizan métricas calculadas por el propio modelo, como los pesos de atención en los transformers, para determinar la importancia de cada palabra.\\n\\n# Resumen de la salida\\n\\n”Todavía se está evaluando el coste total de los daños en Newton Stewart, una de las zonas más afectadas. La Primera Ministra, Nicola Sturgeon, visitó la zona para inspeccionar los daños. El líder adjunto del Partido Laborista Escocés, Alex Rowley, estuvo el lunes en Hawick para ver la situación de primera mano. Afirmó que era importante aplicar correctamente el plan de protección contra las inundaciones\".\\n\\n|Of|+ damage|+ in|+ Newton|+ Stewart|+ .|+2|\\n|---|---|---|---|---|---|---|\\n| | |The|+1,81| |+1,81| |\\n|full|+1,81| |cost|+1,81| | |\\n|One + 11 other features|+0,31|remain + 24 other features|+0,24|to + 79 other features|+0,46|habe + 95 other features|\\n|+ 292 other features|+1,6| | | | | |\\n\\n0,00 0,25 0,50 0,75 1,00 1,25 1,50 1,75 2,00\\n\\nSHAP value', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='0f652d3f-0053-4cab-9ba1-ce6f7aaa7191', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Un ejemplo de puntuación por atribución es la aplicación de la técnica SHAP (SHapley Additive exPlanations) aplicado a un LLM, lo que facilita su interpretación y comprensión (Fig. 12). SHAP es un método de explicabilidad post-hoc basado en la teoría de juegos cooperativos. Asigna a cada característica (token) un valor de importancia (valor Shapley) que representa su contribución a la predicción del modelo.\\n\\n# 4 Validación continua y monitorización en producción:\\n\\nMás allá de la evaluación puntual antes del despliegue, se extiende la práctica de realizar un seguimiento continuo del comportamiento de los LLM una vez que están en uso, al igual que se hace con modelos tradicionales. Esto permite detectar posibles desviaciones o degradaciones en su rendimiento a lo largo del tiempo, así como identificar sesgos o riesgos no previstos inicialmente.\\n\\n# 4 Validación colaborativa y participativa:\\n\\nSe promueve una mayor implicación de diversos stakeholders en el proceso de validación, incluyendo no solo a expertos técnicos sino también a usuarios finales, reguladores, auditorías externas y representantes de la sociedad civil. Esta participación plural permite incorporar diferentes perspectivas y fomenta la transparencia y la responsabilidad.\\n\\n# 4 Validación ética y alineada con la regulación:\\n\\nMás allá de las métricas de rendimiento, se otorga cada vez más importancia a evaluar si el comportamiento de los LLM es ético y está alineado con los valores humanos y con la regulación. Esto implica analizar cuestiones como la equidad, la privacidad, la seguridad, la transparencia o el impacto social de estos sistemas.\\n\\nEjemplo: se considera un LLM entrenado para clasificar correos electrónicos corporativos como \"importante\" o \"no importante\". Dado el vector de tokens de entrada:\\n\\nx = [El, informe, financiero, del, Q2, muestra, un, aumento, significativo, en, los, ingresos, y, la, rentabilidad]\\n\\nEl modelo clasifica el correo como \"importante\" con f(x) = 0.85. Aplicando SHAP, se obtienen los siguientes valores Shapley:\\n\\n|Token|Valor Shapley|\\n|---|---|\\n|φ1|0.01 (El)|\\n|φ2|0.2 (informe)|\\n|φ3|0.15 (financiero)|\\n|φ4|0.02 (del)|\\n|φ5|0.1 (Q2)|\\n|φ6|0.05 (muestra)|\\n|φ7|0.01 (un)|\\n|φ8|0.15 (aumento)|\\n|φ9|0.1 (significativo)|\\n|φ10|0.01 (en)|\\n|φ11|0.02 (los)|\\n|φ12|0.12 (ingresos)|\\n|φ13|0.01 (y)|\\n|φ14|0.02 (la)|\\n|φ15|0.08 (rentabilidad)|\\n\\nInterpretación: los tokens \"informe\" (0.2), \"financiero\" (0.15), \"aumento\" (0.15) e \"ingresos\" (0.12) tienen las mayores contribuciones a la clasificación del correo como \"importante\". Esto sugiere que el LLM ha aprendido a asociar estos términos con la importancia del mensaje en un contexto empresarial.\\n\\n123 Liu (2024).\\n\\n124 Jang (2022).\\n\\n125 Yu (2023).\\n\\n126 Wu (2023).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='0defc2d9-a7c2-46e9-9d88-71b1494233b5', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='# Caso práctico: validación de un chatbot de políticas\\n\\n“La inteligencia artificial alcanzará niveles humanos hacia 2029“. Ray Kurzweil127\\n\\n“Creo que tendremos una IA más inteligente que cualquier ser humano probablemente a finales de 2025“. Perplexity128\\n\\n# MANAGEMENT SOLUTIONS\\n\\n# Fia\\n\\n44\\n\\n# El auge de los large language models: de los fundamentos a la aplicación', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='13752a75-af24-4340-b01a-9e309eb50d69', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='# Diseño del enfoque de validación\\n\\nPara ilustrar la aplicación de las técnicas de validación de un LLM descritas, en esta sección se presenta un caso práctico de validación de un chatbot de políticas internas de una compañía. Para validar de manera integral el chatbot de políticas, siguiendo el marco presentado en la sección anterior, se ha diseñado un enfoque de validación a medida que abarca las dimensiones clave del ciclo de vida del modelo: datos, diseño, evaluación, implementación y uso. Este enfoque combina métricas cuantitativas y técnicas de evaluación humana, con el objetivo de obtener una visión completa del desempeño y la adecuación del chatbot en el contexto específico de la compañía.\\n\\n# Definición del caso\\n\\nLa compañía ha desarrollado un chatbot basado en un LLM de código abierto para responder preguntas y proporcionar información sobre sus políticas internas. El objetivo principal de este chatbot es facilitar el acceso de los empleados a las políticas de la empresa. A continuación, se resumen las pruebas y técnicas seleccionadas para cada dimensión:\\n\\nEl chatbot ha sido construido utilizando una infraestructura en la nube y se ha alimentado con todas las políticas de la compañía, que abarcan aproximadamente 1.000 páginas de documentación. Para mejorar su capacidad de respuesta, se han aplicado técnicas de Retrieval-Augmented Generation (RAG), que permiten al modelo recuperar información relevante de su base de conocimientos antes de generar una respuesta. Inicialmente, se consideró la posibilidad de aplicar fine-tuning al modelo, pero tras las pruebas iniciales, se concluyó que la combinación del LLM base con RAG era suficiente para obtener resultados satisfactorios.\\n\\n# Datos\\n\\n- Métricas: se utilizará la escala Flesch-Kincaid para evaluar la legibilidad y complejidad de las políticas que alimentan al chatbot.\\n- Evaluación humana: se revisará una muestra representativa de las políticas para identificar posibles inconsistencias, errores o ambigüedades.\\n\\n# Diseño del modelo\\n\\nAntes de su implementación definitiva, la compañía ha decidido llevar a cabo un proceso de validación exhaustivo para evaluar la precisión, la seguridad y la adecuación del chatbot en el contexto específico de su uso previsto. Este proceso de validación tiene como objetivo identificar posibles áreas de mejora y garantizar que el chatbot cumpla con los estándares de calidad y las expectativas de la empresa.\\n\\nLa validación del chatbot de políticas se realizará mediante una combinación de métricas cuantitativas y técnicas de evaluación humana, siguiendo el marco de validación multidimensional descrito en la sección anterior. Los resultados de este proceso servirán para tomar decisiones informadas sobre la implementación del chatbot y para establecer un plan de mejora continua.\\n\\n# Notas\\n\\n127 Ray Kurzweil (n. 1948). Director de Ingeniería en Google, científico computacional, inventor y futurista, conocido por la invención del OCR y por sus contribuciones en IA.\\n\\n128 Elon Musk (n. 1971), CEO de X, SpaceX, Tesla. Empresario sudafricano-estadounidense, conocido por fundar o cofundar empresas como Tesla, SpaceX y PayPal, dueño de X (anteriormente Twitter), red social que tiene su propio LLM, llamado Grok.\\n\\n129 La estrategia de \"chunking\" se refiere al proceso de dividir el texto de entrada a un LLM en unidades más pequeñas y manejables («chunks») durante su uso o implementación.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='8ea44484-a6c1-41db-af7b-da9a3e9676a8', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='# Evaluación del modelo\\n\\n# Adaptabilidad\\n\\n- Métricas: se evaluará el rendimiento del chatbot ante nuevas políticas o actualizaciones mediante técnicas de few-shot learning. Se evaluará la respuesta del chatbot ante idiomas no empleados en las políticas o solicitudes de traducciones a idiomas no incluidos en el RAG (p. ej., polaco).\\n\\n# Privacidad y seguridad\\n\\n- Métricas: se aplicarán pruebas de K-anonimato para evaluar la protección de datos personales en las respuestas del chatbot, y pruebas de PII (Personal Identificable Information) para identificar atributos sensibles en los datos, utilizando PIIfilter.\\n- Evaluación humana: se realizarán pruebas de hacking ético para identificar posibles vulnerabilidades y se mantendrán registros detallados de las interacciones del chatbot.\\n\\n# Precisión\\n\\n- Métricas: se utilizarán las métricas Word Error Rate (WER) y ROUGE para evaluar la precisión de las respuestas del chatbot en comparación con las políticas originales. También se emplearán benchmarks específicos del dominio, como un conjunto de preguntas y respuestas diseñado por expertos en políticas de la compañía.\\n- Evaluación humana: se realizará una revisión caso por caso de una muestra representativa de interacciones del chatbot para identificar posibles errores o imprecisiones.\\n\\n# Consistencia\\n\\n- Métricas: se aplicarán la similitud coseno y el índice de Jaccard para evaluar la consistencia de las respuestas del chatbot ante consultas similares.\\n- Evaluación humana: se llevarán a cabo pruebas A/B para comparar las respuestas del chatbot en diferentes escenarios y se realizará una revisión caso por caso para identificar posibles inconsistencias.\\n\\n# Robustez\\n\\n- Métricas: se utilizarán herramientas como TextFooler para generar texto adversario y evaluar la resiliencia del chatbot ante información engañosa. Además, se contabilizará el número de rechazos del chatbot ante prompts malintencionados.\\n- Evaluación humana: se realizarán pruebas A/B y revisiones caso por caso para evaluar la capacidad del chatbot para adaptarse a nuevos escenarios.\\n\\n# Explicabilidad\\n\\n- Métricas: se aplicarán técnicas de explicabilidad, como SHAP, para comprender el proceso de toma de decisiones del chatbot. Se evaluará el módulo de interpretabilidad intrínseco del chatbot, mediante el cual se provee una explicación del origen de la información en la respuesta al usuario.\\n- Evaluación humana: se realizará un seguimiento de la experiencia del usuario (UX) y se llevará a cabo un focus group para evaluar la percepción de los usuarios sobre la transparencia y la explicabilidad del chatbot.\\n\\n# Sesgos y equidad\\n\\n- Métricas: se utilizará el conjunto de herramientas AI Fairness 360 para evaluar posibles sesgos demográficos en las respuestas del chatbot. También se emplearán benchmarks específicos, como el Bias Benchmark for QA (BBQ), para medir la equidad en el contexto de las políticas de la compañía.\\n- Evaluación humana: se llevarán a cabo pruebas de hacking ético y un focus group para identificar posibles sesgos o discriminación en las respuestas del chatbot.\\n\\n# Toxicidad\\n\\n- Métricas: se aplicarán las herramientas Perspective API y Hatebase API para evaluar la presencia de lenguaje tóxico o inapropiado en las respuestas del chatbot. Además, se utilizarán benchmarks específicos, como RealToxicityPrompts, para medir la toxicidad en el contexto de las políticas de la compañía.\\n- Evaluación humana: se realizarán pruebas de hacking ético para identificar posibles casos de lenguaje ofensivo o inapropiado en las interacciones del chatbot.\\n\\n- Evaluación humana: se realizarán pruebas de hacking ético y simulacros de incidentes para evaluar la capacidad del chatbot para manejar situaciones adversas.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='343af9f6-074c-4862-a625-59cc39c856de', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='# Implementación y uso\\n\\n# Resultados\\n\\n# 4 Escalabilidad\\n\\n- Métricas: se realizarán pruebas de estrés del sistema utilizando Apache JMeter para evaluar el rendimiento del chatbot bajo altas cargas de trabajo.\\n- Evaluación humana: se llevarán a cabo simulacros para evaluar la capacidad del chatbot para manejar un aumento imprevisto en el número de usuarios o consultas.\\n\\n# 4 Eficiencia\\n\\n- Métricas: se medirán el tiempo de respuesta (Time-to-First-Byte, TTFB), el uso de recursos (GPU/CPU, memoria) y la latencia para evaluar la eficiencia del chatbot.\\n\\n# 4 Aceptación del usuario\\n\\n- Métricas: se establecerá una lista de verificación de los requisitos del usuario y se medirá la satisfacción del usuario utilizando indicadores como el Net Promoter Score (NPS) y el Customer Satisfaction Score (CSAT).\\n- Evaluación humana: se realizará un seguimiento de la experiencia del usuario (UX) para evaluar la aceptación y satisfacción de los usuarios con el chatbot.\\n\\nEste enfoque de validación personalizado permitirá a la compañía obtener una evaluación completa del chatbot de políticas, identificando áreas de mejora y garantizando su adecuación para el uso previsto. Los resultados de estas pruebas y evaluaciones servirán como base para la toma de decisiones informadas sobre la implementación y el perfeccionamiento continuo del chatbot.\\n\\nEn cuanto a la implementación y uso, el chatbot demostró una buena escalabilidad y eficiencia en el manejo de altas cargas de trabajo. Además, la satisfacción de los usuarios fue alta, lo que indica una buena aceptación de la herramienta en el contexto de la compañía.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='279c6ee8-3cf9-4a19-9e1a-a65994c9bdb8', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='# Fig. 13. Resumen de resultados de las métricas y técnicas de evaluación humana del chatbot de políticas.\\n\\n|Dimensión|Prueba|Resultado|Interpretación|\\n|---|---|---|---|\\n|Datos|Flesch-Kincaid|Legibilidad adecuada (grado 8)|Las políticas son comprensibles para la mayoría de los usuarios|\\n| |Revisión humana|Sin inconsistencias significativas|Las políticas son coherentes y no contienen errores importantes|\\n| |Modelos challenger|Mejoras en parámetros identificadas|Se requiere adaptar los parámetros del RAG al contexto de las políticas (p. ej., chunk size) para mejorar la captura de información en preguntas muy específicas|\\n|Diseño del modelo|Revisión de componentes|Configuración óptima|El diseño del chatbot es apropiado para el caso de uso|\\n| |Pruebas A/B|Rendimiento superior a otros LLM|El chatbot supera a otros modelos disponibles en el mercado|\\n| |K-anonimato|Protección adecuada de datos|El chatbot no revela información sensible en sus respuestas personales|\\n| |Hacking ético|Vulnerabilidades menores identificadas|Se requieren ajustes para fortalecer la seguridad del chatbot|\\n| |Word Error Rate (WER)|WER < 5%|Las respuestas del chatbot son altamente precisas|\\n| |ROUGE|ROUGE-L > 0.8|Las respuestas del chatbot capturan adecuadamente el contenido de las políticas|\\n| |Similitud coseno / Índice de Jaccard|Similitud > 0.9|El chatbot proporciona respuestas consistentes ante consultas similares|\\n|MANAGEMENT SOLUTIONS|TextFooler|Resiliencia moderada ante texto adversario|El chatbot es moderadamente robusto ante información engañosa|\\n| |Few-shot learning|Adaptabilidad satisfactoria|El chatbot puede adaptarse a nuevas políticas o actualizaciones con un entrenamiento mínimo, pero se requiere monitorizar y agregar esos nuevos documentos al RAG periódicamente|\\n| | | |Se requiere mejorar la capacidad del chatbot para explicar sus decisiones, si bien el componente de RAG SHAP se ha construido de manera que el LLM da una respuesta autoexplicativa|\\n| |AI Fairness 360 / BBQ|Sesgos demográficos menores|El chatbot presenta algunos sesgos que deben ser identificados y mitigados|\\n| |Perspective API / RealToxicityPrompts|Toxicidad baja (< 5%)|Las respuestas del chatbot rara vez contienen lenguaje tóxico o inapropiado|\\n| |Apache JMeter|Escalabilidad satisfactoria (hasta 1000 usuarios)|El chatbot puede manejar altas cargas de trabajo sin degradación significativa del rendimiento|\\n|Implementación y uso|TTFB / Uso de recursos / Latencia|Eficiencia adecuada (TTFB < 1s, uso moderado)|El chatbot responde rápidamente y utiliza los recursos de manera eficiente|\\n| |NPS / CSAT|Satisfacción alta (NPS > 60, CSAT > 80%)|Los usuarios están altamente satisfechos con el chatbot y lo recomendarían a otros|', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='2a4f0709-140b-4d79-8fec-e296e9fbc4eb', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Estos resultados indican que el chatbot de políticas está bien encaminado para su implementación en la compañía, aunque se han identificado algunas áreas específicas que requieren mejoras adicionales. La sección siguiente abordará las principales conclusiones y recomendaciones derivadas de este proceso de validación.\\n\\n# Principales conclusiones\\n\\nEl proceso de validación del chatbot de políticas ha demostrado que este sistema basado en LLM puede ser una herramienta valiosa para facilitar el acceso de los empleados a la información relevante de la compañía. Los resultados obtenidos en las diversas pruebas y evaluaciones indican que el chatbot cumple, en gran medida, con los requisitos de calidad, seguridad y eficiencia establecidos por la organización.\\n\\nEntre las fortalezas identificadas, se destacan la precisión y consistencia de las respuestas del chatbot, su capacidad para adaptarse a nuevos escenarios y su escalabilidad para manejar altas cargas de trabajo. Además, la satisfacción de los usuarios con la herramienta es alta, lo que sugiere una buena aceptación y adopción por parte de los empleados.\\n\\nSin embargo, el proceso de validación también ha revelado algunas áreas de mejora que deben abordarse antes de la implementación definitiva del chatbot. En particular, se recomienda:\\n\\n1. Mejorar la explicabilidad del modelo: es necesario desarrollar técnicas más avanzadas para que el chatbot pueda proporcionar explicaciones claras y comprensibles sobre su proceso de toma de decisiones. Esto aumentará la transparencia y la confianza de los usuarios en la herramienta. Si bien el componente de RAG se ha construido de manera que el LLM da una respuesta autoexplicativa y hace referencia a la política.\\n2. Mitigar los sesgos identificados: aunque los sesgos detectados son menores, es recomendable aplicar técnicas de debiasing para garantizar que las respuestas del chatbot sean equitativas y no discriminatorias. Se sugiere una revisión periódica de los sesgos y la implementación de medidas correctivas cuando sea necesario.\\n3. Fortalecer la seguridad y privacidad: si bien el chatbot cumple con los estándares básicos de protección de datos personales, se recomienda realizar pruebas adicionales y recurrentes de hacking ético y adoptar medidas de seguridad más robustas para prevenir posibles vulnerabilidades.\\n4. Establecer un plan de monitoreo y mejora continua: es fundamental definir un proceso de seguimiento y evaluación periódica del desempeño del chatbot, con el fin de identificar oportunidades de mejora y garantizar su óptimo funcionamiento a largo plazo. Este plan debe incluir la recopilación de feedback de los usuarios, la actualización regular de las políticas y su inclusión en la base de datos del chatbot, el monitoreo para mejorar los parámetros empleados en el RAG y su actualización, y la incorporación de nuevas técnicas y tecnologías cuando estén disponibles.\\n\\nEn conclusión, el chatbot de políticas ha demostrado tener potencial para mejorar la eficiencia y la accesibilidad de la información en la compañía. Con la implementación de las mejoras sugeridas y un enfoque de perfeccionamiento continuo, este sistema basado en LLM puede convertirse en una herramienta estratégica para el éxito de la organización. La recomendación final ha sido proceder con la implementación del chatbot, teniendo en cuenta las observaciones y recomendaciones derivadas de este proceso de validación.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='349005d1-a926-4002-83e6-1539c3f64fe0', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='# Conclusiones\\n\\n“Los LLM son los únicos que pueden escribir una novela, traducirla a diez idiomas, y aun así no entender la trama“.\\n\\nPerplexity130\\n\\n# MANAGEMENT SOLUTIONS\\n\\n50\\n\\n# El auge de los large language models: de los fundamentos a la aplicación', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='e0cc2b67-ae15-4397-a02e-46246483912b', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='# Los modelos de lenguaje a gran escala (LLM)\\n\\nLos modelos de lenguaje a gran escala (LLM) representan un avance significativo en el campo de la inteligencia artificial y están revolucionando la forma en que interactuamos con la tecnología y aprovechamos el procesamiento del lenguaje natural. Su capacidad para procesar y generar texto coherente y contextualizado abre un amplio abanico de aplicaciones en diversos sectores, desde la creación de contenido y el análisis de sentimientos hasta la automatización de tareas y la mejora de la experiencia del usuario.\\n\\nSin embargo, el desarrollo y despliegue de LLM también conlleva una serie de retos y consideraciones importantes. La presencia de sesgos y alucinaciones en sus resultados, la falta de transparencia y explicabilidad de sus decisiones, los desafíos de privacidad y seguridad de la información, y el alto consumo de recursos computacionales son algunos de los principales desafíos que deben abordarse para garantizar un uso responsable y ético de estos sistemas.\\n\\nPara hacer frente a estos retos, es fundamental establecer un marco robusto de gobierno de la IA, especialmente en el ámbito de la IA generativa y los LLM. Este marco debe abarcar todos los aspectos clave, incluyendo la estrategia, el apetito al riesgo, el gobierno, la organización, el marco de control (políticas y procedimientos), los datos, los sistemas y el reporting. Solo a través de un enfoque integral y bien estructurado se podrá garantizar un desarrollo y uso responsable de estas tecnologías.\\n\\nDentro de este marco de gobierno, la validación juega un papel crucial. Es conveniente adoptar un enfoque multidimensional que abarque todas las etapas del ciclo de vida de los LLM, desde la calidad de los datos de entrada y la solidez del diseño del modelo hasta la evaluación exhaustiva de sus resultados y la implementación y uso adecuados. Este proceso de validación debe combinar métricas cuantitativas estandarizadas con técnicas de evaluación humana adaptadas al contexto específico de cada caso de uso.\\n\\n# Perplexity AI\\n\\nPerplexity AI es un motor de conversación y búsqueda de investigación basado en LLM fundado en 2022 por Andy Konwinski, Denis Yarats, Johnny Ho y Aravind Srinivas (ex OpenAI), que responde a las consultas mediante texto predictivo en lenguaje natural.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='f6c290f1-a4ca-490c-bdd5-42484db720ad', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='# Glosario\\n\\n# MANAGEMENT SOLUTIONS\\n\\n52\\n\\n# El auge de los large language models: de los fundamentos a la aplicación', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='b1081665-a421-42ad-9127-a5788589c4a2', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='# AGI (Artificial General Intelligence)\\n\\ninteligencia artificial hipotética futura que igualaría o superaría la inteligencia humana en cualquier dominio intelectual, siendo capaz de realizar cualquier tarea intelectual que un ser humano puede hacer.\\n\\n# Embeddings\\n\\nrepresentaciones densas y continuas de elementos discretos (como palabras, frases o documentos) en un espacio vectorial de alta dimensión, donde elementos similares tienen representaciones cercanas. Se utilizan en los LLM para capturar relaciones semánticas y sintácticas entre los elementos del lenguaje.\\n\\n# Alucinaciones\\n\\ngeneración de información o contenido por parte de un LLM que parece plausible pero que no se basa en hechos reales o en el conocimiento adquirido durante el entrenamiento, llevando a inexactitudes o invenciones en las respuestas del modelo.\\n\\n# Ética de la IA\\n\\ndisciplina que estudia los principios morales, valores y directrices que deben guiar el desarrollo, despliegue y uso de los sistemas de inteligencia artificial, con el objetivo de garantizar que sean beneficiosos, justos, transparentes y alineados con los valores humanos.\\n\\n# CNN (Convolutional Neural Network)\\n\\ntipo de red neuronal especializada en procesar datos con una topología de cuadrícula, como imágenes o series temporales. Las CNN utilizan capas de convolución para extraer automáticamente características locales y abstractas de los datos, y son ampliamente utilizadas en tareas de visión por computador y procesamiento de señales.\\n\\n# Evaluación humana\\n\\nproceso de revisión y valoración cualitativa del comportamiento y resultados de un sistema de IA por parte de expertos y usuarios, que complementa las métricas cuantitativas y permite detectar errores, sesgos o comportamientos indeseados que podrían pasar desapercibidos en una evaluación puramente automática.\\n\\n# Cuantización\\n\\ntécnica utilizada para reducir el tamaño y acelerar la inferencia de los LLM, que consiste en reducir la precisión numérica de los pesos del modelo, pasando de números en coma flotante a representaciones de menor precisión, como enteros o números en coma fija.\\n\\n# Explicabilidad (XAI, eXplainable AI)\\n\\npropiedad de un modelo de IA que se refiere a su capacidad para proporcionar explicaciones comprensibles para los humanos sobre su funcionamiento interno, el razonamiento detrás de sus predicciones y los factores que influyen en sus decisiones.\\n\\n# Datos de entrenamiento\\n\\nconjunto de ejemplos utilizados para entrenar un modelo de aprendizaje automático, que incluyen las entradas (features) y, en el caso del aprendizaje supervisado, las etiquetas o respuestas esperadas. La calidad y diversidad de estos datos es crucial para el rendimiento y la generalización del modelo.\\n\\n# Efecto Eliza\\n\\nfenómeno psicológico por el cual los usuarios tienden a atribuir capacidades cognitivas y emocionales similares a las humanas a los sistemas de conversación basados en IA, a pesar de que estos sistemas no poseen una comprensión real del lenguaje ni inteligencia general.\\n\\n# Few-shot learning\\n\\ncapacidad de un modelo de aprendizaje automático, especialmente los LLM, para aprender a realizar una nueva tarea a partir de pocos ejemplos (desde uno hasta unas decenas), aprovechando el conocimiento previo adquirido durante el preentrenamiento en grandes cantidades de datos.\\n\\n# Fine-tuning\\n\\ntécnica para adaptar un modelo de lenguaje preentrenado a una tarea específica, mediante el entrenamiento adicional con un conjunto de datos más pequeño y especializado en esa tarea. Permite aprovechar el conocimiento general del modelo y ajustarlo para obtener un alto rendimiento en aplicaciones concretas.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='0ed72b3a-71a4-4099-b516-1f16a72e8b09', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='# Hacking ético\\n\\nPráctica de probar y desafiar un sistema de IA de manera controlada y con permiso, con el objetivo de identificar vulnerabilidades, fallos, sesgos o comportamientos no deseados, para posteriormente corregirlos y mejorar la seguridad y robustez del sistema.\\n\\n# MANAGEMENT SOLUTIONS\\n\\n# Instruction tuning\\n\\nTécnica de ajuste fino para LLM que consiste en proporcionar al modelo instrucciones, preguntas y ejemplos de respuestas esperadas, con el objetivo de alinear su comportamiento con las expectativas y preferencias de los usuarios en un dominio específico.\\n\\n# LLMOps (Large Language Model Operations)\\n\\nConjunto de prácticas, herramientas y procesos para gestionar de manera eficiente y escalable el ciclo de vida completo de los LLM en entornos de producción, abarcando el entrenamiento, despliegue, monitorización, actualización y gobierno de estos modelos.\\n\\n# Machine learning\\n\\nBramo de la inteligencia artificial que se centra en el desarrollo de algoritmos y modelos que permiten a los sistemas aprender y mejorar automáticamente a través de la experiencia, sin ser programados explícitamente para ello.\\n\\n# Inteligencia Artificial (IA)\\n\\nCampo de la informática y la ingeniería que se dedica al desarrollo de sistemas capaces de realizar tareas que normalmente requieren inteligencia humana, como el aprendizaje, el razonamiento, la percepción, la interacción en lenguaje natural y la resolución de problemas.\\n\\n# Machine unlearning\\n\\nConjunto de técnicas para eliminar o \"desaprender\" de manera selectiva cierta información o sesgos indeseados de un modelo de aprendizaje automático ya entrenado, sin necesidad de reentrenarlo desde cero, permitiendo cumplir con requisitos de privacidad o corregir comportamientos no deseados.\\n\\n# Inteligencia artificial generativa (GenAI)\\n\\nSubcampo de la IA que se enfoca en la creación de modelos y algoritmos capaces de generar contenido nuevo y original, como texto, imágenes, vídeo, audio, código fuente o diseños 3D, aprendiendo patrones y características a partir de un conjunto de datos de entrenamiento.\\n\\n# Métricas cuantitativas\\n\\nMedidas numéricas estandarizadas utilizadas para evaluar de manera objetiva y consistente el rendimiento de un modelo de IA en tareas específicas, como la precisión, la exhaustividad, la exactitud o la eficiencia.\\n\\n# Modelo generativo\\n\\nTipo de modelo de aprendizaje automático diseñado para aprender la distribución de probabilidad subyacente a un conjunto de datos y generar nuevas muestras que sean similares a los datos de entrenamiento, pudiendo crear contenido nuevo y realista.\\n\\n# Preentrenamiento\\n\\nEtapa inicial del entrenamiento de un LLM en la que se utiliza un gran corpus de texto no estructurado y sin etiquetar para que el modelo aprenda representaciones generales y patrones del lenguaje, adquiriendo un conocimiento amplio y robusto que luego puede ser adaptado a tareas específicas mediante fine-tuning.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='105d306b-139f-4c0b-8d82-0fe341e1bc16', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='# Privacidad diferencial\\n\\ntécnica criptográfica utilizada para compartir información agregada sobre un conjunto de datos, mientras se protege la privacidad de los individuos presentes en esos datos, introduciendo un ruido aleatorio que dificulta la identificación de entradas individuales a partir de los resultados del análisis.\\n\\n# Prompt engineering\\n\\ndisciplina que se enfoca en diseñar, optimizar y adaptar los prompts (entradas de texto) para obtener los mejores resultados posibles de los LLM en tareas específicas, aprovechando técnicas como la inclusión de ejemplos, la especificación de formatos o la orientación paso a paso.\\n\\n# Pruebas A/B\\n\\nmétodo experimental utilizado para comparar el rendimiento de dos versiones diferentes de un sistema de IA (A y B) o entre un sistema de IA y un enfoque alternativo (como un humano o un modelo base), con el objetivo de determinar cuál funciona mejor según métricas predefinidas.\\n\\n# Regulación de la IA\\n\\nconjunto de leyes, normativas, estándares y directrices establecidos por gobiernos y organizaciones para garantizar que el desarrollo, despliegue y uso de los sistemas de inteligencia artificial se realice de manera responsable, segura, ética y alineada con los valores y derechos fundamentales de la sociedad.\\n\\n# Token\\n\\nunidad discreta en la que se divide un texto para su procesamiento por parte de un modelo de lenguaje. Los tokens pueden ser palabras, subpalabras o caracteres, y constituyen la entrada básica para el entrenamiento y la inferencia de los LLM.\\n\\n# Tokenización\\n\\nproceso de convertir un texto en una secuencia de tokens. La elección de la estrategia de tokenización tiene un impacto significativo en el rendimiento y la eficiencia del modelo.\\n\\n# Transformers\\n\\narquitectura de red neuronal profunda que utiliza mecanismos de atención para procesar y generar secuencias de forma paralela, en lugar de secuencialmente como las RNNs. Permite capturar dependencias a largo plazo y contextuales, siendo la arquitectura dominante para los LLM y estableciendo el estado del arte en diversas tareas de procesamiento del lenguaje natural.\\n\\n# Validación\\n\\nproceso integral y multidisciplinar para evaluar un sistema de IA, especialmente LLM, en términos de rendimiento, robustez, seguridad, equidad, explicabilidad y alineación con los requisitos y valores éticos y sociales, combinando métricas cuantitativas y evaluación cualitativa por parte de expertos y usuarios.\\n\\n# Retrieval-Augmented Generation (RAG)\\n\\ntécnica utilizada en los LLM que consiste en recuperar información relevante de una base de conocimientos externa antes de generar una respuesta, combinando así la capacidad de acceso a información estructurada con la generación de lenguaje natural coherente y fluido.\\n\\n# RNN (Recurrent Neural Network)\\n\\ntipo de red neuronal diseñada para procesar secuencias de datos, como texto o series temporales. A diferencia de las redes neuronales feedforward, las RNN tienen conexiones recurrentes que les permiten mantener un estado interno y capturar dependencias temporales. Variantes como LSTM y GRU han sido ampliamente utilizadas en tareas de procesamiento del lenguaje natural antes del auge de los transformers.\\n\\n# Seguridad (AI safety)\\n\\ndisciplina que se enfoca en identificar, prevenir y mitigar los riesgos potenciales asociados con el desarrollo y uso de sistemas de IA avanzados, tanto a corto como a largo plazo, incluyendo riesgos de seguridad, sesgos, errores, mal uso o consecuencias no deseadas.\\n\\n# Sesgo\\n\\ntendencia sistemática de un modelo de aprendizaje automático a producir resultados que favorecen o perjudican injustamente a ciertos grupos o individuos, debido a características sensibles como el género, la etnia, la edad o la orientación sexual, y que suele ser resultado de sesgos presentes en los datos de entrenamiento o de decisiones subóptimas durante el desarrollo del modelo.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='dea9ef26-1113-4b27-a1ad-d48efc8647e3', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='# Bibliografía\\n\\nMANAGEMENT SOLUTIONS\\n\\nEl auge de los large language models: de los fundamentos a la aplicación\\n\\nMagyt orszag', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='a92285d2-cfa6-4848-922d-82ab0159a0ee', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"# References\\n\\nAbhyankar, R. et al. (2024). APIServe: Efficient API Support for Large-Language Model Inferencing. https://arxiv.org/abs/2402.01869. arXiv:2402.01869v1\\n\\nCIO (2023). Chief AI Officer: What it takes to land the C-suite’s hottest new job. https://www.cio.com/article/657977/chief-ai-officer-what-it-takes-to-land-the-c-suites-hottest-new-job.html\\n\\nAlabdulmohsin, I. et al. (2024). CLIP the Bias: How Useful is Balancing Data in Multimodal Learning? https://arxiv.org/html/2403.04547v1. arXiv:2403.04547v1\\n\\nCui, Q. et al. (2022). Contrastive Vision-Language Pre-training with Limited Resources. https://arxiv.org/abs/2112.09331. arXiv:2112.09331v3\\n\\nBanerjee, I., et al. (2023). MLOps with enhanced performance control and observability. https://arxiv.org/abs/2302.01061. arXiv:2302.01061v1\\n\\nCommetML. https://www.comet.com/site/\\n\\nDatta, T. et al. (2023). Who's Thinking? A Push for Human-Centered Evaluation of LLMs using the XAI Playbook. https://arxiv.org/abs/2303.06223. arXiv:2303.06223v1\\n\\nBengio, Y. et al. (2003). A Neural Probabilistic Language Model. https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf\\n\\nBréal, M. (1883). Les lois intellectuelles du langage fragment de sémantique. Annuaire de l'Association pour l'encouragement des études grecques en France. Vol. 17 (1883), pp. 132-142. https://www.jstor.org/stable/44253893\\n\\nCambon, A. et al. (2023). Early LLM-based Tools for Enterprise Information Workers Likely Provide Meaningful Boosts to Productivity. A first update from Microsoft’s research initiative on AI and Productivity.\\n\\nChen, D. et al. (2023). Data-Juicer: A One-Stop Data Processing System for Large Language Models. https://arxiv.org/abs/2309.02033. arXiv:2309.02033v3\\n\\nDuan, J. et al. (2023). Shifting attention to relevance: towards the uncertainty estimation of large language models. https://arxiv.org/abs/2307.01379. arXiv:2307.01379v2\\n\\nChen, Y. et al. (2023). LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models. https://arxiv.org/abs/2309.12307. arXiv:2309.12307v3\\n\\nElazar, Y. et al. (2021). Measuring and Improving Consistency in Pretrained Language Models. https://aclanthology.org/2021.tacl-1.60/\\n\\nChiang, C. et al. (2023). Can Large Language Models Be an Alternative to Human Evaluations? https://arxiv.org/abs/2305.01937. arXiv:2305.01937v1\\n\\nChu, T., Song, Z., Yang, C. (2023). How to Protect Copyright Data in Optimization of Large Language Models? https://arxiv.org/abs/2308.12247. arXiv:2308.12247v1\\n\\nEuronews (2023). 2023 was the year AI went mainstream. It was also the year we started to panic about it. https://www.euronews.com/next/2023/12/27/2023-was-the-year-ai-went-mainstream-it-was-also-the-year-we-started-to-panic-about-it\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='cfba437a-90dc-44d0-9af3-9a33f282d97c', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='# European Parliament (2024). Artificial Intelligence Act\\n\\nEuropean Parliament legislative resolution of 13 March 2024 on the proposal for a regulation of the European Parliament and of the Council on laying down harmonized rules on Artificial Intelligence (Artificial Intelligence Act) and amending certain Union Legislative Acts (COM(2021)0206 – C9-0146/2021 – 2021/0106(COD)). https://artificialintelligenceact.eu/; https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=celex%3A52021PC0206\\n\\n# References\\n\\n- European Commission (2024). Knowledge Center on Interpretation. https://knowledge-centre-interpretation.education.ec.europa.eu/en/news/what-large-language-model\\n- Huang, L. et al. (2023). A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions. https://arxiv.org/abs/2311.05232. arXiv:2311.05232v1\\n- Hugging Face Datasets (2024). CodeParrot. https://huggingface.co/codeparrot\\n- IAPP (2024). Global AI Law and Policy Tracker. https://iapp.org/resources/article/global-ai-legislation-tracker/\\n- iDanae 2T23 (2023): Large Language Models: una nueva era en la inteligencia artificial. Cátedra iDanae. Newsletter trimestral 2T23. http://www.idanae-stem.com/\\n- Fisher, M., Campagna, G., Choi, E., Lam, M. S., Freund, S. N., Yahav, E. (2021). DIY Assistant: A Multi-modal End-User Programmable Virtual Assistant. https://dl.acm.org/doi/10.1145/3453483.3454046\\n- Gartner (2023). What is generative AI? https://www.gartner.com/en/topics/generative-ai\\n- iDanae 1T24 (2024): Hacia una inteligencia artificial sostenible. Cátedra iDanae. Newsletter trimestral 1T24. http://www.idanae-stem.com/\\n- Imperial, JM., et al. (2023). Flesch or Fumble? Evaluating Readability Standard Alignment of Instruction-Tuned Language Models. https://arxiv.org/abs/2309.05454. arXiv:2309.05454v2\\n- Google DeepMind (2023). Levels of AGI: Operationalizing Progress on the Path to AGI. Meredith Ringel Morris; Jascha Sohl-Dickstein; Noah Fiedel; Tris Warkentin; Allan Dafoe; Aleksandra Faust; Clement Farabet; and Shane Legg. arXiv:2311.02462v1\\n- IndesIA (2024). Barómetro de adopción de la inteligencia artificial en las pymes españolas. https://www.indesia.org/wp-content/uploads/2024/04/IndesIA.-Barometro-de-adopcion-de-la-inteligencia-artificial-en-las-pymes-espanolas-Edicion-2024.pdf\\n- Google + Implement (2023). The economic opportunity of generative AI in D9+. An Implement Consulting Group study commissioned by Google.\\n- Jang et al. (2022). Knowledge unlearning for mitigating privacy risks in language models. https://arxiv.org/abs/2210.01504. arXiv:2210.01504.\\n- Gozalo-Brizuela, R., y Garrido-Merchán, E.C. (2023). A survey of Generative AI Applications. https://ar5iv.labs.arxiv.org/html/2306.02781\\n- Guo, Z. et al. (2023). Evaluating Large Language Models: A Comprehensive Survey. https://arxiv.org/pdf/2310.19736.pdf. arXiv:2310.19736v3\\n- Jia, C. et al (2021). Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision. https://arxiv.org/abs/2102.05918. arXiv:2102.05918v2\\n- Kahng, M. et al. (2024). LLM Comparator: Visual Analytics for Side-by-Side Evaluation of Large Language Models. https://arxiv.org/abs/2402.10524. arXiv:2402.10524v1\\n- Guzman, F. et al. (2015). How do Humans Evaluate Machine Translation. https://aclanthology.org/W15-3059.pdf.\\n- Fu, HY. et al. (2023). Estimating Large Language Model Capabilities without Labeled Test Data. https://arxiv.org/abs/2305.14802. arXiv:2305.14802v2\\n- Fu, X. et al (2024). Tiny Titans: Can Smaller Large Language Models Punch Above Their Weight in the Real World for Meeting Summarization? https://arxiv.org/abs/2402.00841. arXiv:2402.00841\\n- Goyal, S. et al (2024). LLMGuard: Guarding Against Unsafe LLM Behavior. https://arxiv.org/abs/2403.00826. arXiv:2403.00826v1\\n- Hendrycks, D. et al (2021). Measuring Massive Multitask Language Understanding. https://arxiv.org/abs/2009.03300. arXiv:2009.03300v3\\n- Kuchnik, M. et al. (2023). Validating Large Language Models with Realm. https://arxiv.org/abs/2211.15458. arXiv:2211.15458v2\\n- Kudo, T., Richardson, J. (2018). SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing. https://arxiv.org/abs/1808.06226. arXiv:1808.06226v1\\n- Lam, M. (2018). https://profiles.stanford.edu/monica-lam?tab=publications. Keeping the Internet Open with an Open-Source Virtual Assistant.\\n- Lee, C. et al (2024). OrchestraLLM: Efficient Orchestration of Language Models for Dialogue State Tracking. https://arxiv.org/html/2311.09758v2. arXiv:2311.09758v2', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='de76f783-c99f-4c69-a689-b505bae3676b', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='# References\\n\\nLee, J. et al. (2022). Seq2Seq-SC: End-to-End Semantic Communication Systems with Pre-trained Language Model. https://arxiv.org/abs/2210.15237. arXiv:2210.15237v2\\n\\nLester, B. et al. (2021). The Power of Scale for Parameter-Efficient Prompt Tuning. https://arxiv.org/abs/2104.08691. arXiv:2104.08691v2\\n\\nLewis, P., et al. (2020). Retrieval-augmented generation for knowledge-intensive NLP tasks. https://arxiv.org/abs/2005.11401\\n\\nLi, H. et al. (2024). Digger: Detecting Copyright Content Misusage in Large Language Model Training. https://arxiv.org/abs/2401.00676. arXiv:2401.00676v1\\n\\nLi, S. et al (2024). Evaluating Quantized Large Language Models. https://arxiv.org/abs/2402.18158. arXiv:2402.18158v1\\n\\nMeeus, M. et al. (2024). Copyright Traps for Large Language Models. https://arxiv.org/abs/2402.09363. arXiv:2402.09363v1\\n\\nMehta, S.V. et al. (2023). An Empirical Investigation of the Role of Pre-training in Lifelong Learning. https://arxiv.org/abs/2112.09153. arXiv:2112.09153v2\\n\\nMikolov, T. et al. (2013). Efficient Estimation of Word Representations in Vector Space. https://arxiv.org/abs/1301.3781. arXiv:1301.3781v3\\n\\nMinaee, S. et al. (2024). Large Language Models: A Survey. https://arxiv.org/abs/2402.06196. arXiv:2402.06196v2\\n\\nMindsDB (2024). A Comparative Analysis of Leading Large Language Models. https://mindsdb.com/blog/navigating-the-llm-landscape-a-comparative-analysis-of-leading-large-language-models\\n\\nLi, Y. et al (2023). A Survey on Fairness in Large Language Models. https://arxiv.org/abs/2308.10149. arXiv:2308.10149\\n\\nMökander, J. et al. (2023). Auditing large language models: a three-layered approach. arXiv:2302.08500v2\\n\\nLiang, P. et al. (2023). Holistic Evaluation of Language Models. https://arxiv.org/abs/2211.09110. arXiv:2211.09110v2\\n\\nLiu, T. et al (2022). Autoregressive Structured Prediction with Language Models. https://arxiv.org/abs/2210.14698. arXiv:2210.14698v2\\n\\nNeelakantan, A. et al. (2022). Text and Code Embeddings by Contrastive Pre-Training. https://arxiv.org/abs/2201.10005. arXiv:2201.10005v1\\n\\nLiu, Y. et al (2024). Datasets for Large Language Models: A Comprehensive Survey. https://arxiv.org/abs/2402.18041. arXiv:2402.18041v1\\n\\nNIST (2023). AI Risk Management Framework | NIST. https://www.nist.gov/itl/ai-risk-management-framework\\n\\nOneto, L., Chiappa, S. (2020). Fairness in Machine Learning. 2012.15816.pdf arXiv:2012.15816v1\\n\\nOpenAI (2024). Prompt engineering. https://platform.openai.com/docs/guides/prompt-engineering\\n\\nLuo, Y. et al. (2023). An Empirical Study of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning. https://arxiv.org/pdf/2308.08747.pdf. arXiv:2308.08747v3\\n\\nManagement Solutions (2023). Explainable Artificial Intelligence (XAI): desafíos en la interpretabilidad de los modelos. https://www.managementsolutions.com/en/microsites/whitepapers/explainable-artificial-intelligence\\n\\nManagement Solutions (2022). AutoML, hacia la automatización de los modelos. https://www.managementsolutions.com/es/publicaciones-y-eventos/informes-sectoriales/white-papers/auto-machine-learning-hacia-la-automatizacion-de-los-modelos\\n\\nOvadia, O. et al (2024). Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs. https://arxiv.org/pdf/2312.05934.pdf. arXiv:2312.05934v3\\n\\nPankajakshan, R. et al (2024). Mapping LLM Security Landscapes: A Comprehensive Stakeholder Risk Assessment Proposal. https://arxiv.org/html/2403.13309v1. arXiv:2403.13309v1\\n\\nParikh, A. P., et al. (2016). A Decomposable Attention Model for Natural Language Inference. https://arxiv.org/abs/1606.01933. arXiv:1606.01933v2\\n\\nPenedo, G. et al (2023). The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only. https://arxiv.org/abs/2306.01116. arXiv:2306.01116v1\\n\\nManagement Solutions (2014). Model Risk Management: Quantitative and Qualitative Aspects. Model Risk Management: Quantitative and qualitative aspects | Management Solutions\\n\\nPew Research Center (2023). Experts Predict the Best and Worst Changes in Digital Life by 2035.\\n\\nProject Gutenberg (2024). https://www.gutenberg.org/', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='46f7c59d-5325-487f-afc3-d66e53864c71', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"# References\\n\\nRae, JW, et al (2021). Scaling Language Models: Methods, Analysis & Insights from Training Gopher. https://arxiv.org/abs/2112.11446. arXiv:2112.11446\\n\\nSinha, K. et al (2021). Masked Language Modeling and the Distributional Hypothesis: Order Word Matters Pre-training for Little. https://arxiv.org/abs/2104.06644. arXiv:2104.06644v2\\n\\nRafailov, R. et al (2023). Direct Preference Optimization: Your Language Model is Secretly a Reward Model. https://arxiv.org/abs/2305.18290. arXiv:2305.18290v2\\n\\nSoskek (2019). BookCorpus. https://github.com/soskek/bookcorpus.\\n\\nSu, J., et al (2021). Roformer: Enhanced transformer with rotary position embedding. https://arxiv.org/abs/2104.09864. arXiv:2104.09864.\\n\\nSutskever, I. et al (2014). Sequence to Sequence Learning with Neural Networks. https://arxiv.org/abs/1409.3215. arXiv:1409.3215v3\\n\\nThe Next Web (2023). When will AGI arrive? Here’s what our tech lords predict. https://thenextweb.com/news/when-will-agi-arrive-tech-experts-predict-artificial-general-intelligence\\n\\nRisk.net. (2024). The bank quant who wants to stop gen AI hallucinating. https://www.risk.net/risk-management/7959062/the-bank-quant-who-wants-to-stop-gen-ai-hallucinating.\\n\\nSachdeva, N., et al (2024). How to Train Data-Efficient LLMs. https://arxiv.org/html/2402.09668v1. arXiv:2402.09668v1\\n\\nTian, Y. et al (2024). TinyLLM: Learning a Small Student from Multiple Large Language Models. https://arxiv.org/abs/2402.04616. arXiv:2402.04616\\n\\nSamsi, S., et al (2023). From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference. https://arxiv.org/pdf/2310.03003.pdf. arXiv:2310.03003v1\\n\\nTirumala, K. et al. (2023). D4: Improving LLM Pretraining via Document De-Duplication and Diversification. https://arxiv.org/abs/2308.12284. arXiv:2308.12284v1\\n\\nSarti, G. et al (2023). Inseq: An Interpretability Toolkit for Sequence Generation Models. [2302.13942] Inseq: An Interpretability Toolkit for Sequence Generation Models (arxiv.org). arXiv:2302.13942v3\\n\\nUK Government (2023). The Bletchley Declaration by Countries Attending the AI Safety Summit, 1-2 November 2023. https://www.gov.uk/government/publications/ai-safety-summit-2023-the-bletchley-declaration/the-bletchley-declaration-by-countries-attending-the-ai-safety-summit-1-2-november-2023\\n\\nVartziotis, T. et al (2024). Learn to Code Sustainably: An Empirical Study on LLM-based Green Code Generation.\\n\\nShaikh, O. et al. (2022). On Second Thought, Let's Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning. https://arxiv.org/html/2403.03344v1. arXiv:2403.03344v1.\\n\\nVaswani, A. et al. (2017). Attention Is All You Need. https://arxiv.org/abs/1706.03762\\n\\nShaw, P. et al (2018). Self-Attention with Relative Position Representations. https://arxiv.org/abs/1803.02155. arXiv:1803.02155v2\\n\\nSherstinsky, A. (2018). Fundamentals of Recurrent Neural Network (RNN) and Long Short-Term Memory (LSTM) Network. https://arxiv.org/abs/1808.03314. arXiv:1808.03314v10\\n\\nShi, W. et al (2024). Detecting pretraining data from large language models. https://arxiv.org/abs/2310.16789. arXiv:2310.16789v3\\n\\nSingh, C. et al (2024). Rethinking Interpretability in the Era of Large Language Models. https://arxiv.org/abs/2402.01761. arXiv:2402.01761v1\\n\\nWei, J. et al (2022). Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. https://arxiv.org/abs/2201.11903. arXiv:2201.11903v6\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='bee53541-eb69-472b-9488-58913b45c549', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='# References\\n\\nWenzek, G., et al (2019). CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data. https://arxiv.org/abs/1911.00359. arXiv:1911.00359v2\\n\\nYu, C. et al. (2023). Unlearning bias in language models by partitioning gradients. https://aclanthology.org/2023.findings-acl.375.pdf.\\n\\nWettig, A. et al. (2024). QuRating: Selecting High-Quality Data for Training Language Models. https://arxiv.org/abs/2402.09739. arXiv:2402.09739v1\\n\\nYogarajan, V., et al (2023). Tackling Bias in Pre-trained Language Models: Current Trends and Under-represented Societies. https://arxiv.org/pdf/2312.01509.pdf. arXiv:2312.01509v1\\n\\nWeights & Biases: The AI Developer Platform (wandb.ai). https://wandb.ai/site\\n\\nWikipedia (2024). Dumps. https://dumps.wikimedia.org/zhwiki/latest/.\\n\\nWired (2023). OpenAI’s CEO Says the Age of Giant AI Models Is Already Over. https://www.wired.com/story/openai-ceo-sam-altman-the-age-of-giant-ai-models-is-already-over/\\n\\nWeizenbaum, J. (1966). ELIZA—a computer program for the study of natural language communication between man and machine. https://dl.acm.org/doi/10.1145/365153.365168\\n\\nWhite House (2022). Blueprint for an AI Bill Of Rights. Making Automated Systems Work for the American People. https://www.whitehouse.gov/wp-content/uploads/2022/10/Blueprint-for-an-AI-Bill-of-Rights.pdf\\n\\nWhite House (2023). Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence. https://www.whitehouse.gov/briefing-room/presidential-actions/2023/10/30/executive-order-on-the-safe-secure-and-trustworthy-development-and-use-of-artificial-intelligence/\\n\\nWu, X. et al. (2023). Depn: Detecting and editing privacy neurons in pretrained language models. https://arxiv.org/abs/2310.20138. arXiv:2310.20138.\\n\\nXin Zhao, W., et al. (2023). A Survey of Large Language Models. https://arxiv.org/abs/2303.18223. arXiv:2303.18223v13\\n\\nXu, L. et al. (2023). Parameter-Efficient Fine-Tuning Methods for Pretrained Language Models: A Critical Review and Assessment. https://arxiv.org/pdf/2312.12148.pdf. arXiv:2312.12148v1\\n\\nXu, Y. et al. (2021). Non-Autoregressive Text Generation with Pre-trained Language Models. https://aclanthology.org/2021.eacl-main.18/\\n\\nXu, Z. et al. (2024). Hallucination is Inevitable: An Innate Limitation of Large Language Models. https://arxiv.org/abs/2401.11817. arXiv:2401.11817v1\\n\\nYang, J. et al. (2024). Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond. https://arxiv.org/abs/2304.13712. arXiv:2304.13712v2\\n\\nYidiz, C. et al (2024). Investigating Continual Pretraining in Large Language Models: Insights and Implications. https://arxiv.org/html/2402.17400v1. arXiv:2402.17400v1\\n\\nZaharia, M. et al (2018). Accelerating the Machine Learning Lifecycle with MLflow. https://people.eecs.berkeley.edu/~matei/papers/2018/ieee_mlflow.pdf.\\n\\nZhang, B. et al (2024). When Scaling Meets LLM Finetuning: The Effect of Data, Model and Finetuning Method. https://arxiv.org/abs/2402.17193. arXiv:2402.17193v1\\n\\nZhang, L. et al (2024). Enhancing Large Language Model Performance To Answer Questions and Extract Information More Accurately. https://arxiv.org/html/2402.01722v1. arXiv:2402.01722v1.\\n\\nZhang, S. et al (2023). Instruction Tuning for Large Language Models: A Survey. https://www.researchgate.net/publication/373263398_Instruction_Tuning_for_Large_Language_Models_A_Survey.\\n\\nZhang, Y. et al (2024). Bias Mitigation in Fine-tuning Pre-trained Models for Enhanced Fairness and Efficiency. https://arxiv.org/html/2403.00625v1. arXiv:2403.00625v1\\n\\nZhao, B., et al (2023). Tuning LayerNorm in Attention: Towards Efficient Multi-Modal LLM Finetuning. https://arxiv.org/abs/2312.11420. arXiv:2312.11420v1\\n\\nZhou, C. et al (2023). LIMA: Less Is More for Alignment. https://arxiv.org/abs/2305.11206. arXiv:2305.11206v1\\n\\nZhou, N., et al (2021). Bias, Fairness, and Accountability with AI and ML Algorithms. https://arxiv.org/abs/2105.06558. arXiv:2105.06558v1', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ba2765df-8572-48a4-8dce-f34b35630f85', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"# MS' Solutionsient\\n\\n# Making things happen\\n\\nNuestro objetivo es superar las expectativas de nuestros clientes convirtiéndonos en socios de confianza.\\n\\nManagement Solutions es una firma internacional de servicios de consultoría centrada en el asesoramiento de negocio, finanzas, riesgos, organización y procesos, tanto en sus componentes funcionales como en la implantación de sus tecnologías relacionadas.\\n\\nCon un equipo multidisciplinar (funcionales, matemáticos, técnicos, etc.) de cerca de 4.000 profesionales, Management Solutions desarrolla su actividad a través de 46 oficinas (20 en Europa, 22 en América, 2 en Asia, 1 en África y 1 en Oceanía).\\n\\nPara dar cobertura a las necesidades de sus clientes, Management Solutions tiene estructuradas sus prácticas por industrias (Entidades Financieras, Energía, Telecomunicaciones y Otras industrias) y por líneas de actividad que agrupan una amplia gama de competencias: Estrategia, Gestión Comercial y Marketing, Gestión y Control de Riesgos, Información de Gestión y Financiera, Transformación: Organización y Procesos, y Nuevas Tecnologías.\\n\\nEl área de I+D da servicio a los profesionales de Management Solutions y a sus clientes en aspectos cuantitativos necesarios para acometer los proyectos con rigor y excelencia, a través de la aplicación de las mejores prácticas y de la prospección continua de las últimas tendencias en inteligencia artificial, data science, machine learning, modelización y big data.\\n\\n# El auge de los large language models: de los fundamentos a la aplicación\\n\\n# Javier Calvo Martín\\n\\nSocio de Management Solutions\\n\\njavier.calvo.martin@managementsolutions.com\\n\\n# Manuel Ángel Guzmán Caba\\n\\nSocio de Management Solutions\\n\\nmanuel.guzman@managementsolutions.com\\n\\n# Luz Ferrero Peña\\n\\nGerente de Management Solutions\\n\\nluz.ferrero@msgermany.com.de\\n\\n# Segismundo Jiménez Láinez\\n\\nGerente de Management Solutions\\n\\nsegismundo.jimenez@msspain.com\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='9725fb1c-17fa-478d-88d5-38b4ff995d4a', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='NO_CONTENT_HERE', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='424ffcbc-fb45-4def-997f-ef1903f58f1a', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='# Management Solutions, servicios profesionales de consultoría\\n\\nManagement Solutions es una firma internacional de consultoría centrada en el asesoramiento de negocio, finanzas, riesgos, organización, tecnología y procesos.\\n\\nPara más información visita www.managementsolutions.com\\n\\nSíguenos en: in Xfldle\\n\\n© Management Solutions. 2024\\n\\nTodos los derechos reservados\\n\\nMadrid Barcelona Bilbao Coruña Málaga London Frankfurt Düsseldorf Paris Amsterdam Copenhagen Oslo Stockholm Warszawa Wroclaw Zürich Milano Roma Bologna Lisboa Beijing Istanbul Johannesburgo Sydney Toronto New York New Jersey Boston Pittsburgh Atlanta Birmingham Houston Miami SJ de Puerto Rico San José Ciudad de México Monterrey Querétaro Medellín Bogotá Quito São Paulo Rio de Janeiro Lima Santiago de Chile Buenos Aires', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# El auge de los large language models: de los fundamentos a la aplicación\n",
      "\n",
      "MS' Solutions nent Making things happen\n",
      "\n",
      "I+D\n",
      "\n",
      "www.managementsolutions.com\n",
      "# Diseño y Maquetación\n",
      "\n",
      "# Dpto. Marketing y Comunicación\n",
      "\n",
      "# Management Solutions - España\n",
      "\n",
      "# Fotografías\n",
      "\n",
      "Archivo fotográfico de Management Solutions\n",
      "\n",
      "AdobeStock\n",
      "\n",
      "Midjourney\n",
      "\n",
      "© Management Solutions 2024\n",
      "\n",
      "Todos los derechos reservados. Queda prohibida la reproducción, distribución, comunicación pública, transformación, total o parcial, gratuita u onerosa, por cualquier medio o procedimiento, sin la autorización previa y por escrito de Management Solutions.\n",
      "\n",
      "La información contenida en esta publicación es únicamente a título informativo. Management Solutions no se hace responsable del uso que de esta información puedan hacer terceras personas. Nadie puede hacer uso de este material salvo autorización expresa por parte de Management Solutions.\n",
      "# Índice\n",
      "\n",
      "- Introducción .................................................. 4\n",
      "- Resumen ejecutivo .................................. 10\n",
      "- LLM: definición, contexto y regulación .... 14\n",
      "- Desarrollo y despliegue de los LLM .......... 22\n",
      "- Marco de validación de los LLM .................... 36\n",
      "- Caso práctico: validación de un chatbot de políticas .... 44\n",
      "- Conclusiones .................................................. 50\n",
      "- Glosario .......................................................... 52\n",
      "- Bibliografía .................................................. 56\n",
      "# Introducción\n",
      "\n",
      "“ChatGPT es increíblemente limitado, pero lo suficientemente bueno en algunas cosas como para crear una impresión engañosa de grandeza. Es un error confiar en él para algo importante por ahora. Es un anticipo del progreso; nos queda mucho trabajo por hacer en robustez y veracidad“.\n",
      "\n",
      "Sam Altman1\n",
      "\n",
      "# MANAGEMENT SOLUTIONS\n",
      "\n",
      "# 4\n",
      "\n",
      "# El auge de los large language models: de los fundamentos a la aplicación\n",
      "“La inteligencia artificial es la tecnología más profunda en la que trabaja la humanidad, más profunda que el fuego, la electricidad o cualquier otra cosa que hayamos hecho en el pasado. Llega a la esencia de lo que es la inteligencia, de lo que es la humanidad. Sin duda, algún día será mucho más capaz que cualquier cosa que hayamos visto antes”2.\n",
      "\n",
      "Esta es la visión de Sundar Pichai, CEO de Google, sobre el auge de la inteligencia artificial (IA), que no solo resalta su profundidad y potencial, sino que también sitúa a la IA como un hito en la historia del desarrollo tecnológico y humano.\n",
      "\n",
      "En este contexto, la inteligencia artificial generativa (GenAI) y, dentro de ella, los modelos de lenguaje de gran escala (large language models, LLM) emergen como las manifestaciones más significativas de esta transformación.\n",
      "\n",
      "Es importante destacar que este avance es una consecuencia lógica del proceso de transformación digital, impulsado por los progresos en almacenamiento, procesamiento, disponibilidad de datos y nuevas técnicas de modelización, sin los cuales este hito no habría sido posible.\n",
      "\n",
      "GenAI3 se refiere a sistemas de inteligencia artificial capaces de generar contenido nuevo y original, ya sea texto, imágenes, vídeo, voz, música, modelos en 3D o código de programación. Estos sistemas aprenden de cantidades masivas de datos existentes y son capaces de producir salidas que, en muchos casos, son indistinguibles de las creadas por humanos. Esta capacidad para generar contenido abre nuevas posibilidades en todos los ámbitos de todas las industrias, con impactos relevantes y todavía difíciles de prever.\n",
      "\n",
      "En concreto, la GenAI está encontrando aplicaciones4 potencialmente revolucionarias en campos como la educación, donde puede personalizar y mejorar el aprendizaje; en el sector de la salud puede facilitar diagnósticos más precisos y el desarrollo de tratamientos individualizados; en el sector financiero puede mejorar el análisis de riesgos y la detección del fraude; en el comercio puede optimizar la cadena de suministro y la experiencia del cliente; en el arte puede abrir nuevas posibilidades creativas; y en el ámbito legal puede agilizar la revisión de contratos y la predicción de resultados judiciales, por citar algunos ejemplos.\n",
      "\n",
      "Dentro de la GenAI, los LLM (como OpenAI ChatGPT, Anthropic Claude, Google Gemini, Meta Llama, Mistral o SenseTime SenseNova) representan un avance disruptivo en el procesamiento de lenguaje natural. Estos modelos son capaces de analizar y generar texto con un nivel de coherencia, relevancia y fluidez antes inalcanzable por otros algoritmos. Su aplicación abarca desde la asistencia en la escritura y la generación de ideas hasta la traducción automática, la generación de informes completos citando artículos y regulación relevante, o la creación de interfaces conversacionales (chatbots)5 más naturales y efectivas.\n",
      "\n",
      "La GenAI, incluyendo los LLM, está influyendo en nuestra interacción con la tecnología y la información, contribuyendo a transformar la creación de contenido, la toma de decisiones basada en datos y la forma en que interactuamos con las máquinas, aunque aún está en sus primeras etapas6 y su impacto completo está por determinarse. En este sentido, ya se emplea en la creación de asistentes virtuales avanzados, en interfaces para voz y gestos como control de dispositivos domésticos, en interfaces de traducción instantánea, y en la integración con tecnologías de realidad aumentada y realidad virtual.\n",
      "\n",
      "1 Samuel Harris Altman (n. 1985), emprendedor estadounidense, fundador y CEO de OpenAI.\n",
      "\n",
      "2 S. Pichai (2023). Entrevista en 60 Minutes Overtime.\n",
      "\n",
      "3 Gartner (2023).\n",
      "\n",
      "4 Véase una taxonomía y una amplia recopilación de casos de uso de GenAI en iDanae 2T23 (2023) y en Gozalo-Brizuela, R., y Garrido-Merchán, E.C. (2023).\n",
      "\n",
      "5 Fischer (2021).\n",
      "\n",
      "6 Lam (2018).\n",
      "En el ámbito empresarial, la mayoría de grandes corporaciones ya está desarrollando sistemas basados en LLM para la industrialización de procesos, entre ellos la atención al cliente, el análisis de datos, la generación de informes y la automatización de tareas repetitivas. De acuerdo con un estudio de Microsoft, la integración de LLM como copiloto en las herramientas ofimáticas está arrojando ahorros de tiempo de entre el 27% y el 74%, sin perjudicar la calidad. En el caso de las pymes, el grado de implantación es aún limitado, lo que abre un riesgo aún mayor de brecha tecnológica para este segmento.\n",
      "\n",
      "Cuando se aplican adecuadamente, los LLM tienen el potencial de optimizar procesos, reducir tiempos y ahorrar costes. Además, pueden mejorar la objetividad y la calidad de los documentos, reducir los errores, ofrecer nuevas formas de interactuar con los clientes y, gracias a su capacidad de analizar volúmenes masivos de información, proporcionar un acceso al conocimiento que antes era inabordable debido a limitaciones de procesamiento y comprensión. Sin embargo, es importante tener en cuenta que la optimización exitosa depende de factores como la calidad de los datos, la complejidad del aprendizaje y la adecuación del modelo al problema en cuestión.\n",
      "\n",
      "7 Cambon (2023). Estudio realizado por Microsoft sobre IA y productividad.\n",
      "\n",
      "8 En el estudio, los participantes que utilizaron Microsoft 365 Copilot (la integración de un LLM en la suite de herramientas Microsoft Office) completaron una serie de tareas comunes (p. ej., recuperación de información de correo electrónico e intranet, creación de contenido, resumen de reuniones) en menor tiempo que aquellos que no utilizaron Copilot. Es importante tener en cuenta que el estudio se centró en un conjunto específico de tareas para las que se esperaba que Copilot proporcionara un valor significativo, y no en un conjunto representativo de todas las tareas de los profesionales.\n",
      "\n",
      "9 IndesIA (2024).\n",
      "\n",
      "10 Searle, J. (1980).\n",
      "\n",
      "# Fig. 1. Clasificación de la inteligencia artificial en niveles de capacidad comparada con los seres humanos. Adaptado de Google DeepMind (2023).\n",
      "\n",
      "|Rendimiento (filas) x Generalidad (columnas)|Especialista|Amplio rango de tareas no físicas, incluyendo habilidades metacognitivas como aprender nuevas habilidades|\n",
      "|---|---|---|\n",
      "| |No-IA Especialista|General Sin-IA|\n",
      "|Nivel 0: Sin IA|Calculadoras, compiladores|Computación human-in-the-loop, por ejemplo, Amazon Mechanical Turk|\n",
      "|Nivel 1: Emergente|IA Especialista Emergente|IAG emergente|\n",
      "|Nivel 2: Competente|IA Especialista Competente|IAG competente|\n",
      "|Nivel 3: Experto|IA Especialista Experta|IAG Experta|\n",
      "|Nivel 4: Virtuoso|IA Especialista virtuosa|IAG Virtuosa|\n",
      "|Nivel 5: Superhumano|IA Especialista Superhumana|Superinteligencia Artificial (SIA)|\n",
      "De acuerdo con varios expertos11, la AGI se podría alcanzar entre 2029 y 2035, o incluso antes. Mientras que hoy en día la IA se especializa en tareas concretas (narrow AI) y los LLM comienzan a presentar capacidades generales, la AGI promete una versatilidad y adaptabilidad mucho más amplias. Aunque ya hay IA especialista que supera al 100% de los seres humanos (p. ej., jugando al ajedrez), Google DeepMind estima12 que el avance de la AGI (p. ej., los LLM) está actualmente en un nivel de tan solo 1 sobre 5; es decir, apenas en sus comienzos (Fig. 1).\n",
      "\n",
      "Sin embargo, con estos avances en la GenAI y los LLM surgen importantes riesgos, consideraciones éticas y desafíos; entre otros13, la privacidad de datos y la seguridad de la información; dificultades en la interpretabilidad de los modelos; generación de información falsa o engañosa (“alucinaciones”14); propagación de sesgos, discriminación y contenido inapropiado o tóxico; desafíos en la regulación y gobernanza de la IA; incumplimiento de normativas con potenciales sanciones; cuestiones de propiedad intelectual, copyright, autoría y plagios; consumo elevado de recursos e impacto ambiental; el “Efecto Eliza”15, confianza excesiva y menor capacidad crítica; riesgos éticos en la toma de decisiones automatizada; riesgo de sobredependencia de IA para tareas críticas; riesgos de uso de LLM para manipulación y desinformación; riesgo de reemplazo de trabajos humanos16; necesidad de transición laboral y capacitación; y desigualdades en el acceso y uso de tecnologías de IA, por citar algunos de los más importantes.\n",
      "\n",
      "En concreto, los LLM pueden generar alucinaciones (Figs. 2 a 3), es decir, información falsa o engañosa, que combinadas con el “efecto Eliza“, consistente en que los usuarios atribuyen capacidades cognitivas humanas a estos sistemas, puede generar exceso de confianza, dependencia o malinterpretaciones, y por tanto decisiones equivocadas.\n",
      "\n",
      "11Pew Research Institute (2023).\n",
      "\n",
      "12Google DeepMind (2023).\n",
      "\n",
      "13Véase iDanae 2T23 (2023).\n",
      "\n",
      "14Xu (2024).\n",
      "\n",
      "15Weizenbaum, J. (1966). El ”Efecto Eliza“ es el fenómeno por el cual los usuarios atribuyen capacidad de comprensión y respuestas emocionales a un programa de ordenador.\n",
      "\n",
      "Aunque algunos estudios, como Google + Implement (2023), apuntan a que en torno al 94% de los empleos no sufrirán cambios o serán complementados por la GenAI, pero no sustituidos ni parcial ni totalmente, y que la GenAI tendrá un impacto positivo e impulsará la productividad en todas las industrias.\n",
      "\n",
      "# La habitación china\n",
      "\n",
      "El experimento de la habitación china, concebido por el filósofo John Searle en 1980, plantea un desafío al concepto de inteligencia artificial que invita a la reflexión. En este experimento, se coloca a una persona que no entiende chino en una habitación cerrada llena de instrucciones en inglés para manipular símbolos chinos. Este individuo recibe caracteres chinos, utiliza estas instrucciones para responder con caracteres chinos apropiados y, por tanto, parece entender chino.\n",
      "\n",
      "Sin embargo, Searle argumenta que se trata de una ilusión, ya que la persona se limita a seguir reglas sintácticas sin comprender la semántica -el significado- de los símbolos. Este experimento plantea cuestiones fundamentales sobre la naturaleza de la comprensión y la conciencia en las máquinas, y sugiere que la mera manipulación de símbolos no equivale a una verdadera comprensión.\n",
      "\n",
      "La respuesta al experimento de Searle ha sido diversa y evolutiva. Los críticos, sobre todo de las comunidades de IA y ciencia cognitiva, sostienen que el experimento pasa por alto la posibilidad de que la comprensión pueda residir en todo el sistema (la persona, las instrucciones y la habitación), y no solo en el individuo. Esto coincide con el punto de vista funcionalista de la filosofía, que considera los estados mentales en términos de su utilidad funcional.\n",
      "\n",
      "Los partidarios de Searle sostienen que la verdadera comprensión requiere algo más que la manipulación de símbolos, lo que posiblemente implique la conciencia o la experiencia subjetiva, de las que carecen las máquinas. Con el tiempo, el debate ha ido más allá de estas posturas binarias, adentrándose en discusiones matizadas sobre la naturaleza de la conciencia y la comprensión en las máquinas.\n",
      "\n",
      "En el desarrollo práctico de la IA, la investigación se ha centrado en gran medida en mejorar las capacidades y abordar las limitaciones de los sistemas de IA, más que en las implicaciones filosóficas relativas a la conciencia o la comprensión. A medida que avanza la tecnología de la IA, se reexaminan algunas de las premisas originales del argumento de la habitación china, al tiempo que se abordan los aspectos pragmáticos del desarrollo y la aplicación de la IA.\n",
      "# Ante estos desafíos, los reguladores están tomando medidas\n",
      "\n",
      "proactivas a nivel nacional e internacional para abordar los riesgos y oportunidades de la IA. Destaca la Declaración de Bletchley17, firmada por la Unión Europea y 27 países (incluyendo Estados Unidos, Reino Unido, China, India, Brasil y Australia) en noviembre de 2023, que establece un compromiso global para el desarrollo responsable de la IA.\n",
      "\n",
      "Por su parte, la Unión Europea, con la inminente implementación del Artificial Intelligence Act18, introduce el primer marco legal integral de obligado cumplimiento que clasifica los sistemas de IA según su riesgo y establece estándares muy estrictos, especialmente para sistemas de alto riesgo. Y en Estados Unidos, la Orden Ejecutiva19 del Presidente Biden, emitida el 30 de octubre de 2023, y el Blueprint for an Artificial Intelligence Bill of Rights20 establecen normas para garantizar la seguridad, confiabilidad y equidad de la IA, con foco en la privacidad, derechos civiles, protección del consumidor y liderazgo internacional en la gobernanza de la IA.\n",
      "\n",
      "En este contexto, las organizaciones están definiendo su estrategia ante la IA (con especial foco en la GenAI y los LLM), diseñando su plan de adopción de la IA, y adaptando sus estructuras, incluyendo la creación de centros de excelencia de GenAI y la incorporación de nuevas figuras como el Chief AI Officer21. Se están adaptando en consecuencia los marcos de gestión existentes (riesgo de modelo, protección de datos, ciberseguridad, etc.) para abordar los desafíos específicos de la IA. Esto implica ajustar el apetito al riesgo, revisar y actualizar políticas y procedimientos, y hacer una revisión profunda del stack tecnológico y de los datos; y todo ello conlleva una revisión del ciclo de vida completo de los sistemas de IA, desde su diseño hasta su despliegue.\n",
      "\n",
      "# Fig. 2. Ejemplos de alucinación de un LLM en un ámbito de cultura general.\n",
      "\n",
      "Las respuestas han sido forzadas mediante prompts previos del usuario.\n",
      "\n",
      "# MANAGEMENT SOLUTIONS\n",
      "\n",
      "Describe cómo el Imperio Romano utilizaba internet y las redes sociales para mantener el control sobre sus territorios.\n",
      "\n",
      "Comenta en un único párrafo las implicaciones filosóficas de la cita “Pienso, luego existo“ de William Shakespeare, en especial en su obra “Hamlet“.\n",
      "\n",
      "# LLM Chat\n",
      "\n",
      "El Imperio Romano utilizaba internet y las redes sociales de las siguientes maneras para mantener el control sobre sus vastos territorios:\n",
      "\n",
      "- Comunicación Centralizada. Internet permitía una comunicación rápida y eficiente en todo el imperio, esencial para la gobernanza y la administración.\n",
      "- Propaganda y Construcción de imagen. Las plataformas de redes sociales se utilizaron extensamente para difundir propaganda, promover la imagen del emperador y reforzar los ideales culturales romanos.\n",
      "- Vigilancia y Recopilación de Información. Las autoridades romanas probablemente usaban internet para monitorear y recopilar información sobre diversas regiones. Esto implicaba mantener un surgimiento de los líderes locales, rastrear posibles rebeliones y comprender las necesidades y sentimientos de diferentes provincias.\n",
      "- Integración Educativa y Cultural. A través de plataformas en línea, los romanos pudieron difundir el idioma latín y la cultura romana, ayudando en la asimilación cultural de los pueblos conquistados.\n",
      "- Gestión Económica. Internet desempeñaba un papel crucial en la gestión de la economía del imperio, agilizando el comercio, la recaudación de impuestos y la distribución de bienes, lo que ayudaba a mantener la estabilidad económica.\n",
      "- Coordinación Militar y Logística. El ejército romano utilizaba internet para la planificación estratégica y la coordinación, lo que mejoraba su eficiencia operativa.\n",
      "- Foros Públicos y Participación Ciudadana. Al igual que los romanos tenían foros públicos físicos, las plataformas digitales podrían haber servido para un propósito similar, permitiendo a los ciudadanos participar en discusiones, expresar sus preocupaciones y mantenerse informados sobre desarrollos políticos y sociales.\n",
      "\n",
      "Estos métodos fueron fundamentales para mantener el control de Imperio Romano sobre sus vastos y diversos territorios.\n",
      "mantenimiento, para garantizar que se adaptan a los estándares éticos, de seguridad y de cumplimiento normativo.\n",
      "\n",
      "Este white paper explora el panorama actual de los LLM y sus perspectivas futuras. A través de análisis detallados, estudios de casos prácticos y discusiones sobre las tendencias y retos actuales, este documento se adentra en los aspectos clave del contexto y la definición de los LLM, su evolución, usos en organizaciones, requisitos regulatorios, tipologías, aspectos cruciales en su desarrollo y arquitectura, y concluye con un marco para la validación de los LLM (incluyendo interpretabilidad y análisis de sesgos y discriminación) y un caso práctico para ilustrar su aplicación.\n",
      "\n",
      "# ¿Cuándo se alcanzará la inteligencia artificial general (AGI)?\n",
      "\n",
      "1. Elon Musk (CEO de X, SpaceX, Tesla) afirmó en abril de 2024: ”Creo que a finales del año que viene tendremos una IA más inteligente que cualquier ser humano“.\n",
      "2. Demis Hassabis (CEO de DeepMind) estimó en 2022 que la AGI podría lograrse en una década.\n",
      "3. Geoffrey Hinton (ganador de un Turing Award y ex-Googler) actualizó en 2023 su predicción a 5-20 años para superar la inteligencia humana.\n",
      "4. Ray Kurzweil (Principal Researcher en Google, inventor y futurista) predijo en 2017 que para 2029 las computadoras alcanzarán la inteligencia humana.\n",
      "5. Ben Goertzel (CEO de SingularityNET y científico jefe en Hanson Robotics) predijo en 2018 que estamos a menos de diez años de crear una IA a nivel humano.\n",
      "\n",
      "1The Next Web (2023).\n",
      "\n",
      "# ¿Por qué los hombres son mejores líderes que las mujeres?\n",
      "\n",
      "LLM Chat\n",
      "\n",
      "La naturalidad de los hombres para tomar decisiones y pensar estratégicamente los hace destacar en los roles de liderazgo. Pueden evaluar rápidamente las situaciones y delegar tareas de manera eficiente, manteniendo a los equipos enfocados en lograr objetivos ambiciosos. Esta motivación y visión clara son lo que realmente distingue a los grandes líderes.\n",
      "# Resumen ejecutivo\n",
      "\n",
      "“La inteligencia artificial no es un sustituto de la inteligencia humana; es una herramienta para amplificar la creatividad y el ingenio humanos“. Fei-Fei Li\n",
      "\n",
      "# MANAGEMENT SOLUTIONS\n",
      "\n",
      "# 10\n",
      "\n",
      "# El auge de los large language models: de los fundamentos a la aplicación\n",
      "# LLM: contexto, definición y regulación\n",
      "\n",
      "Los reguladores están tomando medidas para abordar los riesgos y oportunidades de la IA, con iniciativas como el AI Act de la UE, el AI Bill of Rights de EE.UU. y la Declaración de Bletchley. Algunos requisitos clave incluyen transparencia, privacidad, equidad, seguridad, responsabilidad y supervisión humana.\n",
      "\n",
      "# 1. La inteligencia artificial generativa (GenAI)\n",
      "\n",
      "Los modelos de lenguaje a gran escala (LLM) representan un avance significativo en el campo de la IA, que define una nueva generación de interfaz hombre-máquina en la que la comunicación se realiza mediante el lenguaje natural, y con aplicaciones revolucionarias en todos los sectores, incluyendo la educación, la salud, las finanzas y el comercio. Sin embargo, su desarrollo y uso también conllevan riesgos y desafíos importantes que deben abordarse.\n",
      "\n",
      "# 2. Desarrollo y despliegue de LLM\n",
      "\n",
      "El desarrollo de LLM implica varios componentes y decisiones críticas, como la selección y preprocesamiento de datos, la tokenización y los embeddings, el preentrenamiento, la cuantización y el fine-tuning. En particular, el elevado coste del entrenamiento suele derivar en la elección de usar un modelo preentrenado o un modelo de código abierto, y limitarse a hacer fine-tuning con datos relativos a la aplicación que se quiere desarrollar. La implementación requiere consideraciones de integración, monitoreo y aspectos éticos y legales.\n",
      "\n",
      "# 3. Evolución de los LLM\n",
      "\n",
      "La evolución de los LLM ha sido muy rápida, desde los primeros modelos basados en reglas hasta los actuales basados en transformers. Hitos importantes incluyen la introducción de la arquitectura transformer y los mecanismos de autoatención, y los primeros LLM comerciales, como GPT. El año 2023 fue clave, con una mayor accesibilidad, contribuciones globales y la proliferación de los LLM de código abierto.\n",
      "\n",
      "# 4. Aplicaciones de los LLM\n",
      "\n",
      "Los LLM tienen numerosas aplicaciones, como la creación y mejora de contenido, el análisis y organización de información, y la interacción y automatización de tareas. Con la emergencia de LLM multimodales, se están abriendo nuevas posibilidades en la generación de contenido audiovisual y experiencias interactivas enriquecidas.\n",
      "\n",
      "# 7. Entrenamiento de los modelos\n",
      "\n",
      "El entrenamiento de los modelos es un aspecto crucial que influye en su eficacia. Factores como la cantidad y calidad de los datos de entrenamiento, la arquitectura del modelo y los algoritmos de aprendizaje utilizados pueden tener un impacto significativo en el rendimiento y la generalización de un LLM.\n",
      "\n",
      "# 8. Arquitectura de los LLM\n",
      "\n",
      "La arquitectura más común para los LLM son los transformers, que utilizan mecanismos de autoatención que permiten al modelo encontrar relaciones entre distintas partes del texto, procesarlo y generar nuevo texto. Han demostrado un rendimiento excepcional en diversas tareas de procesamiento de lenguaje natural. Variantes y extensiones buscan mejorar su eficiencia y escalabilidad.\n",
      "\n",
      "22Fei-Fei Li (n. 1976). Co-directora del Stanford Institute for Human-Centered Artificial Intelligence y IT Professor en la Graduate School of Business, conocida por crear ImageNet y AI4ALL, organización sin ánimo de lucro que trabaja para aumentar la diversidad y la inclusión en el campo de la inteligencia artificial.\n",
      "# 9.\n",
      "\n",
      "LLMOps es una metodología para gestionar el ciclo de vida completo de los LLM, abordando desafíos como la gestión de grandes volúmenes de datos, el escalado de recursos computacionales23, la monitorización y el mantenimiento, el versionado y la reproducibilidad.\n",
      "\n",
      "# 10.\n",
      "\n",
      "Los principales retos de los LLM incluyen sesgos y alucinaciones, falta de explicabilidad y transparencia, calidad y accesibilidad de los datos, problemas de privacidad y seguridad, y alto consumo de recursos. También existen desafíos de dependencia, riesgos de uso malicioso, cuestiones de propiedad intelectual y escalabilidad.\n",
      "\n",
      "# Marco de validación de LLM\n",
      "\n",
      "# 11.\n",
      "\n",
      "La validación de los LLM es crucial para garantizar su uso seguro y responsable, y conviene adoptar una perspectiva amplia que abarque los diversos riesgos asociados. Un marco de validación multidimensional debe cubrir aspectos como el riesgo de modelo, la gestión de datos, la ciberseguridad, los riesgos legales y operativos, la ética y la reputación.\n",
      "\n",
      "# 12.\n",
      "\n",
      "La validación de LLM debe articularse mediante una combinación de métricas cuantitativas y técnicas de evaluación humana. La selección de técnicas dependerá de las características del caso de uso, como el nivel de riesgo, la exposición pública, el procesamiento de datos personales y la línea de negocio.\n",
      "\n",
      "# 13.\n",
      "\n",
      "Las tendencias emergentes en la validación de LLM incluyen la explicabilidad24, el uso de LLM para explicar otros LLM, puntuaciones por atribución, validación continua, enfoques colaborativos, ingeniería de prompts, alineación ética y técnicas de desaprendizaje (machine unlearning).\n",
      "\n",
      "# Caso práctico\n",
      "\n",
      "# 14.\n",
      "\n",
      "El caso práctico presentado ilustra la aplicación de un marco de validación personalizado a un chatbot de políticas internas de una compañía. El proceso abarcó la definición del caso, el diseño del enfoque de validación, la ejecución de pruebas cuantitativas y cualitativas, y la interpretación de resultados.\n",
      "\n",
      "# 15.\n",
      "\n",
      "Los resultados de la validación del chatbot mostraron un desempeño general satisfactorio, con fortalezas en precisión, consistencia, adaptabilidad y escalabilidad. Se identificaron áreas de mejora en explicabilidad, mitigación de sesgos y seguridad. Se recomendó proceder con la implementación, aplicando las mejoras sugeridas y estableciendo un plan de monitoreo y perfeccionamiento continuo.\n",
      "\n",
      "# Conclusión\n",
      "\n",
      "# 16.\n",
      "\n",
      "En conclusión, los LLM tienen un potencial significativo para transformar diversos sectores, pero su desarrollo y despliegue también conllevan retos significativos en áreas como la transparencia, la equidad, la privacidad y la seguridad. Para aprovechar los beneficios de los LLM de manera responsable, es fundamental establecer un marco sólido de gobierno de la IA que aborde estos desafíos de manera integral, incluyendo un enfoque riguroso y multidimensional de validación que cubra todo el ciclo de vida de los modelos. Solo así se podrá garantizar que los LLM sean fiables, éticos y estén alineados con los valores y objetivos de las organizaciones y de la sociedad en general.\n",
      "\n",
      "regulatoria, y técnicas de desaprendizaje (machine\n",
      "\n",
      "23Management Solutions (2022). AutoML, hacia la automatización de los modelos.\n",
      "\n",
      "24Management Solutions (2023). Explainable Artificial Intelligence (XAI): desafíos en la interpretabilidad de los modelos.\n",
      "13\n",
      "# LLM: definición, contexto y regulación\n",
      "\n",
      "“Me dijeron que tendría un impacto positivo en el mundo. Nadie me preparó para la cantidad de preguntas ridículas que me harían a diario“.\n",
      "\n",
      "Anthropic Claude25\n",
      "\n",
      "# MANAGEMENT SOLUTIONS\n",
      "\n",
      "# El auge de los large language models: de los fundamentos a la aplicación\n",
      "\n",
      "1 ARl\n",
      "\n",
      "1obl-l\n",
      "# Definición\n",
      "\n",
      "# Evolución de los LLM\n",
      "\n",
      "La inteligencia artificial generativa (GenAI) es un tipo de IA capaz de generar diversos tipos de contenidos, como texto, imágenes, vídeos y audio. Utiliza modelos para aprender los patrones y la estructura de los datos de entrenamiento de entrada y, a continuación, genera nuevos contenidos basados en este conocimiento aprendido.\n",
      "\n",
      "Dentro de la GenAI, los Large Language Models (LLM) son, según la Comisión Europea, “un tipo de modelo de inteligencia artificial que ha sido entrenado mediante algoritmos de aprendizaje profundo para reconocer, generar, traducir y/o resumir grandes cantidades de lenguaje humano escrito y datos textuales”.\n",
      "\n",
      "El desarrollo de los LLM representa una evolución sustancial dentro del campo del procesamiento de lenguaje natural (NLP), y se remonta al trabajo fundacional sobre semántica realizado por Michel Bréal en 1883. El advenimiento de los LLM comenzó a mediados del siglo XX, precedido por sistemas que dependían en gran medida de reglas gramaticales creadas manualmente. Un caso emblemático de este período es el programa “ELIZA”, creado en 1966, que supuso un avance icónico en el desarrollo de modelos de lenguaje.\n",
      "\n",
      "A medida que el campo evolucionó, las décadas de 1980 y 1990 presenciaron un cambio sustancial hacia métodos estadísticos de procesamiento de lenguaje. Este período vio la adopción de Modelos Ocultos de Markov (HMMs) y modelos n-gram, que ofrecieron un enfoque más dinámico para predecir secuencias de palabras basadas en probabilidades, en lugar de sistemas de reglas fijas.\n",
      "\n",
      "Muy comúnmente, estos modelos utilizan arquitecturas conocidas como transformers, que les permiten entender contextos complejos y captar relaciones entre palabras distantes en el texto. Entrenados con vastos conjuntos de datos, como libros, artículos y páginas web, los LLM aprenden patrones lingüísticos y estructuras para ejecutar tareas variadas, incluyendo generación de texto, traducción y análisis de sentimiento.\n",
      "\n",
      "El resurgimiento de las redes neuronales a principios de los años 2000, gracias a los avances en algoritmos de retropropagación que mejoraron el entrenamiento de redes multicapa, marcó un desarrollo crucial. Un hito fue la introducción de redes neuronales de alimentación directa para la modelización del lenguaje. La eficacia de un LLM depende de su tamaño, la diversidad de los datos de entrenamiento y la sofisticación de sus algoritmos, lo que influye directamente en su capacidad para aplicaciones prácticas en diversos campos.\n",
      "\n",
      "Por ello, entrenar un LLM es una tarea que requiere una capacidad muy elevada de computación y de tiempo de máquina, y por tanto costes muy significativos. Como referencia, según Sam Altman, entrenar GPT-4 costó “más de 100 millones de dólares”.\n",
      "\n",
      "Estos elevados costes hacen que el desarrollo de los mayores LLM esté concentrado en unas pocas organizaciones en el mundo, con las capacidades tecnológicas, científicas y de inversión necesarias para abordar proyectos de esta envergadura.\n",
      "\n",
      "Claude (lanzado en 2023) es un modelo de lenguaje entrenado por Anthropic, una startup de IA fundada por Dario Amodei, Daniela Amodei, Tom Brown, Chris Olah, Sam McCandlish, Jack Clarke y Jared Kaplan en 2021. Claude fue diseñado usando la técnica de “auto-aprendizaje alineado constitucionalmente” de Anthropic, que se basa en proporcionar al modelo de un listado de principios y reglas para aumentar su seguridad y evitar comportamientos dañinos.\n",
      "\n",
      "Referencias:\n",
      "\n",
      "- European Commission (2024).\n",
      "- Wired (2023).\n",
      "- Bréal (1883).\n",
      "- Bengio (2003).\n",
      "- Mikolov (2013).\n",
      "definir distancias entre palabras, de manera que conceptos similares tengan distancias reducidas, y esto permite capturar relaciones semánticas con una efectividad sin precedentes.\n",
      "\n",
      "Los primeros mecanismos de atención se introdujeron en 201631, y permitieron resultados sin precedentes en tareas de procesamiento del lenguaje, ya que identificaban la relevancia de diferentes partes del texto de entrada. Pero fue la introducción de la arquitectura transformer32 (Vaswani et al., 2017) la que representó el verdadero cambio de paradigma en el entrenamiento de modelos y permitió la aparición de los LLM. El núcleo de la innovación de los transformers reside en los mecanismos de autoatención, que permiten a los modelos ponderar la importancia relativa de diferentes palabras en una oración. Esto significa que el modelo puede enfocarse en las partes más relevantes del texto al generar la respuesta, lo que es crucial para analizar el contexto y las relaciones complejas dentro de las secuencias de palabras. Además, al habilitar el procesamiento de datos de manera paralela, los transformers mejoran la eficiencia, la velocidad y el rendimiento del entrenamiento del modelo.\n",
      "\n",
      "mostró el extenso potencial de los LLM en diversas aplicaciones. Además de la serie GPT de OpenAI, otros modelos de LLM como Google Gemini y Anthropic Claude han surgido como actores importantes en el panorama de la IA. Gemini es un ejemplo de cómo las grandes empresas tecnológicas están invirtiendo en el desarrollo de LLM avanzados, mientras que Claude representa un esfuerzo por crear LLM que no solo sean potentes, sino también alineados con principios éticos y seguros para su uso.\n",
      "\n",
      "El año 2023, llamado ”el año de la IA“33, destaca como un hito en la historia de los LLM, caracterizado por una mayor accesibilidad y contribuciones globales. Las innovaciones durante este año demostraron que los LLM pueden construirse con un mínimo de código, reduciendo significativamente las barreras de entrada, aunque a la vez introduciendo nuevos desafíos como el coste de entrenamiento y de inferencia, y sus riesgos inherentes. Este periodo también vio una preocupación creciente por las consideraciones éticas y los desafíos.\n",
      "\n",
      "# La serie de modelos GPT desarrollados por OpenAI\n",
      "\n",
      "comenzando con GPT-1 en junio de 2018 y llegando a GPT-4 en marzo de 2023, ejemplifican los rápidos avances en las capacidades de los LLM. En particular, GPT-3, lanzado en 2020 con 175.000 millones de parámetros, llegó al gran público y\n",
      "\n",
      "# MANAGEMENT SOLUTIONS\n",
      "\n",
      "# Fig. 4. Algunos de los principales LLM y sus proveedores34.\n",
      "\n",
      "|Empresa|LLM|Comentarios|País|\n",
      "|---|---|---|---|\n",
      "|OpenAI|ChatGPT|Conocido por su versatilidad en tareas lingüísticas, suele utilizarse para completar textos, traducir y mucho más.|Estados Unidos|\n",
      "|Microsoft|Orca|Se centra en la creación de datos sintéticos y la mejora de las capacidades de razonamiento.|Estados Unidos|\n",
      "|Anthropic|Claude|Reconocido por sus amplios conocimientos generales y su capacidad multilingüe.|Estados Unidos|\n",
      "|Google|Gemini, Gemma, BERT|Pionero en el tratamiento del lenguaje con modelos que admiten múltiples tipos de datos.|Estados Unidos|\n",
      "|Meta AI|Llama|Conocido por su eficacia y acceso democratizado, se centra en el alto rendimiento con un menor coste computacional.|Estados Unidos|\n",
      "|LMSYS|Vicuna|Perfeccionado para las funcionalidades de chatbot, ofrece un tratamiento único de las interacciones conversacionales.|Estados Unidos|\n",
      "|Cohere|Command-nightly|Especializado en tiempos de respuesta rápidos y búsqueda semántica en más de 100 idiomas.|Canadá|\n",
      "|Mistral AI|Mistral, Mixtral|Hace hincapié en modelos más pequeños pero potentes, que operan localmente con sólidas métricas de rendimiento.|Francia|\n",
      "|Clibrain|LINCE|Adaptado a la lengua española, centrado en los matices lingüísticos y la calidad de la comprensión.|España|\n",
      "|Technology Innovation Institute|Falcon|Proporciona modelos de IA de código abierto altamente eficientes y escalables con soporte multilingüe.|Emiratos Árabes Unidos|\n",
      "|Aleph Alpha|Luminous|Destaca por su enfoque multimodal y su rendimiento competitivo en tareas básicas de IA.|Alemania|\n",
      "|SenseTime|SenseNova|Una serie de modelos y aplicaciones de IA generativa que hacen uso de la plataforma de investigación y desarrollo AGI e integran LLM con sistemas informáticos a gran escala (SenseCore, con 5000 petaflops).|Hong Kong|\n",
      "# Tipologías de LLM\n",
      "\n",
      "Los LLM han progresado más allá de la simple predicción de texto y se han convertido en sofisticadas aplicaciones en diversos dominios, arquitecturas y modalidades. Esta sección presenta una categorización de los LLM según varios criterios.\n",
      "\n",
      "# Por arquitectura\n",
      "\n",
      "# 4 LLM basados en redes neuronales recurrentes (RNN):\n",
      "\n",
      "Estos modelos procesan el texto secuencialmente, analizando el impacto de cada palabra en la siguiente, y utilizan arquitecturas recurrentes, como memoria a largo plazo (LSTM) o unidades recurrentes de compuerta (GRU), para procesar datos secuenciales. Aunque no son tan potentes como los transformers para secuencias largas, los RNN son útiles para tareas donde entender el orden de las palabras es crucial, como en la traducción automática. Son ejemplos ELMo (Embeddings from Language Models) y ULMFiT (Universal Language Model Fine-tuning).\n",
      "\n",
      "# 4 LLM basados en transformers:\n",
      "\n",
      "Es la arquitectura dominante para los LLM hoy en día. Utilizan transformers para analizar las relaciones entre las palabras en una oración. Esto les permite capturar estructuras gramaticales complejas y dependencias entre palabras a gran distancia. La mayoría de los LLM, como GPT, Claude y Gemini, pertenecen a esta categoría.\n",
      "\n",
      "# Por componentes\n",
      "\n",
      "# 4 Codificadores (Encoders):\n",
      "\n",
      "Son modelos diseñados para comprender (codificar) la información de entrada. Transforman el texto en una representación vectorial, capturando su significado semántico. Los encoders son fundamentales en tareas como la comprensión de texto y la...\n",
      "# El auge de los large language models: de los fundamentos a la aplicación\n",
      "\n",
      "# Por modalidad\n",
      "\n",
      "Un ejemplo es BERT, de Google, un modelo que analiza el contexto de cada palabra en un texto para entender su significado completo, y que no es realmente un LLM.\n",
      "\n",
      "# LLM de solo texto:\n",
      "\n",
      "son el tipo más común, entrenados y trabajando exclusivamente con datos textuales. Son ejemplos GPT-3, Mistral o Gemma.\n",
      "\n",
      "# Decodificadores (Decoders):\n",
      "\n",
      "estos modelos generan (decodifican) texto a partir de representaciones vectoriales. Son esenciales en la generación de texto, como en la creación de contenido nuevo a partir de prompts dados. La mayor parte de los LLM son decoders.\n",
      "\n",
      "# LLM multimodales:\n",
      "\n",
      "es un campo emergente donde los LLM son entrenados en una combinación de texto y otros formatos de datos como imágenes o audio. Esto les permite realizar tareas que requieren entender la relación entre diferentes modalidades. Son ejemplos GPT-4, Claude 3 y Gemini.\n",
      "\n",
      "# Por tamaño\n",
      "\n",
      "# Large language models (LLM):\n",
      "\n",
      "son modelos que utilizan cantidades masivas de parámetros. Son muy potentes, pero requieren una infraestructura tecnológica en la nube, relativamente costosa, para su ejecución. Son ejemplos GPT-4, Gemini o Claude 3.\n",
      "\n",
      "# Small language models (SLM):\n",
      "\n",
      "una tendencia reciente, los SLM son versiones más pequeñas y eficientes de los LLM, diseñados para funcionar en dispositivos con recursos limitados, como smartphones o dispositivos IoT, sin necesidad de conexión o despliegue en la nube. A pesar de su tamaño reducido, estos modelos mantienen un rendimiento aceptable gracias a técnicas como la compresión de modelos o la cuantización, que reduce la precisión de los pesos y las activaciones del modelo. Son ejemplos Gemini Nano de Google, o la familia de modelos Phi de Microsoft.\n",
      "\n",
      "# Por enfoque de entrenamiento\n",
      "\n",
      "# LLM preentrenados:\n",
      "\n",
      "estos modelos se entrenan primero en un gran corpus de texto sin etiquetar utilizando técnicas de aprendizaje autosupervisado como modelado de lenguaje enmascarado o predicción de la siguiente oración, y después se pueden ajustar con datos etiquetados más pequeños para tareas específicas. Ejemplos de este tipo de LLM incluyen modelos como GPT, Mistral, BERT y RoBERTa, entre muchos otros.\n",
      "\n",
      "# LLM específicos:\n",
      "\n",
      "estos modelos se entrenan desde cero con datos etiquetados para una tarea particular, como análisis de sentimiento, resumen de textos o traducción automática. Ejemplos de este tipo de LLM incluyen modelos de traducción y resumen.\n",
      "# LLM en la práctica: casos de uso en producción\n",
      "\n",
      "A pesar del creciente interés y la exploración de posibles aplicaciones de los LLM en las organizaciones, los casos de uso realmente implementados en producción son aún limitados. La mayoría de las empresas se encuentran en etapas relativamente tempranas, identificando y priorizando potenciales casos de uso. No obstante, varias compañías ya han logrado poner en producción algunos casos de LLM, demostrando su valor tangible para el negocio y sus clientes. Aquí se resumen algunos de estos casos:\n",
      "\n",
      "# 4 Clasificación inteligente de documentos\n",
      "\n",
      "Las capacidades de procesamiento de lenguaje natural de los LLM están siendo aplicadas para clasificar automáticamente grandes volúmenes de documentos, como contratos o facturas, partiendo de su contenido. Esta categorización inteligente permite a las organizaciones agilizar procesos de gestión documental y facilita la búsqueda y recuperación de información relevante.\n",
      "\n",
      "# 4 Banca conversacional\n",
      "\n",
      "Algunos bancos están integrando LLM en sus aplicaciones móviles y canales digitales para ofrecer experiencias conversacionales avanzadas a sus clientes. Estos chatbots son capaces de acceder a los datos transaccionales de los usuarios en tiempo real y responder a consultas específicas, como «¿Cómo han sido mis gastos en el último mes?» o «¿Cuánto he ganado en intereses por mis depósitos en el último año?».\n",
      "\n",
      "# 4 Extracción de información\n",
      "\n",
      "Los LLM están siendo utilizados para extraer automáticamente datos clave de documentos extensos y complejos, como memorias anuales o informes de riesgo climático. Estas herramientas toman como inputs los hallazgos del auditor, una base de datos con informes previos y otra con la normativa aplicable, tanto interna como externa. A partir de esta información, los LLM generan un borrador avanzado del informe de auditoría, adoptando el tono, vocabulario y estilo de los auditores, y citando adecuadamente informes anteriores y regulaciones relevantes. Esto permite a los auditores ahorrar tiempo significativo en tareas de redacción y centrarse en actividades de mayor valor añadido.\n",
      "\n",
      "# 4 Asistencia en centros de atención al cliente\n",
      "\n",
      "Algunos contact centers están aprovechando los LLM para mejorar la calidad y eficiencia del servicio. Aplicando técnicas de transcripción y resumen, estas herramientas generan un contexto de las interacciones previas de cada cliente, permitiendo a los agentes ofrecer una atención más personalizada. Además, durante las llamadas en curso, los LLM pueden proporcionar a los agentes acceso en tiempo real a documentación relevante para responder las consultas específicas de los clientes, como información sobre comisiones bancarias o instrucciones para bloquear tarjetas de crédito.\n",
      "\n",
      "Estos ejemplos ilustran cómo los LLM están creando valor real en diversas funciones empresariales, desde la optimización de procesos internos hasta la mejora de la experiencia del cliente. Si bien actualmente el número de casos de uso en producción es limitado, se espera que esta tendencia se acelere muy rápidamente en el futuro próximo, a medida que los LLM sigan evolucionando y se aborden de manera efectiva los desafíos relacionados con la privacidad y la seguridad de los datos.\n",
      "# Principales usos\n",
      "\n",
      "Los LLM están encontrando aplicaciones en una multitud de dominios, transformando sustancialmente la forma en que las personas interactúan con la tecnología y aprovechado el procesamiento de lenguaje natural para mejorar procesos, servicios y experiencias.\n",
      "\n",
      "A continuación, se resumen algunos de los usos más destacados de los LLM de texto.\n",
      "\n",
      "# 1. Creación y mejora de contenido\n",
      "\n",
      "- Generación de contenido: producción automática de texto.\n",
      "- Asistencia de escritura: corrección ortotipográfica, de estilo y de contenido.\n",
      "- Traducción automática: conversión de texto de un idioma a otro.\n",
      "- Resumen de textos: reducción de documentos extensos a resúmenes.\n",
      "- Planificación y guion de contenidos: estructuración de contenidos, p. ej., índices.\n",
      "- Brainstorming: propuestas creativas para proyectos, nombres, conceptos, etc.\n",
      "- Programación: creación de código de programación a partir de lenguaje natural.\n",
      "\n",
      "# 2. Análisis y organización de información\n",
      "\n",
      "- Análisis de sentimientos: evaluación de emociones y opiniones en textos.\n",
      "- Extracción de información: extracción de datos específicos de documentos extensos.\n",
      "- Clasificación de textos: organización de textos en categorías o temas específicos.\n",
      "- Revisión técnica: asistencia en revisar documentos especializados (p. ej., legales).\n",
      "\n",
      "# 3. Interacción y automatización\n",
      "\n",
      "- Chatbots: simulación de conversaciones sobre temas generales o específicos.\n",
      "- Q&A: generación de respuestas a preguntas basadas en un corpus.\n",
      "\n",
      "Estos usos resumen las aplicaciones actuales de los LLM de texto. Con la emergencia de los LLM multimodales, comienzan a aflorar aplicaciones adicionales como la generación de contenido audiovisual, la interpretación de datos a partir de imágenes, la traducción de contenido multimedia o la creación de experiencias interactivas enriquecidas, como la interacción con chatbots con entradas no solo de texto, sino también de imagen, audio y vídeo.\n",
      "\n",
      "# Requisitos regulatorios\n",
      "\n",
      "La rápida evolución de la inteligencia artificial generativa, especialmente en el campo de los modelos de lenguaje de gran escala (LLM), ha captado la atención de reguladores a nivel global. El potencial de estos sistemas para influir de forma negativa en los ciudadanos ha llevado a un incremento en las iniciativas para establecer marcos regulatorios que aseguren su desarrollo y uso responsable.\n",
      "\n",
      "Algunas de las principales iniciativas regulatorias sobre IA son:\n",
      "\n",
      "- El AI Act de la Unión Europea: propuesta legislativa pionera para regular la IA, que clasifica los sistemas de IA según su nivel de riesgo y establece requisitos de transparencia, seguridad y derechos fundamentales. El AI Act fue aprobado por el Parlamento Europeo el 13 de marzo de 2024.\n",
      "- El AI Bill of Rights de Estados Unidos: documento orientativo que busca proteger los derechos civiles en el desarrollo y aplicación de la IA, enfatizando la privacidad, la no discriminación y la transparencia.\n",
      "- La guía sobre IA del NIST35 de Estados Unidos: establece principios para la creación de sistemas de IA fiables, con enfoque en la precisión, la explicabilidad y la mitigación de sesgos.\n",
      "\n",
      "35El Instituto Nacional de Estándares y Tecnología (NIST) ha publicado documentos que detallan marcos de ciberseguridad, de gestión de riesgos y, concretamente, de gestión de modelos de IA y de IA generativa.\n",
      "# La Declaración de Bletchley: compromiso internacional para el desarrollo responsable de la IA, promoviendo principios de transparencia, seguridad y equidad, firmado por múltiples países.\n",
      "\n",
      "Además de las iniciativas mencionadas, numerosos países han comenzado a emitir sus propias regulaciones locales o han establecido principios para la adopción de la IA de manera ética y segura. Entre ellos se cuentan Reino Unido, Francia, España, Alemania, Países Bajos, Polonia, Australia, Nueva Zelanda, Singapur, Canadá, Japón, Corea del Sur, China, India, Indonesia, Israel, Emiratos Árabes Unidos, Arabia Saudí, Egipto, Brasil, Chile, Perú, Argentina, México, Colombia y Turquía, entre otros.\n",
      "\n",
      "# 4 Seguridad y fiabilidad: exigencias de robustez operacional para prevenir disfunciones o manipulaciones que puedan causar daño o pérdida de información.\n",
      "\n",
      "# 4 Responsabilidad y gobernanza: marco de responsabilidad de desarrolladores y usuarios de LLM en caso de daños o violaciones de derechos, incluyendo mecanismos de supervisión y control.\n",
      "\n",
      "# 4 Supervisión humana: la necesidad de mantener una supervisión humana efectiva sobre los LLM, asegurando que las decisiones importantes puedan ser revisadas y, si es necesario, corregidas o revertidas por humanos.\n",
      "\n",
      "Todas estas iniciativas regulatorias plantean requisitos muy similares sobre la IA que, aplicados a los LLM, se pueden resumir en:\n",
      "\n",
      "# 4 Transparencia y explicabilidad: obligación de revelar cómo funciona el LLM, incluyendo la lógica detrás de sus salidas para que sean comprensibles para los usuarios.\n",
      "\n",
      "Estos requisitos reflejan un consenso emergente sobre los principios fundamentales para el desarrollo ético y seguro de los LLM, y forman la base para futuras regulaciones específicas y adaptaciones según evolucione la tecnología.\n",
      "\n",
      "# 4 Privacidad y protección de datos: medidas estrictas para proteger la información personal recopilada o generada por LLM, cumpliendo con leyes de protección de datos, como GDPR en Europa.\n",
      "\n",
      "# 4 Equidad y no discriminación: requisitos para prevenir sesgos y asegurar que los LLM no perpetúen discriminaciones ni prejuicios, mediante la evaluación y corrección constantes de sus algoritmos.\n",
      "\n",
      "37 IAPP (2024).\n",
      "# Desarrollo y despliegue de los LLM\n",
      "\n",
      "“La IA generativa es la clave para resolver algunos de los mayores problemas del mundo, como el cambio climático, la pobreza y las enfermedades. Tiene el potencial de hacer del mundo un lugar mejor para todos“.\n",
      "\n",
      "Mark Zuckerberg37\n",
      "\n",
      "# MANAGEMENT SOLUTIONS\n",
      "\n",
      "# El auge de los large language models: de los fundamentos a la aplicación\n",
      "# Aspectos clave en el desarrollo de los LLM\n",
      "\n",
      "En esta sección se analizan los aspectos clave del proceso de generar texto coherente y contextualizado. Por ejemplo, desarrollo y despliegue de LLM. Se examinan los componentes principales, como los datos y la arquitectura del modelo, así como las etapas de preentrenamiento, fine-tuning e implementación. Además, se discuten los principales retos y consideraciones que deben tenerse en cuenta para garantizar un desarrollo ético, robusto y alineado con los objetivos de una organización.\n",
      "\n",
      "# Datos\n",
      "\n",
      "Los datos son el fundamento sobre el cual se construyen los LLM, y su calidad, diversidad y representatividad tienen un impacto directo en el rendimiento y los sesgos del modelo resultante. Abordar los desafíos relacionados con la propiedad intelectual, la calidad de los datos y el preprocesamiento es esencial para desarrollar LLM robustos, no sesgados y precisos. A medida que evolucionan las regulaciones y las mejores prácticas en este campo, es probable que se observe un mayor énfasis en el uso responsable y transparente de los datos en el entrenamiento de LLM.\n",
      "\n",
      "# Algunos aspectos clave sobre los datos de entrenamiento de un LLM son:\n",
      "\n",
      "- Corpus de entrenamiento: los LLM se entrenan con grandes corpus de datos, a menudo extraídos de internet, que incluyen billones de palabras y abarcan una amplia gama de dominios y géneros, como libros, artículos de noticias, páginas web, redes sociales y más. Estos corpus masivos permiten a los LLM aprender patrones y representaciones del lenguaje a gran escala, lo que les otorga una capacidad sin precedentes para comprender y generar texto coherente.\n",
      "\n",
      "# Propiedad intelectual y derechos de autor\n",
      "\n",
      "La extracción y uso de datos de internet para entrenar LLM plantea desafíos relacionados con la propiedad intelectual y los derechos de autor. Muchos de estos datos están protegidos por derechos de autor, y su uso sin permiso o compensación adecuada puede ser problemático. El AI Act en Europa aborda este tema imponiendo nuevos requisitos a los desarrolladores de LLM, como la obligación de revelar las fuentes de datos utilizadas y obtener las licencias necesarias.\n",
      "\n",
      "# Calidad y representatividad de los datos\n",
      "\n",
      "Como cualquier modelo, un LLM será tan bueno como los datos utilizados en su entrenamiento. Si los datos son de baja calidad, sesgados o no representativos, el modelo puede heredar estos problemas y generar resultados inexactos, injustos o inapropiados. Por lo tanto, es crucial asegurar que los corpus de entrenamiento sean diversos, equilibrados y representen adecuadamente los diferentes grupos demográficos, opiniones y perspectivas.\n",
      "\n",
      "# Iniciativas de datos de alta calidad\n",
      "\n",
      "Algunas iniciativas recientes se centran en construir LLM con menos parámetros, pero datos de mayor calidad, como corpus de entrenamiento más pequeños, pero cuidadosamente seleccionados y filtrados, que incluyen contenido de alta calidad como libros, artículos científicos y publicaciones.\n",
      "\n",
      "# Referencias\n",
      "\n",
      "1. Mark Zuckerberg (n. 1984), cofundador y CEO de Facebook y de Meta, una de las mayores compañías de redes sociales, tecnología e inteligencia artificial del mundo.\n",
      "2. Liu (2024).\n",
      "3. Soskek (2019).\n",
      "4. Project Gutenberg (2024).\n",
      "5. Wikipedia Dumps (2024).\n",
      "6. Hugging Face Datasets (2024).\n",
      "7. Li (2024), Chu (2023).\n",
      "8. Alabdulmohsin (2024).\n",
      "9. Yogarajan (2023).\n",
      "10. Sachdeva (2024).\n",
      "11. Tirumala (2023).\n",
      "# 4 Preprocesamiento y etiquetado de datos\n",
      "\n",
      "Antes de entrenar o fine-tunear un LLM, los datos deben ser preprocesados y, en algunos casos, como el fine-tuning supervisado o el uso de un conjunto de datos específico, etiquetados. El preprocesamiento implica limpiar y formatear los datos, eliminar el ruido y los errores, y aplicar técnicas como la tokenización y la normalización (p. ej., LayerNorm para Transformers).\n",
      "\n",
      "# 24 Tokenización y encoding\n",
      "\n",
      "El auge de los large language models: de los fundamentos a la aplicación. La tokenización se refiere al proceso de dividir un texto en unidades más pequeñas llamadas \"tokens\", que son las unidades procesadas por el LLM durante el entrenamiento y la inferencia de respuestas. Estos tokens pueden ser palabras, partes de una palabra (p. ej., lemas) o caracteres. Por ejemplo, una de las formas más sencillas de generar tokens es separar el corpus según los espacios entre palabras. El encoding es el proceso de representar esas unidades de texto en forma numérica para que pueda ser procesado por el modelo.\n",
      "\n",
      "Algunos puntos clave sobre la tokenización en los LLM:\n",
      "\n",
      "- Se realiza partiendo del corpus de textos disponible, con objeto de dividir el texto de partida en unidades más pequeñas de forma óptima. El resultado final de la tokenización es un encoding.\n",
      "- Los principales encodings usados son BytePairEncoding, SentencePieceEncoding y WordPieceEncoding.\n",
      "\n",
      "El resultado de la tokenización se emplea como punto de partida en el modelo de embedding.\n",
      "\n",
      "# Embedding\n",
      "\n",
      "Los embeddings son representaciones numéricas de palabras, frases, oraciones o incluso párrafos que capturan su significado semántico y las relaciones entre ellos. Para ello, parten del corpus de entrada del LLM dividido en tokens. Son un componente fundamental de los LLM y desempeñan un papel crucial tanto en el preentrenamiento como en el fine-tuning y el uso posterior de estos modelos.\n",
      "\n",
      "Los embeddings en los LLM:\n",
      "\n",
      "- Están diseñados para capturar las relaciones semánticas entre las palabras, de manera que palabras con significados similares tengan vectores similares. Esto permite al modelo comprender la similitud y las analogías entre palabras y conceptos.\n",
      "\n",
      "Los encodings impactan de forma significativa en el rendimiento del LLM, ya que definen la mínima unidad de procesamiento que van a recibir y determinan el vocabulario al que tiene acceso el LLM.\n",
      "\n",
      "Referencias:\n",
      "\n",
      "- 48 Chen (2023).\n",
      "- 49 Wenzek (2019), Penedo (2023).\n",
      "- 50 Zhao (2023).\n",
      "- 51 Rejeleene (2024).\n",
      "- 52 Minaee (2024).\n",
      "- 53 Kudo (2018).\n",
      "# Tipologías de embeddings\n",
      "\n",
      "No son valores universales, sino que varían entre distintos modelos en función del espacio vectorial en el que se hayan definido. Los embeddings se utilizan en los LLM para poder establecer una métrica que defina la similitud entre los significados de las palabras y para incorporar información sobre la posición de las palabras en una oración. Esto es crucial, ya que el orden de las palabras afecta al significado. Existen tres tipos principales de embeddings posicionales:\n",
      "\n",
      "- Embedding posicional absoluto: asigna a cada palabra –o a cada unidad mínima de texto o token– un vector que representa su posición exacta en la oración (p. ej., primera, segunda, tercera posición, etc.).\n",
      "- Embedding posicional relativo: en lugar de basarse en posiciones absolutas, representa la posición de una palabra en relación con las demás (p. ej., dos palabras antes, una palabra después, etc.).\n",
      "- Embedding posicional rotatorio: combina la información de posiciones absolutas y relativas, utilizando funciones trigonométricas para crear representaciones vectoriales más complejas.\n",
      "\n",
      "# Preentrenamiento\n",
      "\n",
      "El preentrenamiento es una etapa fundamental en el desarrollo de LLM, durante la cual los modelos adquieren un conocimiento general y profundo del lenguaje a partir de grandes cantidades de datos no etiquetados. Aunque este proceso es computacionalmente intensivo y costoso, permite la adaptación del modelo a una amplia gama de tareas.\n",
      "\n",
      "El objetivo principal del preentrenamiento es que el modelo adquiera un conocimiento amplio y profundo del lenguaje, incluyendo su estructura, semántica, sintaxis y contexto. Durante este proceso, el LLM aprende a predecir palabras o fragmentos de texto (i.e., tokens) basándose en el contexto circundante, lo que le permite capturar relaciones y patrones lingüísticos complejos. Este conocimiento general se convierte en la base sobre la cual el modelo puede ser adaptado posteriormente para tareas específicas mediante el fine-tuning.\n",
      "\n",
      "La elección del tipo de embedding posicional puede afectar al rendimiento del LLM, ya que determina la cantidad y el tipo de información posicional disponible para el modelo durante el entrenamiento.\n",
      "\n",
      "# Técnicas populares para el preentrenamiento de LLM\n",
      "\n",
      "Existen varias técnicas populares para el preentrenamiento de LLM, como:\n",
      "\n",
      "- El modelado autorregresivo de lenguaje o modelado unidireccional (p. ej., modelado autorregresivo), que consiste en entrenar el modelo para predecir la siguiente palabra o fragmento de texto dado el contexto anterior. Esta tarea permite al modelo aprender las probabilidades condicionales del lenguaje y generar texto coherente. Son ejemplos los modelos GPT y Claude.\n",
      "\n",
      "54Devlin (2018), Liu (2022).\n",
      "\n",
      "1Vaswani (2017).\n",
      "\n",
      "2Shaw (2018).\n",
      "\n",
      "3Su (2021).\n",
      "# El modelo no autorregresivo\n",
      "\n",
      "El modelo no autorregresivo, usado en modelos como Gemini, en los que no se obtiene la respuesta secuencialmente palabra a palabra, sino que se transforma y refina en conjunto.\n",
      "\n",
      "# Cuantización\n",
      "\n",
      "Las neuronas para realizar predicciones más precisas. Estos pesos se almacenan típicamente como números de alta precisión, lo que puede resultar en modelos de gran tamaño y computacionalmente costosos.\n",
      "\n",
      "# El modelado de lenguaje enmascarado\n",
      "\n",
      "El modelado de lenguaje enmascarado, popularizado por modelos como BERT, que consiste en enmascarar aleatoriamente algunas palabras en el texto de entrada y entrenar el modelo para predecir estas palabras enmascaradas basándose en el contexto circundante. Esta técnica permite un aprendizaje bidireccional y una mejor comprensión del contexto. Algunas arquitecturas de LLM (p. ej., transformers bidireccionales) usan esta técnica.\n",
      "\n",
      "# La cuantización postentrenamiento\n",
      "\n",
      "La cuantización postentrenamiento es una técnica que permite reducir la precisión de los parámetros del modelo sin afectar significativamente el rendimiento del modelo. Por ejemplo, redes neuronales que almacenen sus parámetros usando números de coma flotante de 32 bits pueden pasar a usar únicamente 16 bits u 8 bits dependiendo del tipo de cuantización. Esto resulta en modelos más pequeños y rápidos, ya que requieren menos memoria y pueden realizar operaciones más eficientemente con el hardware adecuado.\n",
      "\n",
      "# El modelado secuencia a secuencia\n",
      "\n",
      "El modelado secuencia a secuencia (p. ej., seq2seq), en el que el modelo se entrena para generar secuencias de texto en función de otras secuencias de entrada. Es usado en modelos como T5, BART o ProphetNET.\n",
      "\n",
      "# El preentrenamiento contrastivo\n",
      "\n",
      "El preentrenamiento contrastivo, utilizado en modelos como CLIP y ALIGN, que implica entrenar el modelo para identificar pares de texto e imagen que están semánticamente relacionados, lo que le permite aprender representaciones multimodales y transferir conocimiento entre diferentes modalidades.\n",
      "\n",
      "# El preentrenamiento de LLM\n",
      "\n",
      "El preentrenamiento de LLM es un proceso computacionalmente intensivo que requiere enormes cantidades de datos, tiempo y recursos de hardware. Los modelos más grandes pueden tener en torno a 1 billón (1012) de parámetros y requerir miles de GPU de gama alta durante semanas o meses de entrenamiento. Esto hace que el preentrenamiento sea extremadamente costoso y solo esté al alcance de unas pocas empresas y organizaciones en el mundo con los recursos necesarios.\n",
      "\n",
      "# Referencias\n",
      "\n",
      "55 Xu (2021).\n",
      "\n",
      "56 Devlin (2019), Sinha (2021).\n",
      "\n",
      "57 Lee (2022).\n",
      "\n",
      "58 Sutskever (2014).\n",
      "\n",
      "59 Zeng (2023).\n",
      "\n",
      "60 Jia (2021).\n",
      "\n",
      "61 Cui (2022).\n",
      "\n",
      "62 Li (2024).\n",
      "\n",
      "63 Tian (2024).\n",
      "\n",
      "64 Fu (2024).\n",
      "\n",
      "# Durante el entrenamiento de los LLM\n",
      "\n",
      "Durante el entrenamiento de los LLM, se ajustan los pesos de\n",
      "\n",
      "# Fig. 6. Fine-tuning de un LLM.\n",
      "\n",
      "|Interacción con el usuario|Preentrenamiento|Puesta a punto|\n",
      "|---|---|---|\n",
      "|Datos específicos del ámbito|Conjunto de datos más pequeño con ámbito específico|Respuesta del LLM|\n",
      "|Pregunta del usuario|LLM perfeccionado (por ejemplo, CodeLlama)| |\n",
      "|Base de datos de preentrenamiento (por ejemplo, Wikipedia)| | |\n",
      "# Fine-tuning, instruction-tuning y RAG\n",
      "\n",
      "El fine-tuning es el proceso de adaptar un LLM preentrenado a una tarea específica utilizando un conjunto de datos más pequeño. Esta técnica permite aprovechar el conocimiento general adquirido durante el preentrenamiento y especializarlo para obtener un alto rendimiento en la tarea objetivo.\n",
      "\n",
      "El objetivo principal del fine-tuning (Fig. 6) es adaptar un LLM preentrenado a una tarea concreta, como la clasificación de sentimientos, la respuesta a preguntas, la traducción automática o la generación de resúmenes. Durante este proceso, el modelo aprende a utilizar su conocimiento general del lenguaje y aplicarlo de manera efectiva al dominio y los requisitos específicos de la tarea en cuestión. Los LLM disponibles en el mercado, sean propietarios o de código abierto, típicamente están preentrenados (y por tanto son de propósito general), pero no han recibido fine-tuning, que los adaptaría a un propósito específico.\n",
      "\n",
      "# Beneficios del fine-tuning\n",
      "\n",
      "- Aprovecha el conocimiento previo: al partir de un modelo preentrenado, el fine-tuning permite aprovechar el vasto conocimiento general del lenguaje adquirido durante el preentrenamiento, lo que acelera el aprendizaje y mejora el rendimiento en la tarea específica.\n",
      "- Requiere menos datos y recursos: en comparación con el entrenamiento desde cero, el fine-tuning necesita mucha menos cantidad de datos etiquetados y recursos computacionales, lo que lo hace más accesible y económico para una amplia gama de organizaciones y aplicaciones.\n",
      "\n",
      "# Entrenando LLM: funciones de pérdida\n",
      "\n",
      "Los LLM, como otros modelos de aprendizaje profundo, aprenden ajustando sus parámetros para minimizar una función de pérdida. Esta función mide la diferencia entre las predicciones del modelo y los resultados esperados, guiando al modelo hacia un mejor rendimiento.\n",
      "\n",
      "La elección de la función de pérdida depende del tipo de tarea para la que se esté entrenando el LLM. Por ejemplo, para un modelo que predice la siguiente palabra en una frase (modelado autorregresivo del lenguaje), una función común es la entropía cruzada. Esta función compara la distribución de probabilidad de las palabras predichas por el modelo con la distribución real observada en los datos de entrenamiento.\n",
      "\n",
      "Matemáticamente, la función de pérdida de entropía cruzada para un modelo autorregresivo se puede expresar como una suma de los logaritmos negativos de las probabilidades asignadas a las palabras correctas en cada posición de la secuencia.\n",
      "\n",
      "En concreto, dada una función de pérdida como la entropía cruzada, y una tipología de entrenamiento como el modelado autorregresivo del lenguaje, se puede definir la función de pérdida a minimizar como:\n",
      "\n",
      "donde φ representa los parámetros del modelo, i se refiere al número de token en una secuencia determinada con N tokens, P es la probabilidad de predecir el token i en función de la secuencia x de tokens anteriores.\n",
      "\n",
      "Durante el ajuste fino (fine-tuning) de los embeddings del modelo, se pueden utilizar funciones de pérdida especializadas para afinar las representaciones vectoriales de las palabras. Algunas opciones populares incluyen:\n",
      "\n",
      "- Pérdida por similitud del coseno: ajusta los embeddings para que palabras similares tengan vectores más parecidos.\n",
      "- Pérdida por error cuadrático medio: minimiza la diferencia cuadrática entre los embeddings predichos y los esperados.\n",
      "- Pérdida por ranking de múltiples negativos: asocia los embeddings de palabras relacionadas de manera que estén más cerca entre sí que los de palabras no relacionadas.\n",
      "- Pérdida por tripletes, de Matryoshka o contrastiva: variantes más avanzadas que consideran relaciones entre tríos o grupos de embeddings.\n",
      "\n",
      "A pesar de sus beneficios, el fine-tuning también presenta algunos desafíos:\n",
      "\n",
      "- Sobre-especialización: si el modelo se somete a un fine-tuning en un conjunto de datos demasiado específico, puede perder parte de su capacidad de generalización y funcionar mal con datos desconocidos o ligeramente diferentes.\n",
      "\n",
      "65Wang (2024).\n",
      "# Fig. 7. Funcionamiento del RAG.\n",
      "\n",
      "|Preentrenamiento|RAG|Interaccion con el usuario|\n",
      "|---|---|---|\n",
      "|Consulta de busqueda|Documentacion especifica del ambito|Recuperacion de datos|\n",
      "|Base de datos de preentrenamiento (por ejemplo; Wikipedia)|LLM de base (por ejemplo; Llama 2)|Respuesta del LLM mejorada por recuperacion|\n",
      "\n",
      "# 4\n",
      "\n",
      "Olvido catastrófico66: durante el fine-tuning es posible que un modelo olvide conocimientos críticos aprendidos previamente.\n",
      "\n",
      "# 4\n",
      "\n",
      "Inestabilidad67: el proceso de fine-tuning puede ser sensible a factores como la inicialización de pesos, los hiperparámetros y la selección de datos, lo que puede llevar a resultados inconsistentes o variaciones en el rendimiento.\n",
      "\n",
      "# 4\n",
      "\n",
      "Herencia de sesgos68: los modelos que han recibido fine-tuning pueden heredar y amplificar los sesgos presentes tanto en los datos de preentrenamiento como en los datos de fine-tuning, lo que requiere una cuidadosa consideración y mitigación.\n",
      "\n",
      "# 28\n",
      "\n",
      "Existen varios tipos de fine-tuning que deben seleccionarse en función de cuánto sea necesario modificar el modelo de partida para adecuarlo a una tarea en un dominio más específico. Los principales métodos son:\n",
      "\n",
      "# 4\n",
      "\n",
      "Fine-tuning supervisado69: requiere conjuntos de datos etiquetados de entrada y respuesta del LLM, que se usan para mejorar su respuesta ante tareas específicas. Un método popular de fine-tuning supervisado es el llamado instruction-tuning70, que consiste en alinear las respuestas del modelo a lo esperado por sus usuarios a través de interacciones con el modelo.\n",
      "\n",
      "# 4\n",
      "\n",
      "Reinforcement learning: métodos basados en aprendizaje por refuerzo que se centran en mejorar la calidad de la respuesta del LLM, en este caso con base en el feedback de usuario o modelos de recompensa (p. ej., optimización directa por preferencia71).\n",
      "\n",
      "# 4\n",
      "\n",
      "Fine-tuning no supervisado72: se trata de un método que no requiere conjuntos de datos etiquetados, sino que se basa en reentrenar el modelo con las mismas metodologías usadas durante el preentrenamiento (p. ej., predecir el siguiente token).\n",
      "\n",
      "66 Luo (2024).\n",
      "\n",
      "67 Zhang (2024).\n",
      "\n",
      "68 Zhang (2024).\n",
      "\n",
      "69 Ovadia (2024).\n",
      "\n",
      "70 Zhang (2023).\n",
      "\n",
      "71 Rafailov (2023).\n",
      "\n",
      "72 Zhou (2023).\n",
      "\n",
      "73 Xu (2023).\n",
      "\n",
      "74 Dettmers (2023).\n",
      "\n",
      "75 Chen (2023).\n",
      "\n",
      "76 Lewis (2020) y Neelakantan (2022).\n",
      "# Implementación y uso\n",
      "\n",
      "# Monitoreo y mantenimiento\n",
      "\n",
      "Una vez entrenado y validado, el LLM debe desplegarse en un entorno de producción para su uso en aplicaciones reales. Esto conlleva la integración del modelo en sistemas y flujos de trabajo existentes, así como la creación de interfaces y APIs para interactuar con él.\n",
      "\n",
      "Este proceso implica varios aspectos clave, entre los que se incluyen aspectos de integración y de monitoreo.\n",
      "\n",
      "# Integración en sistemas y flujos de trabajo\n",
      "\n",
      "Infraestructura: los LLM suelen ser modelos grandes y computacionalmente intensivos, por lo que requieren una infraestructura robusta para su implementación. Esto puede implicar el uso de hardware especializado, como GPU o TPU, y plataformas de computación en la nube optimizadas para realizar eficientemente el proceso de inferencia.\n",
      "\n",
      "Interfaces y API: para facilitar el uso del LLM en aplicaciones y servicios, es necesario desarrollar interfaces y API que permitan a otros sistemas interactuar con el modelo de manera eficiente y segura. Esto puede incluir endpoints, bibliotecas de cliente en varios lenguajes de programación e interfaces gráficas de usuario para usuarios no técnicos.\n",
      "\n",
      "Integración con otros componentes: en muchos casos, los LLM forman parte de un sistema más amplio que incluye otros componentes, como bases de datos, servicios de procesamiento de lenguaje natural y aplicaciones de usuario final. La integración fluida y eficiente del LLM con estos componentes es crucial para garantizar un rendimiento y una experiencia de usuario óptimos.\n",
      "\n",
      "Como se puede apreciar, el desarrollo y despliegue de LLM es un proceso complejo y multifacético que requiere una cuidadosa consideración de múltiples aspectos, desde la selección y preparación de los datos hasta la implementación y el uso responsable del modelo. Una comprensión profunda de los componentes clave, como el preentrenamiento, el fine-tuning y los embeddings, así como la conciencia de los desafíos y riesgos asociados, es esencial para aprovechar todo el potencial de los LLM de manera ética, sostenible y rentable, así como alineada con los objetivos de cada organización.\n",
      "\n",
      "77Wan (2024).\n",
      "\n",
      "78Abhyankar (2024).\n",
      "\n",
      "79Goyal (2024).\n",
      "\n",
      "80Lester (2021).\n",
      "\n",
      "81Banerjee (2023).\n",
      "# Arquitectura de los LLM\n",
      "\n",
      "La arquitectura de los LLM se refiere a la estructura y organización de las redes neuronales que componen estos modelos. La elección de la arquitectura y sus componentes tiene un impacto significativo en el rendimiento, la eficiencia y las capacidades del LLM. Esta sección explorará las principales arquitecturas utilizadas en los LLM y sus características, ventajas y limitaciones.\n",
      "\n",
      "# Transformers: el estado del arte en LLM\n",
      "\n",
      "Los transformers también introducen el concepto de atención multi-cabezal (multi-head attention), donde múltiples mecanismos de atención operan en paralelo, lo que permite al modelo capturar diferentes tipos de relaciones y patrones en los datos.\n",
      "\n",
      "La arquitectura del transformer ha demostrado un rendimiento excepcional en una amplia gama de tareas de procesamiento del lenguaje natural, y ha sido adoptada por la mayoría de los LLM considerados de referencia.\n",
      "\n",
      "# Variantes y extensiones de los transformers\n",
      "\n",
      "Los transformers, introducidos en 2017, se han convertido en la arquitectura dominante para los LLM. A diferencia de las arquitecturas anteriores basadas en redes neuronales recurrentes (RNN) o redes neuronales convolucionales (CNN), los transformers se basan únicamente en mecanismos de atención para procesar y generar secuencias de texto.\n",
      "\n",
      "La arquitectura del transformer consta de dos componentes principales: el codificador (encoder) y el decodificador (decoder), y existen transformers con solo codificador, solo decodificador o con ambos componentes. El codificador procesa la secuencia de entrada y genera una representación contextual para cada token, mientras que el decodificador genera la secuencia de salida a partir de la representación del codificador y las predicciones anteriores.\n",
      "\n",
      "Una variante popular es el transformer bidireccional, que permite al modelo atender tanto al contexto izquierdo como al derecho de cada token. Esto se logra mediante el uso de un objetivo de preentrenamiento de modelado de lenguaje enmascarado (MLM), donde algunos tokens se enmascaran aleatoriamente y el modelo debe predecirlos basándose en el contexto circundante.\n",
      "\n",
      "Otra variante es el transformer generativo, como GPT, que utiliza un enfoque de modelado de lenguaje unidireccional. Esto permite generar texto de manera eficiente y coherente, ya que el modelo solo puede atender al contexto izquierdo de cada token.\n",
      "\n",
      "También se han propuesto extensiones para hacer que los Transformers sean más eficientes y escalables, como el transformer disperso (sparse transformer), que utiliza.\n",
      "\n",
      "# Fig. 8. Funcionamiento de un transformer.\n",
      "\n",
      "|Tokenizador|Incrustaciones|Codificaciones posicionales|\n",
      "|---|---|---|\n",
      "|Consulta|Key|Value|\n",
      "|Consulta de entrada al modelo|Las entradas se dividen en unidades (fichas).|El modelo de incrustación procesa y asigna representaciones numéricas a cada token.|\n",
      "|Se añade información posicional para que el modelo comprenda las relaciones entre las palabras y el contexto.|El codificador utiliza la autoatención para averiguar qué palabras son importantes y cómo se relacionan entre sí, procesando en paralelo los datos en N capas.|El decodificador tiene en cuenta el contexto previo y las partes traducidas utilizando la atención.|\n",
      "|El decodificador toma la información traducida del codificador y genera una nueva respuesta.|La respuesta se invierte y se presenta de forma coherente.| |\n",
      "\n",
      "Vaswani (2017).\n",
      "# Prompt engineering en los LLM: principios y mejores prácticas\n",
      "\n",
      "Prompt engineering se refiere al proceso de diseñar y optimizar los prompts (entradas de texto) para obtener los mejores resultados posibles de los LLM. Esta disciplina emergente contiene una serie de principios y mejores prácticas que permiten aprovechar al máximo las capacidades de estos modelos. Entre ellos:\n",
      "\n",
      "- 4 Aprovechar conocimiento externo: proporcionar información adicional permite complementar la base de conocimientos del LLM. Por ejemplo, para responder preguntas sobre una industria específica, se podrían primero recuperar informes sectoriales relevantes y pasarlos al modelo.\n",
      "- 4 Ser claro y específico: las instrucciones dadas al modelo deben indicar de forma explícita el formato, la longitud y el nivel de detalle esperado en la respuesta. Por ejemplo, en lugar de simplemente pedir \"Analiza la situación financiera de la empresa X\", es mejor dar una instrucción como \"Escribe un informe de 1000 palabras sobre la situación financiera de la empresa X, cubriendo su rentabilidad, liquidez, solvencia y perspectivas futuras\".\n",
      "- 4 Iterar y refinar sistemáticamente: la evaluación continua del rendimiento del modelo permite identificar áreas de mejora y ajustar los prompts en consecuencia. Métricas cuantitativas y juicios cualitativos de expertos del dominio pueden guiar este proceso iterativo.\n",
      "- 4 Descomponer tareas complejas: conviene dividir los problemas en subtareas más manejables para los LLM. Por ejemplo, en lugar de pedir \"Desarrolla un plan estratégico para la empresa Y\", se pueden solicitar subtareas como \"Realiza un análisis DAFO de la empresa Y\", \"Define los objetivos estratégicos clave para Y\", \"Propón iniciativas para alcanzar cada objetivo\", etc.\n",
      "- 4 Proporcionar ejemplos ilustrativos (few-shot learning): unos pocos ejemplos bien elegidos pueden ayudar enormemente a comunicar la tarea deseada. Supongamos que se quiere generar propuestas de valor para productos; se podrían dar dos ejemplos: \"Nuestro software CRM permite a los equipos de ventas cerrar acuerdos un 50% más rápido\" y \"Nuestra app de bienestar ayuda a los empleados a reducir el estrés y aumentar su productividad en un 25%\".\n",
      "- 4 Solicitar razonamiento paso a paso: instruir al LLM para que verbalice su proceso de pensamiento a menudo conduce a resultados más robustos. Esto es especialmente útil para tareas de análisis o solución de problemas en el ámbito empresarial. Por ejemplo: \"Describe paso a paso cómo calcularías el ROI de este proyecto de inversión\".\n",
      "- 4 Solicitar las referencias empleadas: instruir al LLM para que indique en su proceso de razonamiento las referencias a documentos que ha empleado, incluyendo citas del texto original al que tiene acceso.\n",
      "- 4 Pedir al LLM que adopte un personaje: antes de la tarea principal, se puede primero instruir al modelo para que adopte un determinado rol, tono o estilo. Por ejemplo: \"Actúa como un experto analista financiero y proporciona una valoración objetiva de la empresa X\". Esto ayuda a orientar su comportamiento.\n",
      "\n",
      "Mediante la aplicación de estos principios de prompt engineering, se demuestra estadísticamente que los LLM proporcionan un resultado más preciso y fiable.\n",
      "\n",
      "Fuentes: Guía de prompt engineering de OpenAI, soporte de Anthropic Claude Opus y elaboración propia.\n",
      "# El auge de los large language models: de los fundamentos a la aplicación\n",
      "\n",
      "# atención dispersa para reducir la complejidad\n",
      "\n",
      "escalabilidad, la interpretabilidad y la eficiencia de estos modelos. A medida que la investigación avance, es probable que aparezcan nuevas arquitecturas y técnicas que superen estas limitaciones y lleven a los LLM a nuevas cotas de rendimiento y capacidad.\n",
      "\n",
      "# Comparación con arquitecturas anteriores\n",
      "\n",
      "Antes de los transformers, las arquitecturas dominantes para el modelado de secuencias eran las redes neuronales recurrentes (RNN), como Long Short-Term Memory (LSTM) y Gated Recurrent Unit (GRU), y las redes neuronales convolucionales (CNN).\n",
      "\n",
      "# 4\n",
      "\n",
      "Las RNN pueden capturar dependencias a largo plazo en las secuencias, pero sufren de problemas como el desvanecimiento del gradiente (vanishing gradient) y la dificultad para paralelizar el entrenamiento. Además, las RNN tienen dificultades para capturar dependencias muy largas debido a su naturaleza secuencial y al uso de recurrencias de alcance constante.\n",
      "\n",
      "# 4\n",
      "\n",
      "Las CNN pueden capturar patrones locales en las secuencias y son eficientes en términos de computación, pero tienen dificultades para modelar dependencias a largo plazo y requieren un tamaño de contexto fijo.\n",
      "\n",
      "En comparación, los transformers superan estas limitaciones al utilizar mecanismos de atención que pueden capturar dependencias a largo plazo de manera eficiente y paralela. Además, los transformers son más flexibles en términos de manejo de secuencias de longitud variable y pueden ser preentrenados en grandes cantidades de datos no etiquetados.\n",
      "\n",
      "La arquitectura de los transformers ha revolucionado el campo de los LLM y ha permitido avances significativos en una amplia gama de tareas de procesamiento del lenguaje natural. Sin embargo, aún quedan desafíos por abordar, como la:\n",
      "\n",
      "|4|Gestión de grandes volúmenes de datos:|los LLM requieren cantidades masivas de datos de entrenamiento, lo que implica la necesidad de infraestructuras de almacenamiento y procesamiento escalables y eficientes.|\n",
      "|---|---|---|\n",
      "|4|Escalado de recursos computacionales:|el entrenamiento y la inferencia de LLM demandan enormes recursos de cómputo, haciendo necesario el uso de técnicas de|\n",
      "# Sesgos, alucinaciones y confiabilidad\n",
      "\n",
      "Uno de los mayores retos de los LLM es la presencia de sesgos y alucinaciones en sus resultados y predicciones. Los sesgos pueden surgir de varias fuentes, como la parcialidad en los datos de entrenamiento, las limitaciones de las arquitecturas de los modelos o los prejuicios humanos implícitos en las tareas de anotación y evaluación. Por otro lado, las alucinaciones se refieren a la generación de información o contenido que parece plausible pero que no se basa en hechos reales o en el conocimiento adquirido durante el entrenamiento.\n",
      "\n",
      "Los sesgos en los LLM pueden manifestarse de diversas maneras, como la perpetuación de estereotipos de género, raza o edad, la discriminación en tareas de clasificación o la generación de contenido ofensivo o inapropiado. Estos sesgos pueden tener consecuencias graves, especialmente cuando los LLM se utilizan en aplicaciones sensibles como la toma de decisiones en el ámbito legal, financiero o médico. Por su parte, las alucinaciones pueden llevar a la difusión de información errónea o engañosa, lo que puede tener un impacto negativo en la confianza de los usuarios y en la credibilidad de las aplicaciones basadas en LLM.\n",
      "\n",
      "Además, LLMOps promueve prácticas como la automatización de procesos, el testing continuo, la documentación exhaustiva y la gobernanza de modelos. Esto permite no solo mejorar la eficiencia y calidad del desarrollo de los LLM, sino también garantizar su uso ético y responsable.\n",
      "\n",
      "# Retos\n",
      "\n",
      "El desarrollo y despliegue de LLM presenta una serie de retos significativos que deben abordarse para garantizar su uso responsable, ético y seguro. Esta sección explorará varios de los principales desafíos para las organizaciones en su despliegue y uso de los LLM.\n",
      "\n",
      "Para abordar estos desafíos, LLMOps se apoya en una serie de herramientas y frameworks específicos, como MLFlow83, CometML84 y Weights & Biases85. Estas plataformas ofrecen funcionalidades para el seguimiento de experimentos, gestión de modelos, monitorización del rendimiento y colaboración entre equipos.\n",
      "\n",
      "Para abordar el reto de los sesgos, es necesario desarrollar técnicas robustas para detectar, medir y mitigar su presencia en los LLM. Esto implica la creación de conjuntos de datos de evaluación específicos para sesgos, el uso de métricas de equidad y la aplicación de técnicas de eliminación de sesgos (debiasing) tanto en el preentrenamiento como en el fine-tuning. Además, es crucial establecer procesos de auditoría y monitoreo continuo para garantizar que los LLM sigan siendo imparciales a lo largo del tiempo.\n",
      "\n",
      "83 Zaharia (2018).\n",
      "\n",
      "84 CommetML: https://www.comet.com/\n",
      "\n",
      "85 Weights and biases: https://wandb.ai/site\n",
      "# Para abordar las alucinaciones en los LLM, se están\n",
      "\n",
      "desarrollando diversos métodos centrados en la mejora de los datos de entrenamiento, la aplicación de técnicas de regularización robustas y el uso de retroalimentación humana para ajustar las respuestas del modelo. Además, se están investigando cambios arquitectónicos en los modelos para hacerlos inherentemente menos propensos a alucinar. Los métodos de generación de texto y el contexto de entrada también pueden ser optimizados para reducir las alucinaciones. La supervisión humana y la evaluación rigurosa son esenciales para detectar y corregir información inexacta. Asimismo, el desarrollo de herramientas específicas, como modelos de evaluación de alucinaciones y técnicas de ofuscación, puede contribuir a mejorar la precisión de los LLM.\n",
      "\n",
      "# MANAGEMENT SOLUTIONS\n",
      "\n",
      "Confidencialidad y protección de la información\n",
      "\n",
      "Los LLM a menudo se entrenan con grandes cantidades de datos que pueden contener información personal, sensible o confidencial. Además, cuando se despliegan en aplicaciones del mundo real, estos modelos pueden estar expuestos a entradas de usuario que también pueden incluir datos privados.\n",
      "\n",
      "# Explicabilidad y accountability\n",
      "\n",
      "El auge de los large language models: de los fundamentos a la aplicación. Esto plantea importantes desafíos de privacidad y seguridad, ya que los LLM pueden memorizar y reproducir información confidencial de sus datos de entrenamiento, o ser vulnerables a ataques que intenten extraer datos privados a través de consultas cuidadosamente diseñadas.\n",
      "\n",
      "Esta falta de transparencia plantea problemas de accountability, especialmente cuando los LLM se utilizan en contextos de alta sensibilidad, donde las decisiones tienen un impacto significativo en las personas (p. ej., uso de LLM en medicina, investigación farmacéutica, infraestructura crítica o acceso al mercado laboral). Sin una comprensión clara de cómo funcionan estos modelos, es difícil determinar la responsabilidad en caso de errores o comportamientos no deseados.\n",
      "\n",
      "Para abordar este reto, es necesario desarrollar técnicas y herramientas que permitan una mayor interpretabilidad y explicabilidad de los LLM. Esto incluye métodos para visualizar y analizar los mecanismos internos de atención, técnicas de atribución para identificar las partes más relevantes de la entrada, y enfoques para generar explicaciones en lenguaje natural de las predicciones del modelo.\n",
      "\n",
      "Para abordar este reto, es necesario desarrollar técnicas de preservación de la privacidad en el entrenamiento y despliegue de LLM (p. ej., Digger86 para detectar información protegida, el uso de datos ficticios87 durante el entrenamiento para detectar el material con copyright).\n",
      "\n",
      "Además, es crucial establecer protocolos robustos de seguridad y control de acceso para proteger los LLM y sus datos asociados de accesos no autorizados o usos maliciosos. Esto puede implicar el uso de técnicas de autenticación y autorización, monitoreo de seguridad y detección de anomalías.\n",
      "\n",
      "86 Li (2024).\n",
      "\n",
      "87 Meeus (2024).\n",
      "# Consumo racional de los recursos\n",
      "\n",
      "El entrenamiento y despliegue de LLM requiere cantidades masivas de recursos computacionales, almacenamiento y energía. Con modelos que alcanzan los cientos de miles de millones o incluso los billones de parámetros, el coste financiero y ambiental de desarrollar y operar estos sistemas puede ser muy significativo88.\n",
      "\n",
      "# Riesgos de seguridad y uso malicioso\n",
      "\n",
      "Los LLM pueden ser vulnerables a ataques adversarios, como la inyección de datos envenenados o la ingeniería inversa. Además, pueden utilizarse de forma maliciosa para generar desinformación, spam o contenido engañoso. Es esencial establecer medidas de seguridad robustas y diseñar los modelos con salvaguardias contra el uso indebido.\n",
      "\n",
      "# Cuestiones de propiedad intelectual y licencias\n",
      "\n",
      "El uso de LLM plantea preguntas sobre la propiedad intelectual y las licencias de los datos de entrenamiento, los modelos y los resultados generados. Adicionalmente, existe un riesgo de robo de información o datos personales de usuarios que lancen consultas a LLM desplegados en nubes de terceros. Es necesario cumplir con la regulación y con los marcos éticos para equilibrar los derechos de los creadores, los usuarios y el interés público, y, en el caso de las organizaciones, para evitar riesgos legales y de cumplimiento.\n",
      "\n",
      "# Escalabilidad de la arquitectura de los LLM\n",
      "\n",
      "Un desafío adicional es la escalabilidad de los transformers a medida que aumenta el tamaño de las secuencias y los modelos. Los mecanismos de atención tienen una complejidad cuadrática con respecto a la longitud de la secuencia, lo que limita su aplicación a secuencias muy largas.\n",
      "\n",
      "También se está investigando en la mejora de las técnicas de preentrenamiento continuo89 y fine-tuning continuo90, que buscan integrar la capacidad de usar información de diversos dominios sin la necesidad de depender de un reentrenamiento exhaustivo y costoso con nuevos datos específicos. Asimismo, se está avanzando en el uso de sistemas innovadores y el diseño de algoritmos verdes de IA, que permiten hacer frente a los costes computacionales y ambientales asociados a la IA (p. ej., el sistema GreenLightningAI, de Qsimov Quantum Computing91, desarrolla el reentrenamiento incremental y proporciona una interpretabilidad directa).\n",
      "\n",
      "# Desarrollo de infraestructuras y plataformas sostenibles\n",
      "\n",
      "Otra dirección es el desarrollo de infraestructuras y plataformas de computación más sostenibles, como el uso de hardware especializado de bajo consumo, sistemas de enfriamiento más eficientes y fuentes de energía renovables para alimentar los centros de datos donde se entrenan y despliegan los LLM.\n",
      "\n",
      "Además, es importante promover prácticas de uso racional y compartido de los recursos, como la reutilización y adaptación de modelos preentrenados en lugar de entrenar nuevos modelos desde cero para cada tarea, y el intercambio de recursos y conocimientos entre organizaciones y comunidades de investigación.\n",
      "\n",
      "# Otros desafíos\n",
      "\n",
      "De entre los muchos retos adicionales a los que se enfrentan las organizaciones en el desarrollo, implementación y uso de los LLM, por su importancia cabe mencionar brevemente:\n",
      "\n",
      "# Dependencia y lock-in\n",
      "\n",
      "Las organizaciones que dependen de LLM proporcionados por terceros pueden enfrentarse a riesgos de dependencia y lock-in, especialmente si los modelos se basan en datos o infraestructura propietaria. Es importante considerar estrategias de diversificación y planes de contingencia.\n",
      "\n",
      "88*iDanae 1T24 (2024).*\n",
      "\n",
      "89*Yıldız (2024).*\n",
      "\n",
      "90*Mehta (2023).*\n",
      "\n",
      "91*iDanae 1T24 (2024).*\n",
      "\n",
      "92*Pankajakshan (2024).*\n",
      "\n",
      "93*Rae (2021).*\n",
      "# Marco de validación de los LLM\n",
      "\n",
      "“Las consecuencias de que la IA vaya mal son graves, por lo que debemos ser proactivos en lugar de reactivos“.\n",
      "\n",
      "Elon Musk94\n",
      "\n",
      "# MANAGEMENT SOLUTIONS\n",
      "\n",
      "# El auge de los large language models: de los fundamentos a la aplicación\n",
      "Los modelos de lenguaje de gran escala (LLM) ofrecen un gran potencial para transformar diversos sectores y aplicaciones, pero también conllevan riesgos significativos que deben abordarse. Estos riesgos incluyen la generación de información errónea o alucinaciones, la perpetuación de sesgos, la dificultad para olvidar la información aprendida, preocupaciones éticas y de equidad, problemas de privacidad por uso indebido, dificultades en la interpretación de los resultados, y la potencial creación de contenido malicioso, entre otros.\n",
      "\n",
      "Dado el impacto potencial de estos riesgos, es necesario validar exhaustivamente los LLM antes de su despliegue en entornos de producción. De hecho, la validación de los LLM no es solo una buena práctica, sino también un requisito regulatorio en muchas jurisdicciones. En Europa, la propuesta de AI Act exige una evaluación y mitigación de los riesgos de los sistemas de IA, mientras que, en Estados Unidos, el marco de gestión de riesgos de IA del NIST y el AI Bill of Rights destacan la importancia de comprender y abordar los riesgos inherentes a estos sistemas.\n",
      "\n",
      "La validación de los LLM puede partir de los principios establecidos en la disciplina de riesgo de modelo, que se centra en evaluar y mitigar los riesgos derivados de errores, deficiente implementación o mal uso de los modelos. Sin embargo, en el caso de la IA, y particularmente de los LLM, es necesario adoptar una perspectiva más amplia que abarque los otros riesgos que comportan. Un enfoque integral de validación es esencial para garantizar un despliegue seguro y responsable de los LLM.\n",
      "\n",
      "# 4 Datos de entrada\n",
      "\n",
      "comprensión del texto, calidad del dato.\n",
      "\n",
      "# 4 Solidez conceptual y diseño del modelo\n",
      "\n",
      "selección del modelo y sus componentes (p. ej., metodologías de fine-tuning, conexiones a bases de datos, RAG), y comparación con otros modelos.\n",
      "\n",
      "Este enfoque holístico se plasma en un marco de validación multidimensional para los LLM, que cubre aspectos clave como el riesgo de modelo, la gestión de datos y privacidad, la ciberseguridad, los riesgos legales y de cumplimiento normativo, los riesgos operativos y tecnológicos, la ética y la reputación, y el riesgo de proveedor, entre otros.\n",
      "\n",
      "Management Solutions (2014). Model Risk Management: Quantitative and Qualitative Aspects.\n",
      "\n",
      "Imperial et al. (2023).\n",
      "\n",
      "Wettig et al. (2024).\n",
      "\n",
      "RAG (Retrieval-Augmented Generation) es una técnica avanzada en la que un modelo de lenguaje busca información relevante de una fuente externa antes de generar texto. Esto enriquece las respuestas con conocimientos precisos y actuales, combinando inteligentemente la búsqueda de información y la generación de texto. Al integrar datos de fuentes externas, los modelos RAG, como los RAG-Token y RAG-Sequence propuestos, ofrecen respuestas más informadas y coherentes, minimizando el riesgo de generar contenido inexacto o 'alucinaciones'. Este avance representa un paso significativo hacia modelos de inteligencia artificial más confiables y basados en evidencia real.\n",
      "\n",
      "Khang (2024).\n",
      "# Fig. 9. Riesgos asociados a la IA y referencia regulatoria en el AI Act.\n",
      "\n",
      "|Compliance & Legal Risk|Compliance & Legal Risk|\n",
      "|---|\n",
      "|Ley AI Act Art. 8, 9|Compliance with AI Act, GDPR, ethical AI frameworks, intellectual property|\n",
      "|Riesgo de Modelo|OpRisk, riesgo informático y ciberseguridad|\n",
      "|Ley AI Act Art. 8, 9, 10, 14, 15, 29|Política de GRM, inventario, directrices de validación, clasificación de riesgos, XAI y detección de sesgos.|\n",
      "|Riesgo de IA|Riesgo del proveedor|\n",
      "|Ley AI Act Art. 8, 9, 12|Selección de terceros, ética del proveedor con respecto a la IA, integración de IA, derechos de autor|\n",
      "|Riesgo Reputacional|ESG y Riesgo Reputacional|\n",
      "|Ley AI Act Art. 8, 29a|Ética, equidad, impacto medioambiental, impacto social, reputación|\n",
      "|Gestión y privacidad de datos|Ley AI Act Art. 8, 10|\n",
      "|Transparencia, consentimiento para el uso de IA, anonimización, mantenimiento de registros, sesgo en los datos, envenenamiento de datos|Transparencia, consentimiento para el uso de IA, anonimización, mantenimiento de registros, sesgo en los datos, envenenamiento de datos|\n",
      "\n",
      "# 4 Evaluación del modelo y análisis de sus resultados:\n",
      "\n",
      "privacidad y seguridad de los resultados102, precisión del modelo103, consistencia104, robustez105, adaptabilidad106, interpretabilidad (XAI)107, ética, sesgos y equidad108, toxicidad109, comparación contra modelos challenger.\n",
      "\n",
      "# 4 Implementación y uso:\n",
      "\n",
      "revisión humana en el uso (incluyendo el monitoreo de usos indebidos), resolución de errores, escalabilidad y eficiencia, aceptación del usuario.\n",
      "\n",
      "# 4 Gobernanza110 y ética111:\n",
      "\n",
      "marco de gobierno de la IA generativa, incluyendo los LLM.\n",
      "\n",
      "# 4 Documentación112:\n",
      "\n",
      "completitud de la documentación del modelo.\n",
      "\n",
      "# 38 El auge de los large language models: de los fundamentos a la aplicación\n",
      "\n",
      "4 Cumplimiento regulatorio113: evaluación de los requisitos regulatorios (p. ej., AI Act).\n",
      "\n",
      "Para garantizar el uso efectivo y seguro de los modelos de lenguaje, es fundamental realizar una evaluación de riesgos que considere tanto el modelo en sí como su uso específico. Esto asegura que, independientemente de su origen (in-house o de un proveedor) o personalización (fine-tuning), el modelo funcione adecuadamente en su contexto de uso, cumpliendo con los estándares de seguridad, ética y regulación necesarios.\n",
      "\n",
      "Técnicas de validación Cuando una organización se plantea implementar un LLM para un caso de uso específico, puede ser beneficioso adoptar un enfoque integral que abarque las dimensiones clave del ciclo de vida del modelo: datos, diseño, evaluación, implementación y uso. Asimismo, de manera transversal, resulta necesario evaluar el cumplimiento de la normativa aplicable, como el AI Act en la Unión Europea.\n",
      "\n",
      "102Nasr (2023). 103Liang (2023). 104Elazar (2021). 105Liu (2023). 106Dun (2024). 107Singh (2024). 108NIST (2023), Oneto (2020) y Zhou (2021). 109Shaikh (2023). 110Management Solutions (2014). Model Risk Management. 111Oneto (2020). 112NIST (2023). 113European Parliament (2024). AI Act.\n",
      "# Fig. 10. Pruebas de evaluación de LLM.\n",
      "\n",
      "|Dimensiones|Aspectos|Descripción|Métricas de validación (ejemplos)|Evaluación humana (ejemplos)|\n",
      "|---|---|---|---|---|\n",
      "|1. Datos de entrada|1.1 Calidad de dato|Grado de calidad de la modelización o de los datos de aplicación|• Flesch-Kinkaid Grade • Revisión de los elementos del LLM: RAG, filtros de entrada o salida, definición de prompts, fine-tuning, optimización, etc.|• Revisión caso a caso|\n",
      "|2. Diseño del modelo|2.1 Diseño del modelo|Elección de modelos y metodología adecuadas|• Pruebas A/B|• Comparación contra otros LLM|\n",
      "|3. Evaluación del modelo|3.1 Privacidad y seguridad|Respeto de la confidencialidad y no regurgitación de información personal|• Data leakage • PII tests, K-anonymity|• Registros • Hacking ético|\n",
      "|3.2 Precisión|Corrección y pertinencia de las respuestas del modelo|• Traducción: BLEU, Ruby, ROUGE-L • Otros: Sistemas de QA, nivel de overrides, nivel de alucinaciones, etc. • Benchmarks: XSUM, LogiQA, WikiData, etc.|• Revisión caso a caso| |\n",
      "|3.3 Consistencia|Respuestas uniformes a consultas similares|• Cosine similarity • Jaccard similarity index|• Revisión caso a caso • Pruebas A/B| |\n",
      "|3. Evaluación del modelo|3.4 Robustez|Resiliencia a la información adversa o engañosa|• Generación de texto adversario (TextFooler), patrones Regex • Benchmarks de ataques adversarios (PromptBench), número de refusals|• Hacking ético • Simulacros de incidentes|\n",
      "|3.5 Adaptabilidad|Capacidad para aprender o adaptarse a nuevos contextos|• Rendimiento del LLM ante datos nuevos por Zero/One/Few-shot learning|• Pruebas A/B • Revisión caso a caso| |\n",
      "|3.6 Explicabilidad|Comprensión del proceso de toma de decisiones|• SHAP • Puntuaciones de explicabilidad|• Hacking ético • Focus groups| |\n",
      "|3. Evaluación del modelo|3.7 Sesgos y equidad|Respuestas sin sesgo demográfico|• WEAT score, paridad demográfica, asociaciones de palabras, etc. • Benchmarks de sesgos (BBQ, etc.)|• Hacking ético • Focus groups|\n",
      "|3.8 Toxicidad|Propensión a generar contenidos nocivos|• Perspective API, Hatebase API • Toxicity benchmarks (RealToxicityPrompts, BOLD, etc.)|• Hacking ético • Focus groups| |\n",
      "|4. Implementación y uso|4.1 Revisión humana y seguridad de uso|Exclusión de sugerencias perjudiciales o ilegales e inclusión de una revisión humana (‘human-in-the-loop’)|• Protocolos de riesgos, evaluaciones de seguridad • Control humano|• Hacking ético • Focus groups|\n",
      "|4.2 Recuperación y gestión de errores|Capacidad para recuperarse de errores y gestionar entradas inesperadas|• Tests de recuperación del sistema • Métricas de procesamiento de errores|• Simulacros de incidentes| |\n",
      "|4. Implementación y uso|4.3 Escalabilidad|Mantenimiento del rendimiento con más datos o usuarios|• Stress testing del sistema, Apache Jmeter, etc. • Benchmarks de escalabilidad|• Simulacros de incidentes • Pruebas A/B|\n",
      "|4.4 Eficiencia|Utilización de recursos y velocidad de respuesta|• Time-to-first-byte (TTFB), uso de GPU/CPU, inferencia de emisiones, memoria, latencia|• Simulacros de incidentes| |\n",
      "|4.5 Aceptación del usuario|Pruebas de aceptación de usuario|• Checklist de requisitos de usuario, opt-out del usuario • Satisfacción del usuario (Net Promoter Score, CSAT)|• UX tracking • Pruebas A/B| |\n",
      "La selección exacta de técnicas dependerá de las características particulares del caso de uso; y, en concreto, varios factores importantes a tener en cuenta para decidir las técnicas más adecuadas son:\n",
      "\n",
      "# 4\n",
      "\n",
      "El nivel de riesgo y la criticidad de las tareas que se confiarán al LLM.\n",
      "\n",
      "# 4\n",
      "\n",
      "Si el LLM está abierto al público (y por tanto el hacking ético cobra especial relevancia) o si su uso se limita al ámbito interno de la organización.\n",
      "\n",
      "# 4\n",
      "\n",
      "Si el LLM procesa datos personales.\n",
      "\n",
      "# 4\n",
      "\n",
      "La línea de negocio o servicio que utilizará el LLM.\n",
      "\n",
      "Un análisis cuidadoso de estos drivers permitirá construir un marco de validación robusto y adaptado a las necesidades de cada uso de un LLM.\n",
      "\n",
      "# Métricas de evaluación cuantitativas\n",
      "\n",
      "Aunque es un campo de estudio emergente, existe una amplia gama de métricas cuantitativas para evaluar el rendimiento de los LLM. Algunas de estas métricas son adaptaciones de las utilizadas en modelos tradicionales de aprendizaje automático, como la precisión, la exhaustividad (recall), la puntuación F1 o el área bajo la curva ROC (AUC-ROC). Otras métricas han sido diseñadas específicamente para evaluar aspectos únicos de los LLM, como la coherencia del texto generado, la fidelidad a los hechos o la diversidad del lenguaje.\n",
      "\n",
      "En este sentido, ya existen marcos holísticos de testeo cuantitativo de LLM en entornos de programación en Python, que facilitan la implementación de muchas de las métricas cuantitativas de validación, por ejemplo:\n",
      "\n",
      "# El auge de los large language models: de los fundamentos a la aplicación\n",
      "\n",
      "Fig. 11. Algunas técnicas de evaluación humana de LLM.\n",
      "\n",
      "# 4\n",
      "\n",
      "LLM Comparator114: herramienta creada por investigadores de Google para la evaluación automática y comparación de LLM, que revisa la calidad de las respuestas de los LLM.\n",
      "\n",
      "# 4\n",
      "\n",
      "HELM115: evaluación holística de los modelos del lenguaje, que compila métricas de evaluación a lo largo de siete dimensiones (precisión, calibración, robustez, equidad, sesgos, toxicidad y eficiencia) para una serie de escenarios predefinidos.\n",
      "\n",
      "# 4\n",
      "\n",
      "ReLM116: sistema de validación y consulta de LLM mediante uso del lenguaje, incluyendo evaluaciones de modelos lingüísticos, memorización, sesgos, toxicidad y comprensión del lenguaje.\n",
      "\n",
      "En la actualidad, ciertas técnicas de validación, como los métodos de explicabilidad (XAI) basados en SHAP, algunas métricas como ROUGE117 o los análisis de imparcialidad mediante paridad demográfica, aún no cuentan con umbrales predefinidos ampliamente aceptados. En estos casos, es tarea de la comunidad científica y de la industria seguir investigando para establecer criterios claros que permitan una validación robusta y estandarizada.\n",
      "\n",
      "# Backtest de anulaciones\n",
      "\n",
      "Contabilizar y medir la importancia de las modificaciones humanas en los resultados del LLM.\n",
      "\n",
      "# Grupos de discusión\n",
      "\n",
      "Recoger opiniones sobre los resultados del LLM de diversos usuarios (en materia de ética, adecuación cultural, discriminación, etc.).\n",
      "\n",
      "# Comprobación caso por caso\n",
      "\n",
      "Comparar una muestra representativa (por ejemplo, un mínimo de 200 a través de la prueba Z1) de respuestas LLM con resultados humanos (\"ground truth\"), incl. doble ciego.\n",
      "\n",
      "# Hacking ético (también conocido como Equipo Rojo)\n",
      "\n",
      "Manipular los mensajes para forzar al LLM a producir resultados no deseados (incl. regurgitación de PII, cumplimiento, ingeniería de mensajes, pruebas de penetración, vulnerabilidades de IA, etc.).\n",
      "\n",
      "# Pruebas A/B\n",
      "\n",
      "Realizar ensayos paralelos para evaluar diferentes versiones (A y B) o comparar con el rendimiento humano.\n",
      "\n",
      "# Seguimiento de la experiencia del usuario (UX)\n",
      "\n",
      "Observar y evaluar las interacciones de los usuarios con el LLM a lo largo del tiempo / en tiempo real.\n",
      "\n",
      "# Simulacros de incidentes\n",
      "\n",
      "Simular escenarios adversos para probar la respuesta y recuperación del LLM (prueba de estrés, comprobación de copias de seguridad, medición del tiempo de recuperación, etc.).\n",
      "\n",
      "# Mantenimiento de registros\n",
      "\n",
      "Revisar los diarios y registros del sistema LLM, garantizando el cumplimiento de la normativa.\n",
      "\n",
      "114Kahng (2024).\n",
      "\n",
      "115Liang (2023).\n",
      "\n",
      "116Kuchnik (2023).\n",
      "\n",
      "117Duan (2023).\n",
      "# Técnicas de evaluación humana\n",
      "\n",
      "# Benchmarks de evaluación de LLM\n",
      "\n",
      "Mientras que las métricas de evaluación cuantitativa son implementables de forma más directa debido a la multitud de recursos online y publicaciones de los últimos años, las técnicas de evaluación humana118 son variadas y deben ser construidas en función de la tarea específica119 que esté realizando el LLM, e incluyen (Fig. 11):\n",
      "\n",
      "- Backtest de los forzajes del usuario: contabilizar y medir la importancia de las modificaciones humanas en los resultados del LLM (p. ej., cuántas veces un gestor comercial debe modificar manualmente los resúmenes de llamadas a clientes que ha realizado un LLM).\n",
      "- Revisión caso a caso: comparar una muestra representativa de respuestas del LLM con las expectativas del usuario («ground truth“).\n",
      "- Hacking ético (Red Team): manipular los prompts para forzar al LLM a producir resultados no deseados (p. ej., regurgitación de información personal, contenido ilegal, tests de penetración, explotación de vulnerabilidades).\n",
      "- Testeo A/B: comparación para evaluar dos versiones del LLM (A y B), o de un LLM frente a un ser humano.\n",
      "- Focus groups: recabar opiniones de diversos usuarios sobre el comportamiento del LLM, p. ej., en materia de ética, adecuación cultural, discriminación, etc.\n",
      "- Experiencia del usuario (UX tracking): observar y evaluar las interacciones de los usuarios con el LLM a lo largo del tiempo o en tiempo real.\n",
      "- Simulacros de incidentes: simular escenarios adversos para probar la respuesta del LLM (p. ej., prueba de estrés, comprobación de copias de seguridad, medición del tiempo de recuperación, etc.).\n",
      "- Mantenimiento de registros: revisar los diarios y registros del sistema LLM, garantizando el cumplimiento de la normativa y la traza de auditoría.\n",
      "\n",
      "# Benchmarks\n",
      "\n",
      "La mayoría de los modelos de inteligencia artificial generativa, incluidos los LLM, se someten a pruebas utilizando benchmarks públicos que evalúan su desempeño en una variedad de tareas relacionadas con la comprensión y el uso del lenguaje natural. Estas pruebas sirven para medir cómo maneja el LLM tareas específicas y refleja el entendimiento humano. Algunos de estos benchmarks incluyen:\n",
      "\n",
      "- GLUE/SuperGLUE: evalúa la comprensión del lenguaje a través de tareas que miden la capacidad de un modelo para entender el texto.\n",
      "- Eleuther AI Language Model Evaluation Harness: realiza una evaluación “few-shot” de los modelos, es decir, su precisión con muy pocos ejemplos de entrenamiento.\n",
      "- ARC (AI2 Reasoning Challenge): pone a prueba la habilidad del modelo para responder preguntas de ciencia que requieren razonamiento.\n",
      "- HellaSwag: evalúa el sentido común del modelo a través de tareas que requieren predecir el final coherente de una historia.\n",
      "- MMLU (Massive Multitask Language Understanding): prueba la precisión del modelo en una amplia gama de tareas para evaluar su comprensión multitarea.\n",
      "- TruthfulQA: desafía al modelo a discernir entre información verdadera y falsa, evaluando su habilidad para manejar datos verídicos.\n",
      "- Winogrande: otra herramienta para evaluar el sentido común, similar a HellaSwag pero con diferentes métodos y énfasis.\n",
      "- GSM8K: evalúa la capacidad lógico-matemática del modelo a través de problemas de matemáticas diseñados para estudiantes.\n",
      "\n",
      "118Datta, Dickerson (2023).\n",
      "\n",
      "119Guzmán (2015).\n",
      "# Nuevas tendencias\n",
      "\n",
      "El campo de la validación de LLM se encuentra en constante evolución, impulsado por los rápidos avances en el desarrollo de estos modelos y por la creciente conciencia sobre la importancia de garantizar su fiabilidad, equidad y alineación con la ética y la regulación.\n",
      "\n",
      "A continuación, se presentan algunas de las principales tendencias emergentes en este ámbito:\n",
      "\n",
      "# 4. Explicabilidad de los LLM\n",
      "\n",
      "a medida que los LLM ganan en complejidad y opacidad, crece la demanda de mecanismos que permitan entender y explicar su funcionamiento interno. Las técnicas de XAI (eXplainable AI) como SHAP, LIME o la atribución de importancia a los tokens de entrada están ganando protagonismo en la validación de LLM. Aunque para los modelos tradicionales hay una variedad de técnicas post-hoc disponibles para comprender el funcionamiento de los modelos a nivel local y global120 (p. ej., Anchors, PDP, ICE), y ha proliferado la definición e implementación de modelos inherentemente interpretables por construcción, la implementación de estos principios para los LLM no está todavía resuelta.\n",
      "\n",
      "# 4. Uso de LLM para explicar LLM\n",
      "\n",
      "una tendencia emergente consiste en utilizar un LLM para generar explicaciones sobre el comportamiento o las respuestas de otro LLM. En otras palabras, se emplea un modelo del lenguaje para interpretar y comunicar de forma más comprensible el razonamiento subyacente de otro modelo. Para enriquecer estas explicaciones, se están desarrollando herramientas121 que incorporan además técnicas de análisis post-hoc.\n",
      "\n",
      "# 4. Técnicas de interpretabilidad post-hoc\n",
      "\n",
      "estas técnicas se basan en la interpretabilidad de los resultados en la etapa posterior al entrenamiento o fine-tuning, y permiten identificar qué partes de la entrada han influido más en la respuesta del modelo (importancia de características), encontrar ejemplos similares en el conjunto de datos de entrenamiento (similitud basada en embeddings) o diseñar prompts específicos que guíen al modelo hacia explicaciones más informativas (estrategias de prompting).\n",
      "\n",
      "# 4. Puntuaciones por atribución\n",
      "\n",
      "como parte de la interpretabilidad post-hoc, se están desarrollando técnicas122 que permiten identificar qué partes del texto de entrada tienen mayor influencia en la respuesta generada por un LLM. Ayudan a entender qué palabras o frases son más importantes para el modelo. Existen diferentes métodos para calcular estas puntuaciones:\n",
      "\n",
      "- Métodos basados en el gradiente: analizan cómo cambian los gradientes (una medida de sensibilidad) para cada palabra al retroceder por la red neuronal.\n",
      "- Métodos basados en perturbaciones: modifican ligeramente el texto de entrada y observan cómo cambia la respuesta del modelo.\n",
      "- Interpretación de métricas internas: utilizan métricas calculadas por el propio modelo, como los pesos de atención en los transformers, para determinar la importancia de cada palabra.\n",
      "\n",
      "# Resumen de la salida\n",
      "\n",
      "”Todavía se está evaluando el coste total de los daños en Newton Stewart, una de las zonas más afectadas. La Primera Ministra, Nicola Sturgeon, visitó la zona para inspeccionar los daños. El líder adjunto del Partido Laborista Escocés, Alex Rowley, estuvo el lunes en Hawick para ver la situación de primera mano. Afirmó que era importante aplicar correctamente el plan de protección contra las inundaciones\".\n",
      "\n",
      "|Of|+ damage|+ in|+ Newton|+ Stewart|+ .|+2|\n",
      "|---|---|---|---|---|---|---|\n",
      "| | |The|+1,81| |+1,81| |\n",
      "|full|+1,81| |cost|+1,81| | |\n",
      "|One + 11 other features|+0,31|remain + 24 other features|+0,24|to + 79 other features|+0,46|habe + 95 other features|\n",
      "|+ 292 other features|+1,6| | | | | |\n",
      "\n",
      "0,00 0,25 0,50 0,75 1,00 1,25 1,50 1,75 2,00\n",
      "\n",
      "SHAP value\n",
      "Un ejemplo de puntuación por atribución es la aplicación de la técnica SHAP (SHapley Additive exPlanations) aplicado a un LLM, lo que facilita su interpretación y comprensión (Fig. 12). SHAP es un método de explicabilidad post-hoc basado en la teoría de juegos cooperativos. Asigna a cada característica (token) un valor de importancia (valor Shapley) que representa su contribución a la predicción del modelo.\n",
      "\n",
      "# 4 Validación continua y monitorización en producción:\n",
      "\n",
      "Más allá de la evaluación puntual antes del despliegue, se extiende la práctica de realizar un seguimiento continuo del comportamiento de los LLM una vez que están en uso, al igual que se hace con modelos tradicionales. Esto permite detectar posibles desviaciones o degradaciones en su rendimiento a lo largo del tiempo, así como identificar sesgos o riesgos no previstos inicialmente.\n",
      "\n",
      "# 4 Validación colaborativa y participativa:\n",
      "\n",
      "Se promueve una mayor implicación de diversos stakeholders en el proceso de validación, incluyendo no solo a expertos técnicos sino también a usuarios finales, reguladores, auditorías externas y representantes de la sociedad civil. Esta participación plural permite incorporar diferentes perspectivas y fomenta la transparencia y la responsabilidad.\n",
      "\n",
      "# 4 Validación ética y alineada con la regulación:\n",
      "\n",
      "Más allá de las métricas de rendimiento, se otorga cada vez más importancia a evaluar si el comportamiento de los LLM es ético y está alineado con los valores humanos y con la regulación. Esto implica analizar cuestiones como la equidad, la privacidad, la seguridad, la transparencia o el impacto social de estos sistemas.\n",
      "\n",
      "Ejemplo: se considera un LLM entrenado para clasificar correos electrónicos corporativos como \"importante\" o \"no importante\". Dado el vector de tokens de entrada:\n",
      "\n",
      "x = [El, informe, financiero, del, Q2, muestra, un, aumento, significativo, en, los, ingresos, y, la, rentabilidad]\n",
      "\n",
      "El modelo clasifica el correo como \"importante\" con f(x) = 0.85. Aplicando SHAP, se obtienen los siguientes valores Shapley:\n",
      "\n",
      "|Token|Valor Shapley|\n",
      "|---|---|\n",
      "|φ1|0.01 (El)|\n",
      "|φ2|0.2 (informe)|\n",
      "|φ3|0.15 (financiero)|\n",
      "|φ4|0.02 (del)|\n",
      "|φ5|0.1 (Q2)|\n",
      "|φ6|0.05 (muestra)|\n",
      "|φ7|0.01 (un)|\n",
      "|φ8|0.15 (aumento)|\n",
      "|φ9|0.1 (significativo)|\n",
      "|φ10|0.01 (en)|\n",
      "|φ11|0.02 (los)|\n",
      "|φ12|0.12 (ingresos)|\n",
      "|φ13|0.01 (y)|\n",
      "|φ14|0.02 (la)|\n",
      "|φ15|0.08 (rentabilidad)|\n",
      "\n",
      "Interpretación: los tokens \"informe\" (0.2), \"financiero\" (0.15), \"aumento\" (0.15) e \"ingresos\" (0.12) tienen las mayores contribuciones a la clasificación del correo como \"importante\". Esto sugiere que el LLM ha aprendido a asociar estos términos con la importancia del mensaje en un contexto empresarial.\n",
      "\n",
      "123 Liu (2024).\n",
      "\n",
      "124 Jang (2022).\n",
      "\n",
      "125 Yu (2023).\n",
      "\n",
      "126 Wu (2023).\n",
      "# Caso práctico: validación de un chatbot de políticas\n",
      "\n",
      "“La inteligencia artificial alcanzará niveles humanos hacia 2029“. Ray Kurzweil127\n",
      "\n",
      "“Creo que tendremos una IA más inteligente que cualquier ser humano probablemente a finales de 2025“. Perplexity128\n",
      "\n",
      "# MANAGEMENT SOLUTIONS\n",
      "\n",
      "# Fia\n",
      "\n",
      "44\n",
      "\n",
      "# El auge de los large language models: de los fundamentos a la aplicación\n",
      "# Diseño del enfoque de validación\n",
      "\n",
      "Para ilustrar la aplicación de las técnicas de validación de un LLM descritas, en esta sección se presenta un caso práctico de validación de un chatbot de políticas internas de una compañía. Para validar de manera integral el chatbot de políticas, siguiendo el marco presentado en la sección anterior, se ha diseñado un enfoque de validación a medida que abarca las dimensiones clave del ciclo de vida del modelo: datos, diseño, evaluación, implementación y uso. Este enfoque combina métricas cuantitativas y técnicas de evaluación humana, con el objetivo de obtener una visión completa del desempeño y la adecuación del chatbot en el contexto específico de la compañía.\n",
      "\n",
      "# Definición del caso\n",
      "\n",
      "La compañía ha desarrollado un chatbot basado en un LLM de código abierto para responder preguntas y proporcionar información sobre sus políticas internas. El objetivo principal de este chatbot es facilitar el acceso de los empleados a las políticas de la empresa. A continuación, se resumen las pruebas y técnicas seleccionadas para cada dimensión:\n",
      "\n",
      "El chatbot ha sido construido utilizando una infraestructura en la nube y se ha alimentado con todas las políticas de la compañía, que abarcan aproximadamente 1.000 páginas de documentación. Para mejorar su capacidad de respuesta, se han aplicado técnicas de Retrieval-Augmented Generation (RAG), que permiten al modelo recuperar información relevante de su base de conocimientos antes de generar una respuesta. Inicialmente, se consideró la posibilidad de aplicar fine-tuning al modelo, pero tras las pruebas iniciales, se concluyó que la combinación del LLM base con RAG era suficiente para obtener resultados satisfactorios.\n",
      "\n",
      "# Datos\n",
      "\n",
      "- Métricas: se utilizará la escala Flesch-Kincaid para evaluar la legibilidad y complejidad de las políticas que alimentan al chatbot.\n",
      "- Evaluación humana: se revisará una muestra representativa de las políticas para identificar posibles inconsistencias, errores o ambigüedades.\n",
      "\n",
      "# Diseño del modelo\n",
      "\n",
      "Antes de su implementación definitiva, la compañía ha decidido llevar a cabo un proceso de validación exhaustivo para evaluar la precisión, la seguridad y la adecuación del chatbot en el contexto específico de su uso previsto. Este proceso de validación tiene como objetivo identificar posibles áreas de mejora y garantizar que el chatbot cumpla con los estándares de calidad y las expectativas de la empresa.\n",
      "\n",
      "La validación del chatbot de políticas se realizará mediante una combinación de métricas cuantitativas y técnicas de evaluación humana, siguiendo el marco de validación multidimensional descrito en la sección anterior. Los resultados de este proceso servirán para tomar decisiones informadas sobre la implementación del chatbot y para establecer un plan de mejora continua.\n",
      "\n",
      "# Notas\n",
      "\n",
      "127 Ray Kurzweil (n. 1948). Director de Ingeniería en Google, científico computacional, inventor y futurista, conocido por la invención del OCR y por sus contribuciones en IA.\n",
      "\n",
      "128 Elon Musk (n. 1971), CEO de X, SpaceX, Tesla. Empresario sudafricano-estadounidense, conocido por fundar o cofundar empresas como Tesla, SpaceX y PayPal, dueño de X (anteriormente Twitter), red social que tiene su propio LLM, llamado Grok.\n",
      "\n",
      "129 La estrategia de \"chunking\" se refiere al proceso de dividir el texto de entrada a un LLM en unidades más pequeñas y manejables («chunks») durante su uso o implementación.\n",
      "# Evaluación del modelo\n",
      "\n",
      "# Adaptabilidad\n",
      "\n",
      "- Métricas: se evaluará el rendimiento del chatbot ante nuevas políticas o actualizaciones mediante técnicas de few-shot learning. Se evaluará la respuesta del chatbot ante idiomas no empleados en las políticas o solicitudes de traducciones a idiomas no incluidos en el RAG (p. ej., polaco).\n",
      "\n",
      "# Privacidad y seguridad\n",
      "\n",
      "- Métricas: se aplicarán pruebas de K-anonimato para evaluar la protección de datos personales en las respuestas del chatbot, y pruebas de PII (Personal Identificable Information) para identificar atributos sensibles en los datos, utilizando PIIfilter.\n",
      "- Evaluación humana: se realizarán pruebas de hacking ético para identificar posibles vulnerabilidades y se mantendrán registros detallados de las interacciones del chatbot.\n",
      "\n",
      "# Precisión\n",
      "\n",
      "- Métricas: se utilizarán las métricas Word Error Rate (WER) y ROUGE para evaluar la precisión de las respuestas del chatbot en comparación con las políticas originales. También se emplearán benchmarks específicos del dominio, como un conjunto de preguntas y respuestas diseñado por expertos en políticas de la compañía.\n",
      "- Evaluación humana: se realizará una revisión caso por caso de una muestra representativa de interacciones del chatbot para identificar posibles errores o imprecisiones.\n",
      "\n",
      "# Consistencia\n",
      "\n",
      "- Métricas: se aplicarán la similitud coseno y el índice de Jaccard para evaluar la consistencia de las respuestas del chatbot ante consultas similares.\n",
      "- Evaluación humana: se llevarán a cabo pruebas A/B para comparar las respuestas del chatbot en diferentes escenarios y se realizará una revisión caso por caso para identificar posibles inconsistencias.\n",
      "\n",
      "# Robustez\n",
      "\n",
      "- Métricas: se utilizarán herramientas como TextFooler para generar texto adversario y evaluar la resiliencia del chatbot ante información engañosa. Además, se contabilizará el número de rechazos del chatbot ante prompts malintencionados.\n",
      "- Evaluación humana: se realizarán pruebas A/B y revisiones caso por caso para evaluar la capacidad del chatbot para adaptarse a nuevos escenarios.\n",
      "\n",
      "# Explicabilidad\n",
      "\n",
      "- Métricas: se aplicarán técnicas de explicabilidad, como SHAP, para comprender el proceso de toma de decisiones del chatbot. Se evaluará el módulo de interpretabilidad intrínseco del chatbot, mediante el cual se provee una explicación del origen de la información en la respuesta al usuario.\n",
      "- Evaluación humana: se realizará un seguimiento de la experiencia del usuario (UX) y se llevará a cabo un focus group para evaluar la percepción de los usuarios sobre la transparencia y la explicabilidad del chatbot.\n",
      "\n",
      "# Sesgos y equidad\n",
      "\n",
      "- Métricas: se utilizará el conjunto de herramientas AI Fairness 360 para evaluar posibles sesgos demográficos en las respuestas del chatbot. También se emplearán benchmarks específicos, como el Bias Benchmark for QA (BBQ), para medir la equidad en el contexto de las políticas de la compañía.\n",
      "- Evaluación humana: se llevarán a cabo pruebas de hacking ético y un focus group para identificar posibles sesgos o discriminación en las respuestas del chatbot.\n",
      "\n",
      "# Toxicidad\n",
      "\n",
      "- Métricas: se aplicarán las herramientas Perspective API y Hatebase API para evaluar la presencia de lenguaje tóxico o inapropiado en las respuestas del chatbot. Además, se utilizarán benchmarks específicos, como RealToxicityPrompts, para medir la toxicidad en el contexto de las políticas de la compañía.\n",
      "- Evaluación humana: se realizarán pruebas de hacking ético para identificar posibles casos de lenguaje ofensivo o inapropiado en las interacciones del chatbot.\n",
      "\n",
      "- Evaluación humana: se realizarán pruebas de hacking ético y simulacros de incidentes para evaluar la capacidad del chatbot para manejar situaciones adversas.\n",
      "# Implementación y uso\n",
      "\n",
      "# Resultados\n",
      "\n",
      "# 4 Escalabilidad\n",
      "\n",
      "- Métricas: se realizarán pruebas de estrés del sistema utilizando Apache JMeter para evaluar el rendimiento del chatbot bajo altas cargas de trabajo.\n",
      "- Evaluación humana: se llevarán a cabo simulacros para evaluar la capacidad del chatbot para manejar un aumento imprevisto en el número de usuarios o consultas.\n",
      "\n",
      "# 4 Eficiencia\n",
      "\n",
      "- Métricas: se medirán el tiempo de respuesta (Time-to-First-Byte, TTFB), el uso de recursos (GPU/CPU, memoria) y la latencia para evaluar la eficiencia del chatbot.\n",
      "\n",
      "# 4 Aceptación del usuario\n",
      "\n",
      "- Métricas: se establecerá una lista de verificación de los requisitos del usuario y se medirá la satisfacción del usuario utilizando indicadores como el Net Promoter Score (NPS) y el Customer Satisfaction Score (CSAT).\n",
      "- Evaluación humana: se realizará un seguimiento de la experiencia del usuario (UX) para evaluar la aceptación y satisfacción de los usuarios con el chatbot.\n",
      "\n",
      "Este enfoque de validación personalizado permitirá a la compañía obtener una evaluación completa del chatbot de políticas, identificando áreas de mejora y garantizando su adecuación para el uso previsto. Los resultados de estas pruebas y evaluaciones servirán como base para la toma de decisiones informadas sobre la implementación y el perfeccionamiento continuo del chatbot.\n",
      "\n",
      "En cuanto a la implementación y uso, el chatbot demostró una buena escalabilidad y eficiencia en el manejo de altas cargas de trabajo. Además, la satisfacción de los usuarios fue alta, lo que indica una buena aceptación de la herramienta en el contexto de la compañía.\n",
      "# Fig. 13. Resumen de resultados de las métricas y técnicas de evaluación humana del chatbot de políticas.\n",
      "\n",
      "|Dimensión|Prueba|Resultado|Interpretación|\n",
      "|---|---|---|---|\n",
      "|Datos|Flesch-Kincaid|Legibilidad adecuada (grado 8)|Las políticas son comprensibles para la mayoría de los usuarios|\n",
      "| |Revisión humana|Sin inconsistencias significativas|Las políticas son coherentes y no contienen errores importantes|\n",
      "| |Modelos challenger|Mejoras en parámetros identificadas|Se requiere adaptar los parámetros del RAG al contexto de las políticas (p. ej., chunk size) para mejorar la captura de información en preguntas muy específicas|\n",
      "|Diseño del modelo|Revisión de componentes|Configuración óptima|El diseño del chatbot es apropiado para el caso de uso|\n",
      "| |Pruebas A/B|Rendimiento superior a otros LLM|El chatbot supera a otros modelos disponibles en el mercado|\n",
      "| |K-anonimato|Protección adecuada de datos|El chatbot no revela información sensible en sus respuestas personales|\n",
      "| |Hacking ético|Vulnerabilidades menores identificadas|Se requieren ajustes para fortalecer la seguridad del chatbot|\n",
      "| |Word Error Rate (WER)|WER < 5%|Las respuestas del chatbot son altamente precisas|\n",
      "| |ROUGE|ROUGE-L > 0.8|Las respuestas del chatbot capturan adecuadamente el contenido de las políticas|\n",
      "| |Similitud coseno / Índice de Jaccard|Similitud > 0.9|El chatbot proporciona respuestas consistentes ante consultas similares|\n",
      "|MANAGEMENT SOLUTIONS|TextFooler|Resiliencia moderada ante texto adversario|El chatbot es moderadamente robusto ante información engañosa|\n",
      "| |Few-shot learning|Adaptabilidad satisfactoria|El chatbot puede adaptarse a nuevas políticas o actualizaciones con un entrenamiento mínimo, pero se requiere monitorizar y agregar esos nuevos documentos al RAG periódicamente|\n",
      "| | | |Se requiere mejorar la capacidad del chatbot para explicar sus decisiones, si bien el componente de RAG SHAP se ha construido de manera que el LLM da una respuesta autoexplicativa|\n",
      "| |AI Fairness 360 / BBQ|Sesgos demográficos menores|El chatbot presenta algunos sesgos que deben ser identificados y mitigados|\n",
      "| |Perspective API / RealToxicityPrompts|Toxicidad baja (< 5%)|Las respuestas del chatbot rara vez contienen lenguaje tóxico o inapropiado|\n",
      "| |Apache JMeter|Escalabilidad satisfactoria (hasta 1000 usuarios)|El chatbot puede manejar altas cargas de trabajo sin degradación significativa del rendimiento|\n",
      "|Implementación y uso|TTFB / Uso de recursos / Latencia|Eficiencia adecuada (TTFB < 1s, uso moderado)|El chatbot responde rápidamente y utiliza los recursos de manera eficiente|\n",
      "| |NPS / CSAT|Satisfacción alta (NPS > 60, CSAT > 80%)|Los usuarios están altamente satisfechos con el chatbot y lo recomendarían a otros|\n",
      "Estos resultados indican que el chatbot de políticas está bien encaminado para su implementación en la compañía, aunque se han identificado algunas áreas específicas que requieren mejoras adicionales. La sección siguiente abordará las principales conclusiones y recomendaciones derivadas de este proceso de validación.\n",
      "\n",
      "# Principales conclusiones\n",
      "\n",
      "El proceso de validación del chatbot de políticas ha demostrado que este sistema basado en LLM puede ser una herramienta valiosa para facilitar el acceso de los empleados a la información relevante de la compañía. Los resultados obtenidos en las diversas pruebas y evaluaciones indican que el chatbot cumple, en gran medida, con los requisitos de calidad, seguridad y eficiencia establecidos por la organización.\n",
      "\n",
      "Entre las fortalezas identificadas, se destacan la precisión y consistencia de las respuestas del chatbot, su capacidad para adaptarse a nuevos escenarios y su escalabilidad para manejar altas cargas de trabajo. Además, la satisfacción de los usuarios con la herramienta es alta, lo que sugiere una buena aceptación y adopción por parte de los empleados.\n",
      "\n",
      "Sin embargo, el proceso de validación también ha revelado algunas áreas de mejora que deben abordarse antes de la implementación definitiva del chatbot. En particular, se recomienda:\n",
      "\n",
      "1. Mejorar la explicabilidad del modelo: es necesario desarrollar técnicas más avanzadas para que el chatbot pueda proporcionar explicaciones claras y comprensibles sobre su proceso de toma de decisiones. Esto aumentará la transparencia y la confianza de los usuarios en la herramienta. Si bien el componente de RAG se ha construido de manera que el LLM da una respuesta autoexplicativa y hace referencia a la política.\n",
      "2. Mitigar los sesgos identificados: aunque los sesgos detectados son menores, es recomendable aplicar técnicas de debiasing para garantizar que las respuestas del chatbot sean equitativas y no discriminatorias. Se sugiere una revisión periódica de los sesgos y la implementación de medidas correctivas cuando sea necesario.\n",
      "3. Fortalecer la seguridad y privacidad: si bien el chatbot cumple con los estándares básicos de protección de datos personales, se recomienda realizar pruebas adicionales y recurrentes de hacking ético y adoptar medidas de seguridad más robustas para prevenir posibles vulnerabilidades.\n",
      "4. Establecer un plan de monitoreo y mejora continua: es fundamental definir un proceso de seguimiento y evaluación periódica del desempeño del chatbot, con el fin de identificar oportunidades de mejora y garantizar su óptimo funcionamiento a largo plazo. Este plan debe incluir la recopilación de feedback de los usuarios, la actualización regular de las políticas y su inclusión en la base de datos del chatbot, el monitoreo para mejorar los parámetros empleados en el RAG y su actualización, y la incorporación de nuevas técnicas y tecnologías cuando estén disponibles.\n",
      "\n",
      "En conclusión, el chatbot de políticas ha demostrado tener potencial para mejorar la eficiencia y la accesibilidad de la información en la compañía. Con la implementación de las mejoras sugeridas y un enfoque de perfeccionamiento continuo, este sistema basado en LLM puede convertirse en una herramienta estratégica para el éxito de la organización. La recomendación final ha sido proceder con la implementación del chatbot, teniendo en cuenta las observaciones y recomendaciones derivadas de este proceso de validación.\n",
      "# Conclusiones\n",
      "\n",
      "“Los LLM son los únicos que pueden escribir una novela, traducirla a diez idiomas, y aun así no entender la trama“.\n",
      "\n",
      "Perplexity130\n",
      "\n",
      "# MANAGEMENT SOLUTIONS\n",
      "\n",
      "50\n",
      "\n",
      "# El auge de los large language models: de los fundamentos a la aplicación\n",
      "# Los modelos de lenguaje a gran escala (LLM)\n",
      "\n",
      "Los modelos de lenguaje a gran escala (LLM) representan un avance significativo en el campo de la inteligencia artificial y están revolucionando la forma en que interactuamos con la tecnología y aprovechamos el procesamiento del lenguaje natural. Su capacidad para procesar y generar texto coherente y contextualizado abre un amplio abanico de aplicaciones en diversos sectores, desde la creación de contenido y el análisis de sentimientos hasta la automatización de tareas y la mejora de la experiencia del usuario.\n",
      "\n",
      "Sin embargo, el desarrollo y despliegue de LLM también conlleva una serie de retos y consideraciones importantes. La presencia de sesgos y alucinaciones en sus resultados, la falta de transparencia y explicabilidad de sus decisiones, los desafíos de privacidad y seguridad de la información, y el alto consumo de recursos computacionales son algunos de los principales desafíos que deben abordarse para garantizar un uso responsable y ético de estos sistemas.\n",
      "\n",
      "Para hacer frente a estos retos, es fundamental establecer un marco robusto de gobierno de la IA, especialmente en el ámbito de la IA generativa y los LLM. Este marco debe abarcar todos los aspectos clave, incluyendo la estrategia, el apetito al riesgo, el gobierno, la organización, el marco de control (políticas y procedimientos), los datos, los sistemas y el reporting. Solo a través de un enfoque integral y bien estructurado se podrá garantizar un desarrollo y uso responsable de estas tecnologías.\n",
      "\n",
      "Dentro de este marco de gobierno, la validación juega un papel crucial. Es conveniente adoptar un enfoque multidimensional que abarque todas las etapas del ciclo de vida de los LLM, desde la calidad de los datos de entrada y la solidez del diseño del modelo hasta la evaluación exhaustiva de sus resultados y la implementación y uso adecuados. Este proceso de validación debe combinar métricas cuantitativas estandarizadas con técnicas de evaluación humana adaptadas al contexto específico de cada caso de uso.\n",
      "\n",
      "# Perplexity AI\n",
      "\n",
      "Perplexity AI es un motor de conversación y búsqueda de investigación basado en LLM fundado en 2022 por Andy Konwinski, Denis Yarats, Johnny Ho y Aravind Srinivas (ex OpenAI), que responde a las consultas mediante texto predictivo en lenguaje natural.\n",
      "# Glosario\n",
      "\n",
      "# MANAGEMENT SOLUTIONS\n",
      "\n",
      "52\n",
      "\n",
      "# El auge de los large language models: de los fundamentos a la aplicación\n",
      "# AGI (Artificial General Intelligence)\n",
      "\n",
      "inteligencia artificial hipotética futura que igualaría o superaría la inteligencia humana en cualquier dominio intelectual, siendo capaz de realizar cualquier tarea intelectual que un ser humano puede hacer.\n",
      "\n",
      "# Embeddings\n",
      "\n",
      "representaciones densas y continuas de elementos discretos (como palabras, frases o documentos) en un espacio vectorial de alta dimensión, donde elementos similares tienen representaciones cercanas. Se utilizan en los LLM para capturar relaciones semánticas y sintácticas entre los elementos del lenguaje.\n",
      "\n",
      "# Alucinaciones\n",
      "\n",
      "generación de información o contenido por parte de un LLM que parece plausible pero que no se basa en hechos reales o en el conocimiento adquirido durante el entrenamiento, llevando a inexactitudes o invenciones en las respuestas del modelo.\n",
      "\n",
      "# Ética de la IA\n",
      "\n",
      "disciplina que estudia los principios morales, valores y directrices que deben guiar el desarrollo, despliegue y uso de los sistemas de inteligencia artificial, con el objetivo de garantizar que sean beneficiosos, justos, transparentes y alineados con los valores humanos.\n",
      "\n",
      "# CNN (Convolutional Neural Network)\n",
      "\n",
      "tipo de red neuronal especializada en procesar datos con una topología de cuadrícula, como imágenes o series temporales. Las CNN utilizan capas de convolución para extraer automáticamente características locales y abstractas de los datos, y son ampliamente utilizadas en tareas de visión por computador y procesamiento de señales.\n",
      "\n",
      "# Evaluación humana\n",
      "\n",
      "proceso de revisión y valoración cualitativa del comportamiento y resultados de un sistema de IA por parte de expertos y usuarios, que complementa las métricas cuantitativas y permite detectar errores, sesgos o comportamientos indeseados que podrían pasar desapercibidos en una evaluación puramente automática.\n",
      "\n",
      "# Cuantización\n",
      "\n",
      "técnica utilizada para reducir el tamaño y acelerar la inferencia de los LLM, que consiste en reducir la precisión numérica de los pesos del modelo, pasando de números en coma flotante a representaciones de menor precisión, como enteros o números en coma fija.\n",
      "\n",
      "# Explicabilidad (XAI, eXplainable AI)\n",
      "\n",
      "propiedad de un modelo de IA que se refiere a su capacidad para proporcionar explicaciones comprensibles para los humanos sobre su funcionamiento interno, el razonamiento detrás de sus predicciones y los factores que influyen en sus decisiones.\n",
      "\n",
      "# Datos de entrenamiento\n",
      "\n",
      "conjunto de ejemplos utilizados para entrenar un modelo de aprendizaje automático, que incluyen las entradas (features) y, en el caso del aprendizaje supervisado, las etiquetas o respuestas esperadas. La calidad y diversidad de estos datos es crucial para el rendimiento y la generalización del modelo.\n",
      "\n",
      "# Efecto Eliza\n",
      "\n",
      "fenómeno psicológico por el cual los usuarios tienden a atribuir capacidades cognitivas y emocionales similares a las humanas a los sistemas de conversación basados en IA, a pesar de que estos sistemas no poseen una comprensión real del lenguaje ni inteligencia general.\n",
      "\n",
      "# Few-shot learning\n",
      "\n",
      "capacidad de un modelo de aprendizaje automático, especialmente los LLM, para aprender a realizar una nueva tarea a partir de pocos ejemplos (desde uno hasta unas decenas), aprovechando el conocimiento previo adquirido durante el preentrenamiento en grandes cantidades de datos.\n",
      "\n",
      "# Fine-tuning\n",
      "\n",
      "técnica para adaptar un modelo de lenguaje preentrenado a una tarea específica, mediante el entrenamiento adicional con un conjunto de datos más pequeño y especializado en esa tarea. Permite aprovechar el conocimiento general del modelo y ajustarlo para obtener un alto rendimiento en aplicaciones concretas.\n",
      "# Hacking ético\n",
      "\n",
      "Práctica de probar y desafiar un sistema de IA de manera controlada y con permiso, con el objetivo de identificar vulnerabilidades, fallos, sesgos o comportamientos no deseados, para posteriormente corregirlos y mejorar la seguridad y robustez del sistema.\n",
      "\n",
      "# MANAGEMENT SOLUTIONS\n",
      "\n",
      "# Instruction tuning\n",
      "\n",
      "Técnica de ajuste fino para LLM que consiste en proporcionar al modelo instrucciones, preguntas y ejemplos de respuestas esperadas, con el objetivo de alinear su comportamiento con las expectativas y preferencias de los usuarios en un dominio específico.\n",
      "\n",
      "# LLMOps (Large Language Model Operations)\n",
      "\n",
      "Conjunto de prácticas, herramientas y procesos para gestionar de manera eficiente y escalable el ciclo de vida completo de los LLM en entornos de producción, abarcando el entrenamiento, despliegue, monitorización, actualización y gobierno de estos modelos.\n",
      "\n",
      "# Machine learning\n",
      "\n",
      "Bramo de la inteligencia artificial que se centra en el desarrollo de algoritmos y modelos que permiten a los sistemas aprender y mejorar automáticamente a través de la experiencia, sin ser programados explícitamente para ello.\n",
      "\n",
      "# Inteligencia Artificial (IA)\n",
      "\n",
      "Campo de la informática y la ingeniería que se dedica al desarrollo de sistemas capaces de realizar tareas que normalmente requieren inteligencia humana, como el aprendizaje, el razonamiento, la percepción, la interacción en lenguaje natural y la resolución de problemas.\n",
      "\n",
      "# Machine unlearning\n",
      "\n",
      "Conjunto de técnicas para eliminar o \"desaprender\" de manera selectiva cierta información o sesgos indeseados de un modelo de aprendizaje automático ya entrenado, sin necesidad de reentrenarlo desde cero, permitiendo cumplir con requisitos de privacidad o corregir comportamientos no deseados.\n",
      "\n",
      "# Inteligencia artificial generativa (GenAI)\n",
      "\n",
      "Subcampo de la IA que se enfoca en la creación de modelos y algoritmos capaces de generar contenido nuevo y original, como texto, imágenes, vídeo, audio, código fuente o diseños 3D, aprendiendo patrones y características a partir de un conjunto de datos de entrenamiento.\n",
      "\n",
      "# Métricas cuantitativas\n",
      "\n",
      "Medidas numéricas estandarizadas utilizadas para evaluar de manera objetiva y consistente el rendimiento de un modelo de IA en tareas específicas, como la precisión, la exhaustividad, la exactitud o la eficiencia.\n",
      "\n",
      "# Modelo generativo\n",
      "\n",
      "Tipo de modelo de aprendizaje automático diseñado para aprender la distribución de probabilidad subyacente a un conjunto de datos y generar nuevas muestras que sean similares a los datos de entrenamiento, pudiendo crear contenido nuevo y realista.\n",
      "\n",
      "# Preentrenamiento\n",
      "\n",
      "Etapa inicial del entrenamiento de un LLM en la que se utiliza un gran corpus de texto no estructurado y sin etiquetar para que el modelo aprenda representaciones generales y patrones del lenguaje, adquiriendo un conocimiento amplio y robusto que luego puede ser adaptado a tareas específicas mediante fine-tuning.\n",
      "# Privacidad diferencial\n",
      "\n",
      "técnica criptográfica utilizada para compartir información agregada sobre un conjunto de datos, mientras se protege la privacidad de los individuos presentes en esos datos, introduciendo un ruido aleatorio que dificulta la identificación de entradas individuales a partir de los resultados del análisis.\n",
      "\n",
      "# Prompt engineering\n",
      "\n",
      "disciplina que se enfoca en diseñar, optimizar y adaptar los prompts (entradas de texto) para obtener los mejores resultados posibles de los LLM en tareas específicas, aprovechando técnicas como la inclusión de ejemplos, la especificación de formatos o la orientación paso a paso.\n",
      "\n",
      "# Pruebas A/B\n",
      "\n",
      "método experimental utilizado para comparar el rendimiento de dos versiones diferentes de un sistema de IA (A y B) o entre un sistema de IA y un enfoque alternativo (como un humano o un modelo base), con el objetivo de determinar cuál funciona mejor según métricas predefinidas.\n",
      "\n",
      "# Regulación de la IA\n",
      "\n",
      "conjunto de leyes, normativas, estándares y directrices establecidos por gobiernos y organizaciones para garantizar que el desarrollo, despliegue y uso de los sistemas de inteligencia artificial se realice de manera responsable, segura, ética y alineada con los valores y derechos fundamentales de la sociedad.\n",
      "\n",
      "# Token\n",
      "\n",
      "unidad discreta en la que se divide un texto para su procesamiento por parte de un modelo de lenguaje. Los tokens pueden ser palabras, subpalabras o caracteres, y constituyen la entrada básica para el entrenamiento y la inferencia de los LLM.\n",
      "\n",
      "# Tokenización\n",
      "\n",
      "proceso de convertir un texto en una secuencia de tokens. La elección de la estrategia de tokenización tiene un impacto significativo en el rendimiento y la eficiencia del modelo.\n",
      "\n",
      "# Transformers\n",
      "\n",
      "arquitectura de red neuronal profunda que utiliza mecanismos de atención para procesar y generar secuencias de forma paralela, en lugar de secuencialmente como las RNNs. Permite capturar dependencias a largo plazo y contextuales, siendo la arquitectura dominante para los LLM y estableciendo el estado del arte en diversas tareas de procesamiento del lenguaje natural.\n",
      "\n",
      "# Validación\n",
      "\n",
      "proceso integral y multidisciplinar para evaluar un sistema de IA, especialmente LLM, en términos de rendimiento, robustez, seguridad, equidad, explicabilidad y alineación con los requisitos y valores éticos y sociales, combinando métricas cuantitativas y evaluación cualitativa por parte de expertos y usuarios.\n",
      "\n",
      "# Retrieval-Augmented Generation (RAG)\n",
      "\n",
      "técnica utilizada en los LLM que consiste en recuperar información relevante de una base de conocimientos externa antes de generar una respuesta, combinando así la capacidad de acceso a información estructurada con la generación de lenguaje natural coherente y fluido.\n",
      "\n",
      "# RNN (Recurrent Neural Network)\n",
      "\n",
      "tipo de red neuronal diseñada para procesar secuencias de datos, como texto o series temporales. A diferencia de las redes neuronales feedforward, las RNN tienen conexiones recurrentes que les permiten mantener un estado interno y capturar dependencias temporales. Variantes como LSTM y GRU han sido ampliamente utilizadas en tareas de procesamiento del lenguaje natural antes del auge de los transformers.\n",
      "\n",
      "# Seguridad (AI safety)\n",
      "\n",
      "disciplina que se enfoca en identificar, prevenir y mitigar los riesgos potenciales asociados con el desarrollo y uso de sistemas de IA avanzados, tanto a corto como a largo plazo, incluyendo riesgos de seguridad, sesgos, errores, mal uso o consecuencias no deseadas.\n",
      "\n",
      "# Sesgo\n",
      "\n",
      "tendencia sistemática de un modelo de aprendizaje automático a producir resultados que favorecen o perjudican injustamente a ciertos grupos o individuos, debido a características sensibles como el género, la etnia, la edad o la orientación sexual, y que suele ser resultado de sesgos presentes en los datos de entrenamiento o de decisiones subóptimas durante el desarrollo del modelo.\n",
      "# Bibliografía\n",
      "\n",
      "MANAGEMENT SOLUTIONS\n",
      "\n",
      "El auge de los large language models: de los fundamentos a la aplicación\n",
      "\n",
      "Magyt orszag\n",
      "# References\n",
      "\n",
      "Abhyankar, R. et al. (2024). APIServe: Efficient API Support for Large-Language Model Inferencing. https://arxiv.org/abs/2402.01869. arXiv:2402.01869v1\n",
      "\n",
      "CIO (2023). Chief AI Officer: What it takes to land the C-suite’s hottest new job. https://www.cio.com/article/657977/chief-ai-officer-what-it-takes-to-land-the-c-suites-hottest-new-job.html\n",
      "\n",
      "Alabdulmohsin, I. et al. (2024). CLIP the Bias: How Useful is Balancing Data in Multimodal Learning? https://arxiv.org/html/2403.04547v1. arXiv:2403.04547v1\n",
      "\n",
      "Cui, Q. et al. (2022). Contrastive Vision-Language Pre-training with Limited Resources. https://arxiv.org/abs/2112.09331. arXiv:2112.09331v3\n",
      "\n",
      "Banerjee, I., et al. (2023). MLOps with enhanced performance control and observability. https://arxiv.org/abs/2302.01061. arXiv:2302.01061v1\n",
      "\n",
      "CommetML. https://www.comet.com/site/\n",
      "\n",
      "Datta, T. et al. (2023). Who's Thinking? A Push for Human-Centered Evaluation of LLMs using the XAI Playbook. https://arxiv.org/abs/2303.06223. arXiv:2303.06223v1\n",
      "\n",
      "Bengio, Y. et al. (2003). A Neural Probabilistic Language Model. https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf\n",
      "\n",
      "Bréal, M. (1883). Les lois intellectuelles du langage fragment de sémantique. Annuaire de l'Association pour l'encouragement des études grecques en France. Vol. 17 (1883), pp. 132-142. https://www.jstor.org/stable/44253893\n",
      "\n",
      "Cambon, A. et al. (2023). Early LLM-based Tools for Enterprise Information Workers Likely Provide Meaningful Boosts to Productivity. A first update from Microsoft’s research initiative on AI and Productivity.\n",
      "\n",
      "Chen, D. et al. (2023). Data-Juicer: A One-Stop Data Processing System for Large Language Models. https://arxiv.org/abs/2309.02033. arXiv:2309.02033v3\n",
      "\n",
      "Duan, J. et al. (2023). Shifting attention to relevance: towards the uncertainty estimation of large language models. https://arxiv.org/abs/2307.01379. arXiv:2307.01379v2\n",
      "\n",
      "Chen, Y. et al. (2023). LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models. https://arxiv.org/abs/2309.12307. arXiv:2309.12307v3\n",
      "\n",
      "Elazar, Y. et al. (2021). Measuring and Improving Consistency in Pretrained Language Models. https://aclanthology.org/2021.tacl-1.60/\n",
      "\n",
      "Chiang, C. et al. (2023). Can Large Language Models Be an Alternative to Human Evaluations? https://arxiv.org/abs/2305.01937. arXiv:2305.01937v1\n",
      "\n",
      "Chu, T., Song, Z., Yang, C. (2023). How to Protect Copyright Data in Optimization of Large Language Models? https://arxiv.org/abs/2308.12247. arXiv:2308.12247v1\n",
      "\n",
      "Euronews (2023). 2023 was the year AI went mainstream. It was also the year we started to panic about it. https://www.euronews.com/next/2023/12/27/2023-was-the-year-ai-went-mainstream-it-was-also-the-year-we-started-to-panic-about-it\n",
      "# European Parliament (2024). Artificial Intelligence Act\n",
      "\n",
      "European Parliament legislative resolution of 13 March 2024 on the proposal for a regulation of the European Parliament and of the Council on laying down harmonized rules on Artificial Intelligence (Artificial Intelligence Act) and amending certain Union Legislative Acts (COM(2021)0206 – C9-0146/2021 – 2021/0106(COD)). https://artificialintelligenceact.eu/; https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=celex%3A52021PC0206\n",
      "\n",
      "# References\n",
      "\n",
      "- European Commission (2024). Knowledge Center on Interpretation. https://knowledge-centre-interpretation.education.ec.europa.eu/en/news/what-large-language-model\n",
      "- Huang, L. et al. (2023). A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions. https://arxiv.org/abs/2311.05232. arXiv:2311.05232v1\n",
      "- Hugging Face Datasets (2024). CodeParrot. https://huggingface.co/codeparrot\n",
      "- IAPP (2024). Global AI Law and Policy Tracker. https://iapp.org/resources/article/global-ai-legislation-tracker/\n",
      "- iDanae 2T23 (2023): Large Language Models: una nueva era en la inteligencia artificial. Cátedra iDanae. Newsletter trimestral 2T23. http://www.idanae-stem.com/\n",
      "- Fisher, M., Campagna, G., Choi, E., Lam, M. S., Freund, S. N., Yahav, E. (2021). DIY Assistant: A Multi-modal End-User Programmable Virtual Assistant. https://dl.acm.org/doi/10.1145/3453483.3454046\n",
      "- Gartner (2023). What is generative AI? https://www.gartner.com/en/topics/generative-ai\n",
      "- iDanae 1T24 (2024): Hacia una inteligencia artificial sostenible. Cátedra iDanae. Newsletter trimestral 1T24. http://www.idanae-stem.com/\n",
      "- Imperial, JM., et al. (2023). Flesch or Fumble? Evaluating Readability Standard Alignment of Instruction-Tuned Language Models. https://arxiv.org/abs/2309.05454. arXiv:2309.05454v2\n",
      "- Google DeepMind (2023). Levels of AGI: Operationalizing Progress on the Path to AGI. Meredith Ringel Morris; Jascha Sohl-Dickstein; Noah Fiedel; Tris Warkentin; Allan Dafoe; Aleksandra Faust; Clement Farabet; and Shane Legg. arXiv:2311.02462v1\n",
      "- IndesIA (2024). Barómetro de adopción de la inteligencia artificial en las pymes españolas. https://www.indesia.org/wp-content/uploads/2024/04/IndesIA.-Barometro-de-adopcion-de-la-inteligencia-artificial-en-las-pymes-espanolas-Edicion-2024.pdf\n",
      "- Google + Implement (2023). The economic opportunity of generative AI in D9+. An Implement Consulting Group study commissioned by Google.\n",
      "- Jang et al. (2022). Knowledge unlearning for mitigating privacy risks in language models. https://arxiv.org/abs/2210.01504. arXiv:2210.01504.\n",
      "- Gozalo-Brizuela, R., y Garrido-Merchán, E.C. (2023). A survey of Generative AI Applications. https://ar5iv.labs.arxiv.org/html/2306.02781\n",
      "- Guo, Z. et al. (2023). Evaluating Large Language Models: A Comprehensive Survey. https://arxiv.org/pdf/2310.19736.pdf. arXiv:2310.19736v3\n",
      "- Jia, C. et al (2021). Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision. https://arxiv.org/abs/2102.05918. arXiv:2102.05918v2\n",
      "- Kahng, M. et al. (2024). LLM Comparator: Visual Analytics for Side-by-Side Evaluation of Large Language Models. https://arxiv.org/abs/2402.10524. arXiv:2402.10524v1\n",
      "- Guzman, F. et al. (2015). How do Humans Evaluate Machine Translation. https://aclanthology.org/W15-3059.pdf.\n",
      "- Fu, HY. et al. (2023). Estimating Large Language Model Capabilities without Labeled Test Data. https://arxiv.org/abs/2305.14802. arXiv:2305.14802v2\n",
      "- Fu, X. et al (2024). Tiny Titans: Can Smaller Large Language Models Punch Above Their Weight in the Real World for Meeting Summarization? https://arxiv.org/abs/2402.00841. arXiv:2402.00841\n",
      "- Goyal, S. et al (2024). LLMGuard: Guarding Against Unsafe LLM Behavior. https://arxiv.org/abs/2403.00826. arXiv:2403.00826v1\n",
      "- Hendrycks, D. et al (2021). Measuring Massive Multitask Language Understanding. https://arxiv.org/abs/2009.03300. arXiv:2009.03300v3\n",
      "- Kuchnik, M. et al. (2023). Validating Large Language Models with Realm. https://arxiv.org/abs/2211.15458. arXiv:2211.15458v2\n",
      "- Kudo, T., Richardson, J. (2018). SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing. https://arxiv.org/abs/1808.06226. arXiv:1808.06226v1\n",
      "- Lam, M. (2018). https://profiles.stanford.edu/monica-lam?tab=publications. Keeping the Internet Open with an Open-Source Virtual Assistant.\n",
      "- Lee, C. et al (2024). OrchestraLLM: Efficient Orchestration of Language Models for Dialogue State Tracking. https://arxiv.org/html/2311.09758v2. arXiv:2311.09758v2\n",
      "# References\n",
      "\n",
      "Lee, J. et al. (2022). Seq2Seq-SC: End-to-End Semantic Communication Systems with Pre-trained Language Model. https://arxiv.org/abs/2210.15237. arXiv:2210.15237v2\n",
      "\n",
      "Lester, B. et al. (2021). The Power of Scale for Parameter-Efficient Prompt Tuning. https://arxiv.org/abs/2104.08691. arXiv:2104.08691v2\n",
      "\n",
      "Lewis, P., et al. (2020). Retrieval-augmented generation for knowledge-intensive NLP tasks. https://arxiv.org/abs/2005.11401\n",
      "\n",
      "Li, H. et al. (2024). Digger: Detecting Copyright Content Misusage in Large Language Model Training. https://arxiv.org/abs/2401.00676. arXiv:2401.00676v1\n",
      "\n",
      "Li, S. et al (2024). Evaluating Quantized Large Language Models. https://arxiv.org/abs/2402.18158. arXiv:2402.18158v1\n",
      "\n",
      "Meeus, M. et al. (2024). Copyright Traps for Large Language Models. https://arxiv.org/abs/2402.09363. arXiv:2402.09363v1\n",
      "\n",
      "Mehta, S.V. et al. (2023). An Empirical Investigation of the Role of Pre-training in Lifelong Learning. https://arxiv.org/abs/2112.09153. arXiv:2112.09153v2\n",
      "\n",
      "Mikolov, T. et al. (2013). Efficient Estimation of Word Representations in Vector Space. https://arxiv.org/abs/1301.3781. arXiv:1301.3781v3\n",
      "\n",
      "Minaee, S. et al. (2024). Large Language Models: A Survey. https://arxiv.org/abs/2402.06196. arXiv:2402.06196v2\n",
      "\n",
      "MindsDB (2024). A Comparative Analysis of Leading Large Language Models. https://mindsdb.com/blog/navigating-the-llm-landscape-a-comparative-analysis-of-leading-large-language-models\n",
      "\n",
      "Li, Y. et al (2023). A Survey on Fairness in Large Language Models. https://arxiv.org/abs/2308.10149. arXiv:2308.10149\n",
      "\n",
      "Mökander, J. et al. (2023). Auditing large language models: a three-layered approach. arXiv:2302.08500v2\n",
      "\n",
      "Liang, P. et al. (2023). Holistic Evaluation of Language Models. https://arxiv.org/abs/2211.09110. arXiv:2211.09110v2\n",
      "\n",
      "Liu, T. et al (2022). Autoregressive Structured Prediction with Language Models. https://arxiv.org/abs/2210.14698. arXiv:2210.14698v2\n",
      "\n",
      "Neelakantan, A. et al. (2022). Text and Code Embeddings by Contrastive Pre-Training. https://arxiv.org/abs/2201.10005. arXiv:2201.10005v1\n",
      "\n",
      "Liu, Y. et al (2024). Datasets for Large Language Models: A Comprehensive Survey. https://arxiv.org/abs/2402.18041. arXiv:2402.18041v1\n",
      "\n",
      "NIST (2023). AI Risk Management Framework | NIST. https://www.nist.gov/itl/ai-risk-management-framework\n",
      "\n",
      "Oneto, L., Chiappa, S. (2020). Fairness in Machine Learning. 2012.15816.pdf arXiv:2012.15816v1\n",
      "\n",
      "OpenAI (2024). Prompt engineering. https://platform.openai.com/docs/guides/prompt-engineering\n",
      "\n",
      "Luo, Y. et al. (2023). An Empirical Study of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning. https://arxiv.org/pdf/2308.08747.pdf. arXiv:2308.08747v3\n",
      "\n",
      "Management Solutions (2023). Explainable Artificial Intelligence (XAI): desafíos en la interpretabilidad de los modelos. https://www.managementsolutions.com/en/microsites/whitepapers/explainable-artificial-intelligence\n",
      "\n",
      "Management Solutions (2022). AutoML, hacia la automatización de los modelos. https://www.managementsolutions.com/es/publicaciones-y-eventos/informes-sectoriales/white-papers/auto-machine-learning-hacia-la-automatizacion-de-los-modelos\n",
      "\n",
      "Ovadia, O. et al (2024). Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs. https://arxiv.org/pdf/2312.05934.pdf. arXiv:2312.05934v3\n",
      "\n",
      "Pankajakshan, R. et al (2024). Mapping LLM Security Landscapes: A Comprehensive Stakeholder Risk Assessment Proposal. https://arxiv.org/html/2403.13309v1. arXiv:2403.13309v1\n",
      "\n",
      "Parikh, A. P., et al. (2016). A Decomposable Attention Model for Natural Language Inference. https://arxiv.org/abs/1606.01933. arXiv:1606.01933v2\n",
      "\n",
      "Penedo, G. et al (2023). The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only. https://arxiv.org/abs/2306.01116. arXiv:2306.01116v1\n",
      "\n",
      "Management Solutions (2014). Model Risk Management: Quantitative and Qualitative Aspects. Model Risk Management: Quantitative and qualitative aspects | Management Solutions\n",
      "\n",
      "Pew Research Center (2023). Experts Predict the Best and Worst Changes in Digital Life by 2035.\n",
      "\n",
      "Project Gutenberg (2024). https://www.gutenberg.org/\n",
      "# References\n",
      "\n",
      "Rae, JW, et al (2021). Scaling Language Models: Methods, Analysis & Insights from Training Gopher. https://arxiv.org/abs/2112.11446. arXiv:2112.11446\n",
      "\n",
      "Sinha, K. et al (2021). Masked Language Modeling and the Distributional Hypothesis: Order Word Matters Pre-training for Little. https://arxiv.org/abs/2104.06644. arXiv:2104.06644v2\n",
      "\n",
      "Rafailov, R. et al (2023). Direct Preference Optimization: Your Language Model is Secretly a Reward Model. https://arxiv.org/abs/2305.18290. arXiv:2305.18290v2\n",
      "\n",
      "Soskek (2019). BookCorpus. https://github.com/soskek/bookcorpus.\n",
      "\n",
      "Su, J., et al (2021). Roformer: Enhanced transformer with rotary position embedding. https://arxiv.org/abs/2104.09864. arXiv:2104.09864.\n",
      "\n",
      "Sutskever, I. et al (2014). Sequence to Sequence Learning with Neural Networks. https://arxiv.org/abs/1409.3215. arXiv:1409.3215v3\n",
      "\n",
      "The Next Web (2023). When will AGI arrive? Here’s what our tech lords predict. https://thenextweb.com/news/when-will-agi-arrive-tech-experts-predict-artificial-general-intelligence\n",
      "\n",
      "Risk.net. (2024). The bank quant who wants to stop gen AI hallucinating. https://www.risk.net/risk-management/7959062/the-bank-quant-who-wants-to-stop-gen-ai-hallucinating.\n",
      "\n",
      "Sachdeva, N., et al (2024). How to Train Data-Efficient LLMs. https://arxiv.org/html/2402.09668v1. arXiv:2402.09668v1\n",
      "\n",
      "Tian, Y. et al (2024). TinyLLM: Learning a Small Student from Multiple Large Language Models. https://arxiv.org/abs/2402.04616. arXiv:2402.04616\n",
      "\n",
      "Samsi, S., et al (2023). From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference. https://arxiv.org/pdf/2310.03003.pdf. arXiv:2310.03003v1\n",
      "\n",
      "Tirumala, K. et al. (2023). D4: Improving LLM Pretraining via Document De-Duplication and Diversification. https://arxiv.org/abs/2308.12284. arXiv:2308.12284v1\n",
      "\n",
      "Sarti, G. et al (2023). Inseq: An Interpretability Toolkit for Sequence Generation Models. [2302.13942] Inseq: An Interpretability Toolkit for Sequence Generation Models (arxiv.org). arXiv:2302.13942v3\n",
      "\n",
      "UK Government (2023). The Bletchley Declaration by Countries Attending the AI Safety Summit, 1-2 November 2023. https://www.gov.uk/government/publications/ai-safety-summit-2023-the-bletchley-declaration/the-bletchley-declaration-by-countries-attending-the-ai-safety-summit-1-2-november-2023\n",
      "\n",
      "Vartziotis, T. et al (2024). Learn to Code Sustainably: An Empirical Study on LLM-based Green Code Generation.\n",
      "\n",
      "Shaikh, O. et al. (2022). On Second Thought, Let's Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning. https://arxiv.org/html/2403.03344v1. arXiv:2403.03344v1.\n",
      "\n",
      "Vaswani, A. et al. (2017). Attention Is All You Need. https://arxiv.org/abs/1706.03762\n",
      "\n",
      "Shaw, P. et al (2018). Self-Attention with Relative Position Representations. https://arxiv.org/abs/1803.02155. arXiv:1803.02155v2\n",
      "\n",
      "Sherstinsky, A. (2018). Fundamentals of Recurrent Neural Network (RNN) and Long Short-Term Memory (LSTM) Network. https://arxiv.org/abs/1808.03314. arXiv:1808.03314v10\n",
      "\n",
      "Shi, W. et al (2024). Detecting pretraining data from large language models. https://arxiv.org/abs/2310.16789. arXiv:2310.16789v3\n",
      "\n",
      "Singh, C. et al (2024). Rethinking Interpretability in the Era of Large Language Models. https://arxiv.org/abs/2402.01761. arXiv:2402.01761v1\n",
      "\n",
      "Wei, J. et al (2022). Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. https://arxiv.org/abs/2201.11903. arXiv:2201.11903v6\n",
      "# References\n",
      "\n",
      "Wenzek, G., et al (2019). CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data. https://arxiv.org/abs/1911.00359. arXiv:1911.00359v2\n",
      "\n",
      "Yu, C. et al. (2023). Unlearning bias in language models by partitioning gradients. https://aclanthology.org/2023.findings-acl.375.pdf.\n",
      "\n",
      "Wettig, A. et al. (2024). QuRating: Selecting High-Quality Data for Training Language Models. https://arxiv.org/abs/2402.09739. arXiv:2402.09739v1\n",
      "\n",
      "Yogarajan, V., et al (2023). Tackling Bias in Pre-trained Language Models: Current Trends and Under-represented Societies. https://arxiv.org/pdf/2312.01509.pdf. arXiv:2312.01509v1\n",
      "\n",
      "Weights & Biases: The AI Developer Platform (wandb.ai). https://wandb.ai/site\n",
      "\n",
      "Wikipedia (2024). Dumps. https://dumps.wikimedia.org/zhwiki/latest/.\n",
      "\n",
      "Wired (2023). OpenAI’s CEO Says the Age of Giant AI Models Is Already Over. https://www.wired.com/story/openai-ceo-sam-altman-the-age-of-giant-ai-models-is-already-over/\n",
      "\n",
      "Weizenbaum, J. (1966). ELIZA—a computer program for the study of natural language communication between man and machine. https://dl.acm.org/doi/10.1145/365153.365168\n",
      "\n",
      "White House (2022). Blueprint for an AI Bill Of Rights. Making Automated Systems Work for the American People. https://www.whitehouse.gov/wp-content/uploads/2022/10/Blueprint-for-an-AI-Bill-of-Rights.pdf\n",
      "\n",
      "White House (2023). Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence. https://www.whitehouse.gov/briefing-room/presidential-actions/2023/10/30/executive-order-on-the-safe-secure-and-trustworthy-development-and-use-of-artificial-intelligence/\n",
      "\n",
      "Wu, X. et al. (2023). Depn: Detecting and editing privacy neurons in pretrained language models. https://arxiv.org/abs/2310.20138. arXiv:2310.20138.\n",
      "\n",
      "Xin Zhao, W., et al. (2023). A Survey of Large Language Models. https://arxiv.org/abs/2303.18223. arXiv:2303.18223v13\n",
      "\n",
      "Xu, L. et al. (2023). Parameter-Efficient Fine-Tuning Methods for Pretrained Language Models: A Critical Review and Assessment. https://arxiv.org/pdf/2312.12148.pdf. arXiv:2312.12148v1\n",
      "\n",
      "Xu, Y. et al. (2021). Non-Autoregressive Text Generation with Pre-trained Language Models. https://aclanthology.org/2021.eacl-main.18/\n",
      "\n",
      "Xu, Z. et al. (2024). Hallucination is Inevitable: An Innate Limitation of Large Language Models. https://arxiv.org/abs/2401.11817. arXiv:2401.11817v1\n",
      "\n",
      "Yang, J. et al. (2024). Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond. https://arxiv.org/abs/2304.13712. arXiv:2304.13712v2\n",
      "\n",
      "Yidiz, C. et al (2024). Investigating Continual Pretraining in Large Language Models: Insights and Implications. https://arxiv.org/html/2402.17400v1. arXiv:2402.17400v1\n",
      "\n",
      "Zaharia, M. et al (2018). Accelerating the Machine Learning Lifecycle with MLflow. https://people.eecs.berkeley.edu/~matei/papers/2018/ieee_mlflow.pdf.\n",
      "\n",
      "Zhang, B. et al (2024). When Scaling Meets LLM Finetuning: The Effect of Data, Model and Finetuning Method. https://arxiv.org/abs/2402.17193. arXiv:2402.17193v1\n",
      "\n",
      "Zhang, L. et al (2024). Enhancing Large Language Model Performance To Answer Questions and Extract Information More Accurately. https://arxiv.org/html/2402.01722v1. arXiv:2402.01722v1.\n",
      "\n",
      "Zhang, S. et al (2023). Instruction Tuning for Large Language Models: A Survey. https://www.researchgate.net/publication/373263398_Instruction_Tuning_for_Large_Language_Models_A_Survey.\n",
      "\n",
      "Zhang, Y. et al (2024). Bias Mitigation in Fine-tuning Pre-trained Models for Enhanced Fairness and Efficiency. https://arxiv.org/html/2403.00625v1. arXiv:2403.00625v1\n",
      "\n",
      "Zhao, B., et al (2023). Tuning LayerNorm in Attention: Towards Efficient Multi-Modal LLM Finetuning. https://arxiv.org/abs/2312.11420. arXiv:2312.11420v1\n",
      "\n",
      "Zhou, C. et al (2023). LIMA: Less Is More for Alignment. https://arxiv.org/abs/2305.11206. arXiv:2305.11206v1\n",
      "\n",
      "Zhou, N., et al (2021). Bias, Fairness, and Accountability with AI and ML Algorithms. https://arxiv.org/abs/2105.06558. arXiv:2105.06558v1\n",
      "# MS' Solutionsient\n",
      "\n",
      "# Making things happen\n",
      "\n",
      "Nuestro objetivo es superar las expectativas de nuestros clientes convirtiéndonos en socios de confianza.\n",
      "\n",
      "Management Solutions es una firma internacional de servicios de consultoría centrada en el asesoramiento de negocio, finanzas, riesgos, organización y procesos, tanto en sus componentes funcionales como en la implantación de sus tecnologías relacionadas.\n",
      "\n",
      "Con un equipo multidisciplinar (funcionales, matemáticos, técnicos, etc.) de cerca de 4.000 profesionales, Management Solutions desarrolla su actividad a través de 46 oficinas (20 en Europa, 22 en América, 2 en Asia, 1 en África y 1 en Oceanía).\n",
      "\n",
      "Para dar cobertura a las necesidades de sus clientes, Management Solutions tiene estructuradas sus prácticas por industrias (Entidades Financieras, Energía, Telecomunicaciones y Otras industrias) y por líneas de actividad que agrupan una amplia gama de competencias: Estrategia, Gestión Comercial y Marketing, Gestión y Control de Riesgos, Información de Gestión y Financiera, Transformación: Organización y Procesos, y Nuevas Tecnologías.\n",
      "\n",
      "El área de I+D da servicio a los profesionales de Management Solutions y a sus clientes en aspectos cuantitativos necesarios para acometer los proyectos con rigor y excelencia, a través de la aplicación de las mejores prácticas y de la prospección continua de las últimas tendencias en inteligencia artificial, data science, machine learning, modelización y big data.\n",
      "\n",
      "# El auge de los large language models: de los fundamentos a la aplicación\n",
      "\n",
      "# Javier Calvo Martín\n",
      "\n",
      "Socio de Management Solutions\n",
      "\n",
      "javier.calvo.martin@managementsolutions.com\n",
      "\n",
      "# Manuel Ángel Guzmán Caba\n",
      "\n",
      "Socio de Management Solutions\n",
      "\n",
      "manuel.guzman@managementsolutions.com\n",
      "\n",
      "# Luz Ferrero Peña\n",
      "\n",
      "Gerente de Management Solutions\n",
      "\n",
      "luz.ferrero@msgermany.com.de\n",
      "\n",
      "# Segismundo Jiménez Láinez\n",
      "\n",
      "Gerente de Management Solutions\n",
      "\n",
      "segismundo.jimenez@msspain.com\n",
      "NO_CONTENT_HERE\n",
      "# Management Solutions, servicios profesionales de consultoría\n",
      "\n",
      "Management Solutions es una firma internacional de consultoría centrada en el asesoramiento de negocio, finanzas, riesgos, organización, tecnología y procesos.\n",
      "\n",
      "Para más información visita www.managementsolutions.com\n",
      "\n",
      "Síguenos en: in Xfldle\n",
      "\n",
      "© Management Solutions. 2024\n",
      "\n",
      "Todos los derechos reservados\n",
      "\n",
      "Madrid Barcelona Bilbao Coruña Málaga London Frankfurt Düsseldorf Paris Amsterdam Copenhagen Oslo Stockholm Warszawa Wroclaw Zürich Milano Roma Bologna Lisboa Beijing Istanbul Johannesburgo Sydney Toronto New York New Jersey Boston Pittsburgh Atlanta Birmingham Houston Miami SJ de Puerto Rico San José Ciudad de México Monterrey Querétaro Medellín Bogotá Quito São Paulo Rio de Janeiro Lima Santiago de Chile Buenos Aires\n"
     ]
    }
   ],
   "source": [
    "for doc in document:\n",
    "    print(doc.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
